 1/1: import pypsa
 1/2:
import pandas as pd
import plotly.graph_objects as go
 1/3: n = pypsa.Network()
 1/4: n.add?
 1/5:
n.add("Bus", "mybus")
n.add("Load", "myload", bus="mybus", p_set=100)
n.add("Generator", "mygen", bus="mybus", p_nom=100, marginal_cost=20)
 1/6:
# load an example network
n = pypsa.examples.ac_dc_meshed()
 1/7:
# run the optimisation
n.lopf()
 1/8: n.lpopf?
 1/9: n.lopf?
1/10:
# run the optimisation
n.lopf(solver="gurobi")
1/11:
# run the optimisation
n.lopf(solver_name="gurobi")
1/12:
# plot results
n.generators_t.p.plot()
n.plot()
1/13:
# plot results
n.generators_t.p.plot()
1/14: n.plot()
 2/1:
import pypsa
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from pyomo.environ import Constraint
 2/2:
override_component_attrs = pypsa.descriptors.Dict(
    {k: v.copy() for k, v in pypsa.components.component_attrs.items()}
)
override_component_attrs["Link"].loc["bus2"] = [
    "string",
    np.nan,
    np.nan,
    "2nd bus",
    "Input (optional)",
]
override_component_attrs["Link"].loc["efficiency2"] = [
    "static or series",
    "per unit",
    1.0,
    "2nd bus efficiency",
    "Input (optional)",
]
override_component_attrs["Link"].loc["p2"] = [
    "series",
    "MW",
    0.0,
    "2nd bus output",
    "Output",
]
 2/3:
network = pypsa.Network(override_component_attrs=override_component_attrs)
network.set_snapshots(pd.date_range("2016-01-01 00:00", "2016-01-01 03:00", freq="H"))
 2/4: override_component_attrs
 2/5: override_component_attrs["Link"]
 2/6: override_component_attrs.keys()
 2/7:
network = pypsa.Network(override_component_attrs=override_component_attrs)
network.set_snapshots(pd.date_range("2016-01-01 00:00", "2016-01-01 03:00", freq="H"))
 2/8:
network.add("Carrier", "reservoir")
network.add("Carrier", "rain")

network.add("Bus", "0", carrier="AC")
network.add("Bus", "1", carrier="AC")

network.add("Bus", "0 reservoir", carrier="reservoir")
network.add("Bus", "1 reservoir", carrier="reservoir")


network.add(
    "Generator",
    "rain",
    bus="0 reservoir",
    carrier="rain",
    p_nom=1000,
    p_max_pu=[0.0, 0.2, 0.7, 0.4],
)


network.add("Load", "0 load", bus="0", p_set=20.0)

network.add("Load", "1 load", bus="1", p_set=30.0)
 2/9:
network.add(
    "Link",
    "spillage",
    bus0="0 reservoir",
    bus1="1 reservoir",
    efficiency=0.5,
    p_nom_extendable=True,
)


# water from turbine also goes into next reservoir
network.add(
    "Link",
    "0 turbine",
    bus0="0 reservoir",
    bus1="0",
    bus2="1 reservoir",
    efficiency=0.9,
    efficiency2=0.5,
    capital_cost=1000,
    p_nom_extendable=True,
)

network.add(
    "Link",
    "1 turbine",
    bus0="1 reservoir",
    bus1="1",
    efficiency=0.9,
    capital_cost=1000,
    p_nom_extendable=True,
)


network.add(
    "Store", "0 reservoir", bus="0 reservoir", e_cyclic=True, e_nom_extendable=True
)

network.add(
    "Store", "1 reservoir", bus="1 reservoir", e_cyclic=True, e_nom_extendable=True
)
2/10:
network.lopf(network.snapshots)
print("Objective:", network.objective)
2/11:
network.lopf(network.snapshots, solver_name="gurobi")
print("Objective:", network.objective)
2/12:
network.generators_t.p.plot.area(figsize=(9, 4))
plt.tight_layout()
2/13:
network.links_t.p0.plot(figsize=(9, 4), lw=3)
plt.tight_layout()
2/14:
network.links_t.p1.plot(figsize=(9, 4), lw=3)
plt.tight_layout()
2/15:
network.links_t.p2.plot(figsize=(9, 4), lw=3)
plt.tight_layout()
2/16: pd.DataFrame({attr: network.stores_t[attr]["0 reservoir"] for attr in ["p", "e"]})
2/17: pd.DataFrame({attr: network.stores_t[attr]["1 reservoir"] for attr in ["p", "e"]})
 3/1:
import pypsa
import numpy as np
import matplotlib.pyplot as plt
 3/2:
override_component_attrs = pypsa.descriptors.Dict(
    {k: v.copy() for k, v in pypsa.components.component_attrs.items()}
)
override_component_attrs["Link"].loc["bus2"] = [
    "string",
    np.nan,
    np.nan,
    "2nd bus",
    "Input (optional)",
]
override_component_attrs["Link"].loc["bus3"] = [
    "string",
    np.nan,
    np.nan,
    "3rd bus",
    "Input (optional)",
]
override_component_attrs["Link"].loc["efficiency2"] = [
    "static or series",
    "per unit",
    1.0,
    "2nd bus efficiency",
    "Input (optional)",
]
override_component_attrs["Link"].loc["efficiency3"] = [
    "static or series",
    "per unit",
    1.0,
    "3rd bus efficiency",
    "Input (optional)",
]
override_component_attrs["Link"].loc["p2"] = [
    "series",
    "MW",
    0.0,
    "2nd bus output",
    "Output",
]
override_component_attrs["Link"].loc["p3"] = [
    "series",
    "MW",
    0.0,
    "3rd bus output",
    "Output",
]
 3/3:
n = pypsa.Network(override_component_attrs=override_component_attrs)
n.set_snapshots(range(10))
 3/4:
n.add("Bus", "bus")
n.add("Load", "load", bus="bus", p_set=1.0)
 3/5:
n.add("Bus", "transport")
n.add("Load", "transport", bus="transport", p_set=1.0)


n.add("Bus", "diesel")


n.add("Store", "diesel", bus="diesel", e_cyclic=True, e_nom=1000.0)
 3/6:
n.add("Bus", "hydrogen")

n.add("Store", "hydrogen", bus="hydrogen", e_cyclic=True, e_nom=1000.0)

# n.add("Load","hydrogen",
#      bus="hydrogen",
#      p_set=1.)

n.add("Link", "electrolysis", p_nom=2.0, efficiency=0.8, bus0="bus", bus1="hydrogen")
 3/7:
n.add(
    "Link",
    "FT",
    p_nom=4,
    bus0="hydrogen",
    bus1="diesel",
    bus2="co2 stored",
    efficiency=1.0,
    efficiency2=-1,
)

# minus sign because opposite to how fossil fuels used:
# CH4 burning puts CH4 down, atmosphere up
n.add("Carrier", "co2", co2_emissions=-1.0)

# this tracks CO2 in the atmosphere
n.add("Bus", "co2 atmosphere", carrier="co2")

# NB: can also be negative
n.add("Store", "co2 atmosphere", e_nom=1000, e_min_pu=-1, bus="co2 atmosphere")

# this tracks CO2 stored, e.g. underground
n.add("Bus", "co2 stored")

# NB: can also be negative
n.add("Store", "co2 stored", e_nom=1000, e_min_pu=-1, bus="co2 stored")
 3/8:
n.add(
    "Link",
    "DAC",
    bus0="bus",
    bus1="co2 stored",
    bus2="co2 atmosphere",
    efficiency=1,
    efficiency2=-1,
    p_nom=5.0,
)
 3/9:
n.add(
    "Link",
    "diesel car",
    bus0="diesel",
    bus1="transport",
    bus2="co2 atmosphere",
    efficiency=1.0,
    efficiency2=1.0,
    p_nom=2.0,
)

n.add("Bus", "gas")

n.add("Store", "gas", e_initial=50, e_nom=50, marginal_cost=20, bus="gas")

n.add(
    "Link",
    "OCGT",
    bus0="gas",
    bus1="bus",
    bus2="co2 atmosphere",
    p_nom_extendable=True,
    efficiency=0.5,
    efficiency2=1,
)


n.add(
    "Link",
    "OCGT+CCS",
    bus0="gas",
    bus1="bus",
    bus2="co2 stored",
    bus3="co2 atmosphere",
    p_nom_extendable=True,
    efficiency=0.4,
    efficiency2=0.9,
    efficiency3=0.1,
)
3/10:
biomass_marginal_cost = [20.0, 50.0]
biomass_stored = [40.0, 15.0]

for i in range(2):
    n.add("Bus", "biomass" + str(i))

    n.add(
        "Store",
        "biomass" + str(i),
        bus="biomass" + str(i),
        e_nom_extendable=True,
        marginal_cost=biomass_marginal_cost[i],
        e_nom=biomass_stored[i],
        e_initial=biomass_stored[i],
    )

    # simultaneously empties and refills co2 atmosphere
    n.add(
        "Link",
        "biomass" + str(i),
        bus0="biomass" + str(i),
        bus1="bus",
        p_nom_extendable=True,
        efficiency=0.5,
    )

    n.add(
        "Link",
        "biomass+CCS" + str(i),
        bus0="biomass" + str(i),
        bus1="bus",
        bus2="co2 stored",
        bus3="co2 atmosphere",
        p_nom_extendable=True,
        efficiency=0.4,
        efficiency2=1.0,
        efficiency3=-1,
    )


# can go to -50, but at some point can't generate enough electricity for DAC and demand
target = -50
3/11:
n.add(
    "GlobalConstraint",
    "co2_limit",
    sense="<=",
    carrier_attribute="co2_emissions",
    constant=target,
)
3/12: n.lopf();
3/13: n.lopf(solver_name="gurobi");
3/14:
n.stores_t.e.plot(figsize=(9, 7), lw=3)
plt.tight_layout()
3/15:
n.links_t.p0[["biomass+CCS0", "biomass+CCS1", "OCGT+CCS", "DAC"]].plot(
    subplots=True, figsize=(9, 7)
)
plt.tight_layout()
3/16: n.stores_t.e[["co2 stored", "co2 atmosphere", "gas", "diesel"]].sum(axis=1)
 4/1:
import pypsa
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import cartopy.crs as ccrs

%matplotlib inline
 4/2: network = pypsa.examples.scigrid_de(from_master=True)
 4/3:
fig, ax = plt.subplots(
    1, 1, subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(8, 8)
)

load_distribution = (
    network.loads_t.p_set.loc[network.snapshots[0]].groupby(network.loads.bus).sum()
)
network.plot(bus_sizes=1e-5 * load_distribution, ax=ax, title="Load distribution");
 4/4: network.generators.groupby("carrier")["p_nom"].sum()
 4/5: network.storage_units.groupby("carrier")["p_nom"].sum()
 4/6: network.df
 4/7: network.df()
 4/8: network.df?
 4/9: network.df("Load")
4/10: network.df("Generator")
4/11: network.transformers
4/12: network.transformers
4/13: network.transformers
4/14: network.transformers
4/15: network.transformers
4/16: network.transformers
4/17: network.transformers
4/18:
techs = ["Gas", "Brown Coal", "Hard Coal", "Wind Offshore", "Wind Onshore", "Solar"]

n_graphs = len(techs)
n_cols = 3
if n_graphs % n_cols == 0:
    n_rows = n_graphs // n_cols
else:
    n_rows = n_graphs // n_cols + 1


fig, axes = plt.subplots(
    nrows=n_rows, ncols=n_cols, subplot_kw={"projection": ccrs.EqualEarth()}
)
size = 6
fig.set_size_inches(size * n_cols, size * n_rows)

for i, tech in enumerate(techs):
    i_row = i // n_cols
    i_col = i % n_cols

    ax = axes[i_row, i_col]
    gens = network.generators[network.generators.carrier == tech]
    gen_distribution = (
        gens.groupby("bus").sum()["p_nom"].reindex(network.buses.index, fill_value=0.0)
    )
    network.plot(ax=ax, bus_sizes=2e-5 * gen_distribution)
    ax.set_title(tech)
fig.tight_layout()
4/19:
techs = ["Gas", "Brown Coal", "Hard Coal", "Wind Offshore", "Wind Onshore", "Solar", "Nuclear"]

n_graphs = len(techs)
n_cols = 3
if n_graphs % n_cols == 0:
    n_rows = n_graphs // n_cols
else:
    n_rows = n_graphs // n_cols + 1


fig, axes = plt.subplots(
    nrows=n_rows, ncols=n_cols, subplot_kw={"projection": ccrs.EqualEarth()}
)
size = 6
fig.set_size_inches(size * n_cols, size * n_rows)

for i, tech in enumerate(techs):
    i_row = i // n_cols
    i_col = i % n_cols

    ax = axes[i_row, i_col]
    gens = network.generators[network.generators.carrier == tech]
    gen_distribution = (
        gens.groupby("bus").sum()["p_nom"].reindex(network.buses.index, fill_value=0.0)
    )
    network.plot(ax=ax, bus_sizes=2e-5 * gen_distribution)
    ax.set_title(tech)
fig.tight_layout()
4/20:
contingency_factor = 0.7
network.lines.s_max_pu = contingency_factor
4/21: network.lines.loc[["316", "527", "602"], "s_nom"] = 1715
4/22: network.lines
4/23: network.lines.loc[["316", "527", "602"]]
4/24: network.lines["s_nom"]
4/25:
group_size = 4
network.storage_units.state_of_charge_initial = 0.0

for i in range(int(24 / group_size)):
    # set the initial state of charge based on previous round
    if i:
        network.storage_units.state_of_charge_initial = (
            network.storage_units_t.state_of_charge.loc[
                network.snapshots[group_size * i - 1]
            ]
        )
    network.lopf(
        network.snapshots[group_size * i : group_size * i + group_size],
        solver_name="gurobi",
        pyomo=False,
    )
4/26:
p_by_carrier = network.generators_t.p.groupby(network.generators.carrier, axis=1).sum()
p_by_carrier.drop(
    (p_by_carrier.max()[p_by_carrier.max() < 1700.0]).index, axis=1, inplace=True
)
p_by_carrier.columns
4/27:
colors = {
    "Brown Coal": "brown",
    "Hard Coal": "k",
    "Nuclear": "r",
    "Run of River": "green",
    "Wind Onshore": "blue",
    "Solar": "yellow",
    "Wind Offshore": "cyan",
    "Waste": "orange",
    "Gas": "orange",
}
# reorder
cols = [
    "Nuclear",
    "Run of River",
    "Brown Coal",
    "Hard Coal",
    "Gas",
    "Wind Offshore",
    "Wind Onshore",
    "Solar",
]
p_by_carrier = p_by_carrier[cols]
4/28:
c = [colors[col] for col in p_by_carrier.columns]

fig, ax = plt.subplots(figsize=(12, 6))
(p_by_carrier / 1e3).plot(kind="area", ax=ax, linewidth=4, color=c, alpha=0.7)
ax.legend(ncol=4, loc="upper left")
ax.set_ylabel("GW")
ax.set_xlabel("")
fig.tight_layout()
4/29:
fig, ax = plt.subplots(figsize=(12, 6))

p_storage = network.storage_units_t.p.sum(axis=1)
state_of_charge = network.storage_units_t.state_of_charge.sum(axis=1)
p_storage.plot(label="Pumped hydro dispatch", ax=ax, linewidth=3)
state_of_charge.plot(label="State of charge", ax=ax, linewidth=3)

ax.legend()
ax.grid()
ax.set_ylabel("MWh")
ax.set_xlabel("")
fig.tight_layout()
4/30: now = network.snapshots[4]
4/31:
loading = network.lines_t.p0.loc[now] / network.lines.s_nom
loading.describe()
4/32:
fig, ax = plt.subplots(subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(9, 9))
network.plot(
    ax=ax,
    line_colors=abs(loading),
    line_cmap=plt.cm.jet,
    title="Line loading",
    bus_sizes=1e-3,
    bus_alpha=0.7,
)
fig.tight_layout();
4/33: network.buses_t.marginal_price.loc[now].describe()
4/34:
fig, ax = plt.subplots(subplot_kw={"projection": ccrs.PlateCarree()}, figsize=(8, 8))

plt.hexbin(
    network.buses.x,
    network.buses.y,
    gridsize=20,
    C=network.buses_t.marginal_price.loc[now],
    cmap=plt.cm.jet,
    zorder=3,
)
network.plot(ax=ax, line_widths=pd.Series(0.5, network.lines.index), bus_sizes=0)

cb = plt.colorbar(location="bottom")
cb.set_label("Locational Marginal Price (EUR/MWh)")
fig.tight_layout()
4/35:
carrier = "Wind Onshore"

capacity = network.generators.groupby("carrier").sum().at[carrier, "p_nom"]
p_available = network.generators_t.p_max_pu.multiply(network.generators["p_nom"])
p_available_by_carrier = p_available.groupby(network.generators.carrier, axis=1).sum()
p_curtailed_by_carrier = p_available_by_carrier - p_by_carrier
4/36:
p_df = pd.DataFrame(
    {
        carrier + " available": p_available_by_carrier[carrier],
        carrier + " dispatched": p_by_carrier[carrier],
        carrier + " curtailed": p_curtailed_by_carrier[carrier],
    }
)

p_df[carrier + " capacity"] = capacity
4/37: p_df["Wind Onshore curtailed"][p_df["Wind Onshore curtailed"] < 0.0] = 0.0
4/38:
fig, ax = plt.subplots(figsize=(10, 4))
p_df[[carrier + " dispatched", carrier + " curtailed"]].plot(
    kind="area", ax=ax, linewidth=3
)
p_df[[carrier + " available", carrier + " capacity"]].plot(ax=ax, linewidth=3)

ax.set_xlabel("")
ax.set_ylabel("Power [MW]")
ax.set_ylim([0, 40000])
ax.legend()
fig.tight_layout()
4/39:
network.generators_t.p_set = network.generators_t.p
network.storage_units_t.p_set = network.storage_units_t.p
4/40:
network.generators.control = "PV"

# Need some PQ buses so that Jacobian doesn't break
f = network.generators[network.generators.bus == "492"]
network.generators.loc[f.index, "control"] = "PQ"
4/41: info = network.pf();
4/42: (~info.converged).any().any()
4/43: (network.lines_t.p0.loc[now] / network.lines.s_nom).describe()
4/44:
df = network.lines.copy()

for b in ["bus0", "bus1"]:
    df = pd.merge(
        df, network.buses_t.v_ang.loc[[now]].T, how="left", left_on=b, right_index=True
    )

s = df[str(now) + "_x"] - df[str(now) + "_y"]

(s * 180 / np.pi).describe()
4/45:
fig, ax = plt.subplots(subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(9, 9))

q = network.buses_t.q.loc[now]
bus_colors = pd.Series("r", network.buses.index)
bus_colors[q < 0.0] = "b"

network.plot(
    bus_sizes=1e-4 * abs(q),
    ax=ax,
    bus_colors=bus_colors,
    title="Reactive power feed-in (red=+ve, blue=-ve)",
);
4/46: network
4/47: network.generators
4/48: network.generators.columns
 5/1:
import pypsa
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import cartopy.crs as ccrs

%matplotlib inline
 5/2: network = pypsa.examples.scigrid_de(from_master=True)
 5/3:
fig, ax = plt.subplots(
    1, 1, subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(8, 8)
)

load_distribution = (
    network.loads_t.p_set.loc[network.snapshots[0]].groupby(network.loads.bus).sum()
)
network.plot(bus_sizes=1e-5 * load_distribution, ax=ax, title="Load distribution");
 5/4: network.generators.groupby("carrier")["p_nom"].sum()
 5/5: network.storage_units.groupby("carrier")["p_nom"].sum()
 5/6:
techs = ["Gas", "Brown Coal", "Hard Coal", "Wind Offshore", "Wind Onshore", "Solar", "Nuclear"]

n_graphs = len(techs)
n_cols = 3
if n_graphs % n_cols == 0:
    n_rows = n_graphs // n_cols
else:
    n_rows = n_graphs // n_cols + 1


fig, axes = plt.subplots(
    nrows=n_rows, ncols=n_cols, subplot_kw={"projection": ccrs.EqualEarth()}
)
size = 6
fig.set_size_inches(size * n_cols, size * n_rows)

for i, tech in enumerate(techs):
    i_row = i // n_cols
    i_col = i % n_cols

    ax = axes[i_row, i_col]
    gens = network.generators[network.generators.carrier == tech]
    gen_distribution = (
        gens.groupby("bus").sum()["p_nom"].reindex(network.buses.index, fill_value=0.0)
    )
    network.plot(ax=ax, bus_sizes=2e-5 * gen_distribution)
    ax.set_title(tech)
fig.tight_layout()
 5/7:
contingency_factor = 0.7
network.lines.s_max_pu = contingency_factor
 5/8: network.lines.loc[["316", "527", "602"], "s_nom"] = 1715
 5/9: network.lines.loc[["316", "527", "602"]]
5/10:
group_size = 4
network.storage_units.state_of_charge_initial = 0.0

for i in range(int(24 / group_size)):
    # set the initial state of charge based on previous round
    if i:
        network.storage_units.state_of_charge_initial = (
            network.storage_units_t.state_of_charge.loc[
                network.snapshots[group_size * i - 1]
            ]
        )
    network.lopf(
        network.snapshots[group_size * i : group_size * i + group_size],
        solver_name="gurobi",
        pyomo=False,
    )
5/11:
p_by_carrier = network.generators_t.p.groupby(network.generators.carrier, axis=1).sum()
p_by_carrier.drop(
    (p_by_carrier.max()[p_by_carrier.max() < 1700.0]).index, axis=1, inplace=True
)
p_by_carrier.columns
5/12:
colors = {
    "Brown Coal": "brown",
    "Hard Coal": "k",
    "Nuclear": "r",
    "Run of River": "green",
    "Wind Onshore": "blue",
    "Solar": "yellow",
    "Wind Offshore": "cyan",
    "Waste": "orange",
    "Gas": "orange",
}
# reorder
cols = [
    "Nuclear",
    "Run of River",
    "Brown Coal",
    "Hard Coal",
    "Gas",
    "Wind Offshore",
    "Wind Onshore",
    "Solar",
]
p_by_carrier = p_by_carrier[cols]
5/13:
c = [colors[col] for col in p_by_carrier.columns]

fig, ax = plt.subplots(figsize=(12, 6))
(p_by_carrier / 1e3).plot(kind="area", ax=ax, linewidth=4, color=c, alpha=0.7)
ax.legend(ncol=4, loc="upper left")
ax.set_ylabel("GW")
ax.set_xlabel("")
fig.tight_layout()
5/14:
fig, ax = plt.subplots(figsize=(12, 6))

p_storage = network.storage_units_t.p.sum(axis=1)
state_of_charge = network.storage_units_t.state_of_charge.sum(axis=1)
p_storage.plot(label="Pumped hydro dispatch", ax=ax, linewidth=3)
state_of_charge.plot(label="State of charge", ax=ax, linewidth=3)

ax.legend()
ax.grid()
ax.set_ylabel("MWh")
ax.set_xlabel("")
fig.tight_layout()
5/15: now = network.snapshots[4]
5/16:
loading = network.lines_t.p0.loc[now] / network.lines.s_nom
loading.describe()
5/17:
fig, ax = plt.subplots(subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(9, 9))
network.plot(
    ax=ax,
    line_colors=abs(loading),
    line_cmap=plt.cm.jet,
    title="Line loading",
    bus_sizes=1e-3,
    bus_alpha=0.7,
)
fig.tight_layout();
5/18: network.buses_t.marginal_price.loc[now].describe()
5/19:
fig, ax = plt.subplots(subplot_kw={"projection": ccrs.PlateCarree()}, figsize=(8, 8))

plt.hexbin(
    network.buses.x,
    network.buses.y,
    gridsize=20,
    C=network.buses_t.marginal_price.loc[now],
    cmap=plt.cm.jet,
    zorder=3,
)
network.plot(ax=ax, line_widths=pd.Series(0.5, network.lines.index), bus_sizes=0)

cb = plt.colorbar(location="bottom")
cb.set_label("Locational Marginal Price (EUR/MWh)")
fig.tight_layout()
5/20:
carrier = "Wind Onshore"

capacity = network.generators.groupby("carrier").sum().at[carrier, "p_nom"]
p_available = network.generators_t.p_max_pu.multiply(network.generators["p_nom"])
p_available_by_carrier = p_available.groupby(network.generators.carrier, axis=1).sum()
p_curtailed_by_carrier = p_available_by_carrier - p_by_carrier
5/21:
p_df = pd.DataFrame(
    {
        carrier + " available": p_available_by_carrier[carrier],
        carrier + " dispatched": p_by_carrier[carrier],
        carrier + " curtailed": p_curtailed_by_carrier[carrier],
    }
)

p_df[carrier + " capacity"] = capacity
5/22: p_df["Wind Onshore curtailed"][p_df["Wind Onshore curtailed"] < 0.0] = 0.0
5/23:
fig, ax = plt.subplots(figsize=(10, 4))
p_df[[carrier + " dispatched", carrier + " curtailed"]].plot(
    kind="area", ax=ax, linewidth=3
)
p_df[[carrier + " available", carrier + " capacity"]].plot(ax=ax, linewidth=3)

ax.set_xlabel("")
ax.set_ylabel("Power [MW]")
ax.set_ylim([0, 40000])
ax.legend()
fig.tight_layout()
5/24:
network.generators_t.p_set = network.generators_t.p
network.storage_units_t.p_set = network.storage_units_t.p
5/25:
network.generators.control = "PV"

# Need some PQ buses so that Jacobian doesn't break
f = network.generators[network.generators.bus == "492"]
network.generators.loc[f.index, "control"] = "PQ"
5/26: info = network.pf();
5/27: (~info.converged).any().any()
5/28: (network.lines_t.p0.loc[now] / network.lines.s_nom).describe()
5/29:
df = network.lines.copy()

for b in ["bus0", "bus1"]:
    df = pd.merge(
        df, network.buses_t.v_ang.loc[[now]].T, how="left", left_on=b, right_index=True
    )

s = df[str(now) + "_x"] - df[str(now) + "_y"]

(s * 180 / np.pi).describe()
5/30:
fig, ax = plt.subplots(subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(9, 9))

q = network.buses_t.q.loc[now]
bus_colors = pd.Series("r", network.buses.index)
bus_colors[q < 0.0] = "b"

network.plot(
    bus_sizes=1e-4 * abs(q),
    ax=ax,
    bus_colors=bus_colors,
    title="Reactive power feed-in (red=+ve, blue=-ve)",
);
5/31: network
5/32: network.generators.columns
5/33: network
5/34: network.generators_t
5/35: network.generators_t.keys()
5/36: network.generators_t["p_set"]
5/37: network.snapshots
5/38: network.srid
5/39: network.now
5/40: network.graph()
5/41: network.sub_networks
5/42: network.buses
5/43: network.global_constraints
5/44: network.storage_units
5/45: network.stores
5/46: network.loads
5/47: network.shunt_impedances
5/48: network.line_types
5/49: network.transformer_types
5/50: network.transformers
5/51: network.transformers["type"]
5/52: network.transformers["type"].unique()
5/53: network.links
5/54: network.export_to_csv_folder?
5/55: network.export_to_csv_folder("germany_pypsa")
5/56: !ls
 7/1: import pypsa
 7/2:
import pypsa
import numpy as np
 7/3:
### Preliminaries
#
# Here libraries are imported and data is defined.

import numpy as np
import pypsa
 7/4:
# marginal costs in EUR/MWh
marginal_costs = {"Wind": 0, "Hydro": 0, "Coal": 30, "Gas": 60, "Oil": 80}
 7/5:
# power plant capacities (nominal powers in MW) in each country (not necessarily realistic)
power_plant_p_nom = {
    "South Africa": {"Coal": 35000, "Wind": 3000, "Gas": 8000, "Oil": 2000},
    "Mozambique": {
        "Hydro": 1200,
    },
    "Swaziland": {
        "Hydro": 600,
    },
}
 7/6:
# transmission capacities in MW (not necessarily realistic)
transmission = {
    "South Africa": {"Mozambique": 500, "Swaziland": 250},
    "Mozambique": {"Swaziland": 100},
}
 7/7:
# country electrical loads in MW (not necessarily realistic)
loads = {"South Africa": 42000, "Mozambique": 650, "Swaziland": 250}
 7/8:
### Single bidding zone with fixed load, one period
#
# In this example we consider a single market bidding zone, South Africa.
#
# The inelastic load has essentially infinite marginal utility (or higher than the marginal cost of any generator).

country = "South Africa"
 7/9: network = pypsa.Network()
7/10: network.add("Bus", country)
7/11:
for tech in power_plant_p_nom[country]:
    network.add(
        "Generator",
        "{} {}".format(country, tech),
        bus=country,
        p_nom=power_plant_p_nom[country][tech],
        marginal_cost=marginal_costs[tech],
    )
7/12: network.add("Load", "{} load".format(country), bus=country, p_set=loads[country])
7/13:
# Run optimisation to determine market dispatch
network.lopf()
7/14:
# Run optimisation to determine market dispatch
network.lopf()
7/15:
# print the load active power (P) consumption
network.loads_t.p
7/16:
# print the generator active power (P) dispatch
network.generators_t.p
7/17:
# print the clearing price (corresponding to gas)
network.buses_t.marginal_price
7/18: network.constraints
7/19: network.objective
7/20: network.optimize?
7/21: network.model
7/22: network
7/23: pypsa.linopf.prepare_lopf(n)
7/24: pypsa.linopf.prepare_lopf(network)
7/25:
### Two bidding zones connected by transmission, one period
#
# In this example we have bidirectional transmission capacity between two bidding zones. The power transfer is treated as controllable (like an A/NTC (Available/Net Transfer Capacity) or HVDC line). Note that in the physical grid, power flows passively according to the network impedances.
7/26: network = pypsa.Network()
7/27: countries = ["Mozambique", "South Africa"]
7/28:
for country in countries:
    network.add("Bus", country)

    for tech in power_plant_p_nom[country]:
        network.add(
            "Generator",
            "{} {}".format(country, tech),
            bus=country,
            p_nom=power_plant_p_nom[country][tech],
            marginal_cost=marginal_costs[tech],
        )

    network.add("Load", "{} load".format(country), bus=country, p_set=loads[country])

    # add transmission as controllable Link
    if country not in transmission:
        continue

    for other_country in countries:
        if other_country not in transmission[country]:
            continue

        # NB: Link is by default unidirectional, so have to set p_min_pu = -1
        # to allow bidirectional (i.e. also negative) flow
        network.add(
            "Link",
            "{} - {} link".format(country, other_country),
            bus0=country,
            bus1=other_country,
            p_nom=transmission[country][other_country],
            p_min_pu=-1,
        )
7/29: network.lopf()
7/30: network.loads_t.p
7/31: network.generators_t.p
7/32: network.links_t.p0
7/33:
# print the clearing price (corresponding to water in Mozambique and gas in SA)
network.buses_t.marginal_price
7/34:
# link shadow prices
network.links_t.mu_lower
7/35: network = pypsa.Network()
7/36: countries = ["Swaziland", "Mozambique", "South Africa"]
7/37:
for country in countries:
    network.add("Bus", country)

    for tech in power_plant_p_nom[country]:
        network.add(
            "Generator",
            "{} {}".format(country, tech),
            bus=country,
            p_nom=power_plant_p_nom[country][tech],
            marginal_cost=marginal_costs[tech],
        )

    network.add("Load", "{} load".format(country), bus=country, p_set=loads[country])

    # add transmission as controllable Link
    if country not in transmission:
        continue

    for other_country in countries:
        if other_country not in transmission[country]:
            continue

        # NB: Link is by default unidirectional, so have to set p_min_pu = -1
        # to allow bidirectional (i.e. also negative) flow
        network.add(
            "Link",
            "{} - {} link".format(country, other_country),
            bus0=country,
            bus1=other_country,
            p_nom=transmission[country][other_country],
            p_min_pu=-1,
        )
7/38: network.lopf()
7/39: network.loads_t.p
7/40: network.generators_t.p
7/41: network.links_t.p0
7/42:
# print the clearing price (corresponding to hydro in S and M, and gas in SA)
network.buses_t.marginal_price
7/43:
# link shadow prices
network.links_t.mu_lower
7/44: country = "South Africa"
7/45: network = pypsa.Network()
7/46: network.add("Bus", country)
7/47:
for tech in power_plant_p_nom[country]:
    network.add(
        "Generator",
        "{} {}".format(country, tech),
        bus=country,
        p_nom=power_plant_p_nom[country][tech],
        marginal_cost=marginal_costs[tech],
    )
7/48:
# standard high marginal utility consumers
network.add("Load", "{} load".format(country), bus=country, p_set=loads[country])
7/49:
# add an industrial load as a dummy negative-dispatch generator with marginal utility of 70 EUR/MWh for 8000 MW
network.add(
    "Generator",
    "{} industrial load".format(country),
    bus=country,
    p_max_pu=0,
    p_min_pu=-1,
    p_nom=8000,
    marginal_cost=70,
)
7/50: network.lopf()
7/51: network.loads_t.p
7/52:
# NB only half of industrial load is served, because this maxes out
# Gas. Oil is too expensive with a marginal cost of 80 EUR/MWh
network.generators_t.p
7/53: network.buses_t.marginal_price
7/54: network.loads
7/55: country = "South Africa"
7/56: network = pypsa.Network()
7/57:
# snapshots labelled by [0, 1, 2, 3]
network.set_snapshots(range(4))
7/58: network.add("Bus", country)
7/59:
# p_max_pu is variable for wind
for tech in power_plant_p_nom[country]:
    network.add(
        "Generator",
        "{} {}".format(country, tech),
        bus=country,
        p_nom=power_plant_p_nom[country][tech],
        marginal_cost=marginal_costs[tech],
        p_max_pu=([0.3, 0.6, 0.4, 0.5] if tech == "Wind" else 1),
    )
7/60:
# load which varies over the snapshots
network.add(
    "Load",
    "{} load".format(country),
    bus=country,
    p_set=loads[country] + np.array([0, 1000, 3000, 4000]),
)
7/61:
# specify that we consider all snapshots
network.lopf(network.snapshots)
7/62: network.loads_t.p
7/63: network.generators_t.p
7/64: network.buses_t.marginal_price
7/65: country = "South Africa"
7/66: network = pypsa.Network()
7/67:
# snapshots labelled by [0, 1, 2, 3]
network.set_snapshots(range(4))
7/68: network.add("Bus", country)
7/69:
# p_max_pu is variable for wind
for tech in power_plant_p_nom[country]:
    network.add(
        "Generator",
        "{} {}".format(country, tech),
        bus=country,
        p_nom=power_plant_p_nom[country][tech],
        marginal_cost=marginal_costs[tech],
        p_max_pu=([0.3, 0.6, 0.4, 0.5] if tech == "Wind" else 1),
    )

# load which varies over the snapshots
network.add(
    "Load",
    "{} load".format(country),
    bus=country,
    p_set=loads[country] + np.array([0, 1000, 3000, 4000]),
)

# storage unit to do price arbitrage
network.add(
    "StorageUnit",
    "{} pumped hydro".format(country),
    bus=country,
    p_nom=1000,
    max_hours=6,  # energy storage in terms of hours at full power
)
7/70: network.lopf(network.snapshots)
7/71: network.loads_t.p
7/72: network.generators_t.p
7/73: network.storage_units_t.p
7/74: network.storage_units_t.state_of_charge
7/75: network.buses_t.marginal_price
7/76: network.plot()
7/77: network.iplot()
7/78: network.iplot()
7/79: !conda list | grep nbformat
7/80: network.iplot()
 8/1: import pypsa
 8/2:
import pandas as pd
import plotly.graph_objects as go
 8/3: n = pypsa.Network()
 8/4:
n.add("Bus", "mybus")
n.add("Load", "myload", bus="mybus", p_set=100)
n.add("Generator", "mygen", bus="mybus", p_nom=100, marginal_cost=20)
 8/5:
# load an example network
n = pypsa.examples.ac_dc_meshed()
 8/6:
# run the optimisation
n.lopf(solver_name="gurobi")
 8/7:
# plot results
n.generators_t.p.plot()
 8/8: n.plot()
 8/9:
### Preliminaries
#
# Here libraries are imported and data is defined.

import numpy as np
import pypsa
8/10:
# marginal costs in EUR/MWh
marginal_costs = {"Wind": 0, "Hydro": 0, "Coal": 30, "Gas": 60, "Oil": 80}
8/11:
# power plant capacities (nominal powers in MW) in each country (not necessarily realistic)
power_plant_p_nom = {
    "South Africa": {"Coal": 35000, "Wind": 3000, "Gas": 8000, "Oil": 2000},
    "Mozambique": {
        "Hydro": 1200,
    },
    "Swaziland": {
        "Hydro": 600,
    },
}
8/12:
# transmission capacities in MW (not necessarily realistic)
transmission = {
    "South Africa": {"Mozambique": 500, "Swaziland": 250},
    "Mozambique": {"Swaziland": 100},
}
8/13:
# country electrical loads in MW (not necessarily realistic)
loads = {"South Africa": 42000, "Mozambique": 650, "Swaziland": 250}
8/14: country = "South Africa"
8/15: network = pypsa.Network()
8/16: network.add("Bus", country)
8/17:
for tech in power_plant_p_nom[country]:
    network.add(
        "Generator",
        "{} {}".format(country, tech),
        bus=country,
        p_nom=power_plant_p_nom[country][tech],
        marginal_cost=marginal_costs[tech],
    )
8/18: network.add("Load", "{} load".format(country), bus=country, p_set=loads[country])
8/19:
# Run optimisation to determine market dispatch
network.lopf()
8/20:
# print the load active power (P) consumption
network.loads_t.p
8/21:
# print the generator active power (P) dispatch
network.generators_t.p
8/22:
# print the clearing price (corresponding to gas)
network.buses_t.marginal_price
8/23: network.objective
8/24: network.optimize?
8/25: network.model
8/26: pypsa.linopf.prepare_lopf(network)
8/27: network = pypsa.Network()
8/28: countries = ["Mozambique", "South Africa"]
8/29:
for country in countries:
    network.add("Bus", country)

    for tech in power_plant_p_nom[country]:
        network.add(
            "Generator",
            "{} {}".format(country, tech),
            bus=country,
            p_nom=power_plant_p_nom[country][tech],
            marginal_cost=marginal_costs[tech],
        )

    network.add("Load", "{} load".format(country), bus=country, p_set=loads[country])

    # add transmission as controllable Link
    if country not in transmission:
        continue

    for other_country in countries:
        if other_country not in transmission[country]:
            continue

        # NB: Link is by default unidirectional, so have to set p_min_pu = -1
        # to allow bidirectional (i.e. also negative) flow
        network.add(
            "Link",
            "{} - {} link".format(country, other_country),
            bus0=country,
            bus1=other_country,
            p_nom=transmission[country][other_country],
            p_min_pu=-1,
        )
8/30: network.lopf()
8/31: network.loads_t.p
8/32: network.generators_t.p
8/33: network.links_t.p0
8/34:
# print the clearing price (corresponding to water in Mozambique and gas in SA)
network.buses_t.marginal_price
8/35:
# link shadow prices
network.links_t.mu_lower
8/36: network = pypsa.Network()
8/37: countries = ["Swaziland", "Mozambique", "South Africa"]
8/38:
for country in countries:
    network.add("Bus", country)

    for tech in power_plant_p_nom[country]:
        network.add(
            "Generator",
            "{} {}".format(country, tech),
            bus=country,
            p_nom=power_plant_p_nom[country][tech],
            marginal_cost=marginal_costs[tech],
        )

    network.add("Load", "{} load".format(country), bus=country, p_set=loads[country])

    # add transmission as controllable Link
    if country not in transmission:
        continue

    for other_country in countries:
        if other_country not in transmission[country]:
            continue

        # NB: Link is by default unidirectional, so have to set p_min_pu = -1
        # to allow bidirectional (i.e. also negative) flow
        network.add(
            "Link",
            "{} - {} link".format(country, other_country),
            bus0=country,
            bus1=other_country,
            p_nom=transmission[country][other_country],
            p_min_pu=-1,
        )
8/39: network.lopf()
8/40: network.loads_t.p
8/41: network.generators_t.p
8/42: network.links_t.p0
8/43:
# print the clearing price (corresponding to hydro in S and M, and gas in SA)
network.buses_t.marginal_price
8/44:
# link shadow prices
network.links_t.mu_lower
8/45: country = "South Africa"
8/46: network = pypsa.Network()
8/47: network.add("Bus", country)
8/48:
for tech in power_plant_p_nom[country]:
    network.add(
        "Generator",
        "{} {}".format(country, tech),
        bus=country,
        p_nom=power_plant_p_nom[country][tech],
        marginal_cost=marginal_costs[tech],
    )
8/49:
# standard high marginal utility consumers
network.add("Load", "{} load".format(country), bus=country, p_set=loads[country])
8/50:
# add an industrial load as a dummy negative-dispatch generator with marginal utility of 70 EUR/MWh for 8000 MW
network.add(
    "Generator",
    "{} industrial load".format(country),
    bus=country,
    p_max_pu=0,
    p_min_pu=-1,
    p_nom=8000,
    marginal_cost=70,
)
8/51: network.lopf()
8/52: network.loads_t.p
8/53: network.loads
8/54:
# NB only half of industrial load is served, because this maxes out
# Gas. Oil is too expensive with a marginal cost of 80 EUR/MWh
network.generators_t.p
8/55: network.buses_t.marginal_price
8/56: country = "South Africa"
8/57: network = pypsa.Network()
8/58:
# snapshots labelled by [0, 1, 2, 3]
network.set_snapshots(range(4))
8/59: network.add("Bus", country)
8/60:
# p_max_pu is variable for wind
for tech in power_plant_p_nom[country]:
    network.add(
        "Generator",
        "{} {}".format(country, tech),
        bus=country,
        p_nom=power_plant_p_nom[country][tech],
        marginal_cost=marginal_costs[tech],
        p_max_pu=([0.3, 0.6, 0.4, 0.5] if tech == "Wind" else 1),
    )
8/61:
# load which varies over the snapshots
network.add(
    "Load",
    "{} load".format(country),
    bus=country,
    p_set=loads[country] + np.array([0, 1000, 3000, 4000]),
)
8/62:
# specify that we consider all snapshots
network.lopf(network.snapshots)
8/63: network.loads_t.p
8/64: network.generators_t.p
8/65: network.buses_t.marginal_price
8/66: country = "South Africa"
8/67: network = pypsa.Network()
8/68:
# snapshots labelled by [0, 1, 2, 3]
network.set_snapshots(range(4))
8/69: network.add("Bus", country)
8/70:
# p_max_pu is variable for wind
for tech in power_plant_p_nom[country]:
    network.add(
        "Generator",
        "{} {}".format(country, tech),
        bus=country,
        p_nom=power_plant_p_nom[country][tech],
        marginal_cost=marginal_costs[tech],
        p_max_pu=([0.3, 0.6, 0.4, 0.5] if tech == "Wind" else 1),
    )

# load which varies over the snapshots
network.add(
    "Load",
    "{} load".format(country),
    bus=country,
    p_set=loads[country] + np.array([0, 1000, 3000, 4000]),
)

# storage unit to do price arbitrage
network.add(
    "StorageUnit",
    "{} pumped hydro".format(country),
    bus=country,
    p_nom=1000,
    max_hours=6,  # energy storage in terms of hours at full power
)
8/71: network.lopf(network.snapshots)
8/72: network.loads_t.p
8/73: network.generators_t.p
8/74: network.storage_units_t.p
8/75: network.storage_units_t.state_of_charge
8/76: network.buses_t.marginal_price
8/77: network.iplot()
8/78: !conda list | grep nbformat
8/79:
import pypsa
import re
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import pandas as pd
from pypsa.networkclustering import get_clustering_from_busmap, busmap_by_kmeans
8/80:
n = pypsa.examples.scigrid_de()
n.lines["type"] = np.nan  # delete the 'type' specifications to make this example easier
8/81:
groups = n.buses.operator.apply(lambda x: re.split(" |,|;", x)[0])
busmap = groups.where(groups != "", n.buses.index)
8/82: n.buses
8/83: groups
8/84: C = get_clustering_from_busmap(n, busmap)
8/85: nc = C.network
8/86:
fig, (ax, ax1) = plt.subplots(
    1, 2, subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(12, 12)
)
plot_kwrgs = dict(bus_sizes=1e-3, line_widths=0.5)
n.plot(ax=ax, title="original", **plot_kwrgs)
nc.plot(ax=ax1, title="clustered by operator", **plot_kwrgs)
fig.tight_layout()
8/87: groups.unique()
8/88: groups.value_counts()
8/89: groups.value_counts().head()
8/90: # Cluster by k-means
8/91: weighting = pd.Series(1, n.buses.index)
8/92: weighting
8/93: busmap2 = busmap_by_kmeans(n, bus_weightings=weighting, n_clusters=50)
8/94: busmap2 = busmap_by_kmeans(n, bus_weightings=weighting, n_clusters=50)
8/95: busmap2 = busmap_by_kmeans(n, bus_weightings=weighting, n_clusters=50)
8/96:
fig, (ax, ax1) = plt.subplots(
    1, 2, subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(12, 12)
)
plot_kwrgs = dict(bus_sizes=1e-3, line_widths=0.5)
n.plot(ax=ax, title="original", **plot_kwrgs)
nc2.plot(ax=ax1, title="clustered by kmeans", **plot_kwrgs)
fig.tight_layout()
8/97:
C2 = get_clustering_from_busmap(n, busmap2)
nc2 = C2.network
8/98:
fig, (ax, ax1) = plt.subplots(
    1, 2, subplot_kw={"projection": ccrs.EqualEarth()}, figsize=(12, 12)
)
plot_kwrgs = dict(bus_sizes=1e-3, line_widths=0.5)
n.plot(ax=ax, title="original", **plot_kwrgs)
nc2.plot(ax=ax1, title="clustered by kmeans", **plot_kwrgs)
fig.tight_layout()
8/99: nc2.buses
8/100: nc2.generators
8/101: nc2.lines
 9/1:
import pypsa
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
 9/2:
n = pypsa.Network()
years = [2020, 2030, 2040, 2050]
freq = "24"

snapshots = pd.DatetimeIndex([])
for year in years:
    period = pd.date_range(
        start="{}-01-01 00:00".format(year),
        freq="{}H".format(freq),
        periods=8760 / float(freq),
    )
    snapshots = snapshots.append(period)

# convert to multiindex and assign to network
n.snapshots = pd.MultiIndex.from_arrays([snapshots.year, snapshots])
n.investment_periods = years

n.snapshot_weightings
 9/3: n.investment_periods
 9/4:
n.investment_period_weightings["years"] = list(np.diff(years)) + [10]

r = 0.01
T = 0
for period, nyears in n.investment_period_weightings.years.items():
    discounts = [(1 / (1 + r) ** t) for t in range(T, T + nyears)]
    n.investment_period_weightings.at[period, "objective"] = sum(discounts)
    T += nyears
n.investment_period_weightings
 9/5:
for i in range(3):
    n.add("Bus", "bus {}".format(i))

# add three lines in a ring
n.add(
    "Line",
    "line 0->1",
    bus0="bus 0",
    bus1="bus 1",
)

n.add(
    "Line",
    "line 1->2",
    bus0="bus 1",
    bus1="bus 2",
    capital_cost=10,
    build_year=2030,
)

n.add(
    "Line",
    "line 2->0",
    bus0="bus 2",
    bus1="bus 0",
)

n.lines["x"] = 0.0001
n.lines["s_nom_extendable"] = True
 9/6: n.lines
 9/7:
# add some generators
p_nom_max = pd.Series(
    (np.random.uniform() for sn in range(len(n.snapshots))),
    index=n.snapshots,
    name="generator ext 2020",
)

# renewable (can operate 2020, 2030)
n.add(
    "Generator",
    "generator ext 0 2020",
    bus="bus 0",
    p_nom=50,
    build_year=2020,
    lifetime=20,
    marginal_cost=2,
    capital_cost=1,
    p_max_pu=p_nom_max,
    carrier="solar",
    p_nom_extendable=True,
)

# can operate 2040, 2050
n.add(
    "Generator",
    "generator ext 0 2040",
    bus="bus 0",
    p_nom=50,
    build_year=2040,
    lifetime=11,
    marginal_cost=25,
    capital_cost=10,
    carrier="OCGT",
    p_nom_extendable=True,
)

# can operate in 2040
n.add(
    "Generator",
    "generator fix 1 2040",
    bus="bus 1",
    p_nom=50,
    build_year=2040,
    lifetime=10,
    carrier="CCGT",
    marginal_cost=20,
    capital_cost=1,
)

n.generators
 9/8:
n.add(
    "StorageUnit",
    "storageunit non-cyclic 2030",
    bus="bus 2",
    p_nom=0,
    capital_cost=2,
    build_year=2030,
    lifetime=21,
    cyclic_state_of_charge=False,
    p_nom_extendable=False,
)

n.add(
    "StorageUnit",
    "storageunit periodic 2020",
    bus="bus 2",
    p_nom=0,
    capital_cost=1,
    build_year=2020,
    lifetime=21,
    cyclic_state_of_charge=True,
    cyclic_state_of_charge_per_period=True,
    p_nom_extendable=True,
)

n.storage_units
 9/9:
load_var = pd.Series(
    100 * np.random.rand(len(n.snapshots)), index=n.snapshots, name="load"
)
n.add("Load", "load 2", bus="bus 2", p_set=load_var)

load_fix = pd.Series(75, index=n.snapshots, name="load")
n.add("Load", "load 1", bus="bus 1", p_set=load_fix)
9/10: n.loads_t.p_set
9/11: n.lopf(pyomo=False, multi_investment_periods=True)
9/12:
c = "Generator"
df = pd.concat(
    {
        period: n.get_active_assets(c, period) * n.df(c).p_nom_opt
        for period in n.investment_periods
    },
    axis=1,
)
df.T.plot.bar(
    stacked=True,
    edgecolor="white",
    width=1,
    ylabel="Capacity",
    xlabel="Investment Period",
    rot=0,
    figsize=(10, 5),
)
plt.tight_layout()
9/13:
df = n.generators_t.p.sum(level=0).T
df.T.plot.bar(
    stacked=True,
    edgecolor="white",
    width=1,
    ylabel="Generation",
    xlabel="Investment Period",
    rot=0,
    figsize=(10, 5),
)
10/1:
import pypsa, os
import cartopy.crs as ccrs
import matplotlib.pyplot as plt
import pandas as pd

import warnings
from shapely.errors import ShapelyDeprecationWarning

warnings.filterwarnings("ignore", category=ShapelyDeprecationWarning)
plt.rc("figure", figsize=(10, 8))
10/2:
n = pypsa.examples.ac_dc_meshed(from_master=True)
n.lopf()
10/3: gen = n.generators.assign(g=n.generators_t.p.mean()).groupby(["bus", "carrier"]).g.sum()
10/4:
# links are not displayed for prettier output ('link_widths=0')
n.plot(
    bus_sizes=gen / 5e3,
    bus_colors={"gas": "indianred", "wind": "midnightblue"},
    margin=0.5,
    flow="mean",
    line_widths=0.1,
    link_widths=0,
)
plt.show()
10/5:
# links are not displayed for prettier output ('link_widths=0')
n.plot(
    bus_sizes=gen / 5e3,
    bus_colors={"gas": "indianred", "wind": "midnightblue"},
    margin=0.5,
    flow="mean",
    line_widths=0.1,
    link_widths=0,
    projection=ccrs.EqualEarth(),
    color_geomap=True,
)
plt.show()
10/6: flow = pd.Series(10, index=n.branches().index)
10/7: flow
10/8:
# links are not displayed for prettier output ('link_widths=0')
n.plot(
    bus_sizes=gen / 5e3,
    bus_colors={"gas": "indianred", "wind": "midnightblue"},
    margin=0.5,
    flow=flow,
    line_widths=2.7,
    link_widths=0,
    projection=ccrs.EqualEarth(),
    color_geomap=True,
)
plt.show()
10/9:
# Pandas series with MultiIndex
# links are not displayed for prettier output ('link_widths=0')
collection = n.plot(
    bus_sizes=gen / 5e3,
    bus_colors={"gas": "indianred", "wind": "midnightblue"},
    margin=0.5,
    flow=flow,
    line_widths=2.7,
    link_widths=0,
    projection=ccrs.EqualEarth(),
    color_geomap=True,
    line_colors=n.lines_t.p0.mean().abs(),
)

plt.colorbar(collection[2], fraction=0.04, pad=0.004, label="Flow in MW")
plt.show()
13/1: link = "https://www.nve.no/energi/energisystem/vannkraft/vannkraftdatabase/#"
13/2: import pandas as pd
13/3: df = pd.read_csv(link)
13/4: pd.read_csv?
13/5: df = pd.read_csv(link, skiprows=3)
13/6: df = pd.read_csv(link, skiprows=2)
13/7: df = pd.read_csv(link, skiprows=2, sep=";")
13/8: df = pd.read_csv(link, skiprows=3, sep=";")
13/9: df = pd.read_csv(link, skiprows=3)#, sep=";")
13/10: df = pd.read_csv(link, skiprows=3, on_bad_lines="warn")#, sep=";")
13/11: df.head()
13/12: link = "https://www.nve.no/umbraco/api/Powerplant/GetHydroPowerPlantsInOperation"
13/13: pd.read_json(link)
13/14: df.head()
13/15: df.head()
13/16:
link = "https://www.nve.no/umbraco/api/Powerplant/GetHydroPowerPlantsInOperation"

df = pd.read_json(link)
13/17: df.head()
13/18: df["MidProd_91_20"].plot()
13/19: df["MidProd_91_20"].sum()
16/1: import nve_sintef_model
16/2: dir(nve_sintef_model)
16/3: nve_sintef_model.__spec__
16/4: nve_sintef_model.__doc__
16/5: nve_sintef_model.__loader__
16/6: nve_sintef_model.__name__
16/7: nve_sintef_model.__package__
16/8:     nve_sintef_model.__path__
16/9: from nve_sintef_model import exe
16/10: exe.vansimtap.vansimtap_kjor_forste_gang()
16/11: import nve_sintef_model.exe.kurvetegn.Nettflyt
16/12: path = "/Users/mah/Library/CloudStorage/OneDrive-NTNU/Postdoc/Dataset/nve-datasett/detd-filer/DETD_EMPS-omrâ€ der"
16/13: from nve_sintef_model.io.enmd import get_fastkontrakter
16/14: get_fastkontrakter(path)
16/15: from pathlib import Path
16/16: root_folder = Path(path)
16/17: [i for i in root_folder.glob()]
16/18: [i for i in root_folder.glob(*/*)]
16/19: [i for i in root_folder.glob("*/*")]
16/20: [i for i in root_folder.glob("**/*")]
16/21:
for i in root_folder.glob("**/*"):
    i.stem
16/22:
for i in root_folder.glob("**/*"):
    print(i.stem)
16/23:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = get_fastkontrakter(i)
16/24: i
16/25: i.open()
16/26:
rows = []
with i.open() as f:
    for line in f:
        if "* Dellastnr, Kategori, Navn, Eget(T)" in line:
            ss = line.split(",")
            ss = [s.strip() for s in ss][:4]
            if ss[0] == "99":
                break
            rows.append(ss)
df = pd.DataFrame(rows, columns=["fastnr", "katnr", "navn", "eget"])
df["fastnr"] = df["fastnr"].astype(int)
df["katnr"] = df["katnr"].astype(int)
df["navn"] = df["navn"].apply(lambda s : s.replace("'", "").strip())
return df
16/27: f
16/28:
for line in f:
    line
16/29: i
16/30: i.open("r")
16/31:
with i.open("r") as f:
    for line in f:
        line
16/32: f
16/33: area
16/34: i
16/35:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
16/36: area
16/37: area["HALLINGDAL"].open("r")
16/38: with area["HALLINGDAL"].open("r") as f:
16/39:
with area["HALLINGDAL"].open("r") as f:
    for l in f:
        print(l)
16/40:
rows = []
with area["HALLINGDAL"].open() as f:
    for line in f:
        if "* Dellastnr, Kategori, Navn, Eget(T)" in line:
            ss = line.split(",")
            ss = [s.strip() for s in ss][:4]
            if ss[0] == "99":
                break
            rows.append(ss)
df = pd.DataFrame(rows, columns=["fastnr", "katnr", "navn", "eget"])
df["fastnr"] = df["fastnr"].astype(int)
df["katnr"] = df["katnr"].astype(int)
df["navn"] = df["navn"].apply(lambda s : s.replace("'", "").strip())
return df
16/41:
rows = []
with area["HALLINGDAL"].open("r") as f:
    for line in f:
        if "* Dellastnr, Kategori, Navn, Eget(T)" in line:
            ss = line.split(",")
            ss = [s.strip() for s in ss][:4]
            if ss[0] == "99":
                break
            rows.append(ss)
df = pd.DataFrame(rows, columns=["fastnr", "katnr", "navn", "eget"])
df["fastnr"] = df["fastnr"].astype(int)
df["katnr"] = df["katnr"].astype(int)
df["navn"] = df["navn"].apply(lambda s : s.replace("'", "").strip())
return df
16/42:
with area["HALLINGDAL"].open("r") as f:
    for l in f:
        print(l)
16/43:
rows = []
with area["HALLINGDAL"].open("r") as f:
    for line in f:
        print(line)
        if "* Dellastnr, Kategori, Navn, Eget(T)" in line:
            ss = line.split(",")
            ss = [s.strip() for s in ss][:4]
            if ss[0] == "99":
                break
            rows.append(ss)
df = pd.DataFrame(rows, columns=["fastnr", "katnr", "navn", "eget"])
df["fastnr"] = df["fastnr"].astype(int)
df["katnr"] = df["katnr"].astype(int)
df["navn"] = df["navn"].apply(lambda s : s.replace("'", "").strip())
return df
16/44: df
16/45:
import codecs
BLOCKSIZE = 1048576 # or some other, desired size in bytes
with codecs.open(sourceFileName, "r", "iso-8859-1") as sourceFile:
    with codecs.open(targetFileName, "w", "utf-8") as targetFile:
        while True:
            contents = sourceFile.read(BLOCKSIZE)
            if not contents:
                break
            targetFile.write(contents)
16/46: sourceFileName="HALLINGDAL.DETD"
16/47: targetFileName="HALLINGDALUTF8.DETD"
16/48:
import codecs
BLOCKSIZE = 1048576 # or some other, desired size in bytes
with codecs.open(sourceFileName, "r", "iso-8859-1") as sourceFile:
    with codecs.open(targetFileName, "w", "utf-8") as targetFile:
        while True:
            contents = sourceFile.read(BLOCKSIZE)
            if not contents:
                break
            targetFile.write(contents)
16/49: path
16/50:
import codecs
BLOCKSIZE = 1048576 # or some other, desired size in bytes
with codecs.open(path/sourceFileName, "r", "iso-8859-1") as sourceFile:
    with codecs.open(path/targetFileName, "w", "utf-8") as targetFile:
        while True:
            contents = sourceFile.read(BLOCKSIZE)
            if not contents:
                break
            targetFile.write(contents)
16/51: path
16/52: root_folder
16/53:
import codecs
BLOCKSIZE = 1048576 # or some other, desired size in bytes
with codecs.open(root_path/sourceFileName, "r", "iso-8859-1") as sourceFile:
    with codecs.open(root_path/targetFileName, "w", "utf-8") as targetFile:
        while True:
            contents = sourceFile.read(BLOCKSIZE)
            if not contents:
                break
            targetFile.write(contents)
16/54:
import codecs
BLOCKSIZE = 1048576 # or some other, desired size in bytes
with codecs.open(root_folder/sourceFileName, "r", "iso-8859-1") as sourceFile:
    with codecs.open(root_folder/targetFileName, "w", "utf-8") as targetFile:
        while True:
            contents = sourceFile.read(BLOCKSIZE)
            if not contents:
                break
            targetFile.write(contents)
16/55:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
16/56: area
16/57:
with area["HALLINGDALUTF8"].open("r") as f:
    for l in f:
        print(l)
16/58: root_folder.open?
16/59:
rows = []
with area["HALLINGDAL"].open(mode="r",encoding="iso-8859-1") as f:
    for line in f:
        print(line)
        if "* Dellastnr, Kategori, Navn, Eget(T)" in line:
            ss = line.split(",")
            ss = [s.strip() for s in ss][:4]
            if ss[0] == "99":
                break
            rows.append(ss)
df = pd.DataFrame(rows, columns=["fastnr", "katnr", "navn", "eget"])
df["fastnr"] = df["fastnr"].astype(int)
df["katnr"] = df["katnr"].astype(int)
df["navn"] = df["navn"].apply(lambda s : s.replace("'", "").strip())
return df
16/60: import pandas as pd
16/61: area
16/62:
with area["HALLINGDALUTF8"].open("r") as f:
    for l in f:
        print(l)
16/63:
with area["HALLINGDAL"].open("r") as f:
    for l in f:
        print(l)
16/64: line
16/65: get_fastkontrakter(path)
16/66: get_fastkontrakter(area["OSTLAND"])
16/67: get_fastkontrakter(area["HALLINGDAL"])
16/68: from nve_sintef_model.io.enmd import get_fastkontrakter
16/69: get_fastkontrakter(area["HALLINGDAL"])
16/70:
%load_ext autoreload
%autoreload 2
16/71: get_fastkontrakter(area["HALLINGDAL"])
16/72: get_fastkontrakter?
16/73: !pip install --upgrade nve_sintef_model
16/74:
import sys
import os

folder_path = "/Users/mah/gitsource/nve-sintef-models"
sys.path.insert(0, folder_path)

# Now you can import modules and packages from the added folder
import your_module
16/75:
import sys
import os

folder_path = "/Users/mah/gitsource/nve-sintef-models"
sys.path.insert(0, folder_path)

# Now you can import modules and packages from the added folder
import nve_sintef_model
16/76: dir(nve_sintef_model)
16/77: dir(nve_sintef_model)
16/78: import pandas as pd
16/79: dir(nve_sintef_model)
16/80:
import sys
import os

folder_path = "/Users/mah/gitsource/nve-sintef-models"
sys.path.insert(0, folder_path)

# Now you can import modules and packages from the added folder
import nve_sintef_model
16/81: import pandas as pd
16/82: dir(nve_sintef_model)
16/83: from nve_sintef_model.io.enmd import get_fastkontrakter
16/84: root_folder = Path(path)
16/85:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
16/86: area
16/87: first(area)
16/88: area.first()
16/89: area.first
16/90:
%load_ext autoreload
%autoreload 2
16/91: get_fastkontrakter?
17/1:
%load_ext autoreload
%autoreload 2
17/2:
import sys
import os
from pathlib import Path

folder_path = "/Users/mah/gitsource/nve-sintef-models"
sys.path.insert(0, folder_path)

# Now you can import modules and packages from the added folder
import nve_sintef_model
17/3:
import sys
import os
from pathlib import Path

folder_path = "/Users/mah/gitsource/nve-sintef-model"
sys.path.insert(0, folder_path)

# Now you can import modules and packages from the added folder
import nve_sintef_model
17/4: import pandas as pd
17/5: dir(nve_sintef_model)
17/6: path = "/Users/mah/Library/CloudStorage/OneDrive-NTNU/Postdoc/Dataset/nve-datasett/detd-filer/DETD_EMPS-omrâ€ der"
17/7: from nve_sintef_model.io.enmd import get_fastkontrakter
17/8: root_folder = Path(path)
17/9:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
17/10: get_fastkontrakter?
17/11: get_fastkontrakter(area["HALLINGDAL")
17/12: get_fastkontrakter(area["HALLINGDAL"])
17/13: from IPython.display import display
17/14:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
    display(get_fastkontrakter(i))
17/15:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
    print(i)
    display(get_fastkontrakter(i))
17/16:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
    print(i.stem)
    display(get_fastkontrakter(i))
17/17: # Fastkontrakter
17/18: from nve_sintef_model.io.enmd import les_brensel_enmd_filer
17/19: from nve_sintef_model.io.enmd import les_brensel_enmd_filer, les_brensel_enmd_fil
17/20: les_brensel_enmd_fil(root_folder)
17/21: df = les_brensel_enmd_filer(root_folder)
17/22: les_brensel_enmd_fil(area["HALLINGDAL"])
17/23: les_brensel_enmd_fil(area["HALLINGDAL"])
17/24:
for a in area:
    print(a)
    display(les_brensel_enmd_fil(area["HALLINGDAL"]))
17/25:
df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df.name = 'My DataFrame'

print(df.name)
17/26:
df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df.name = 'My DataFrame'

df
17/27:
df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df.name = 'My DataFrame'

df.columns.name = "T"
df
17/28:
df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df.name = 'My DataFrame'

df.columns.name = "T"
df
17/29:
for a in area:
    print(a)
    display(les_brensel_enmd_fil(area["HALLINGDAL"]))
17/30:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
    print(i.stem)
    display(get_fastkontrakter(i))
17/31:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
    display(get_fastkontrakter(i))
17/32: from nve_sintef_model.io.enmd import les_brensel_enmd_filer, les_brensel_enmd_fil
17/33:
for a in area:
    print(a)
    display(les_brensel_enmd_fil(area["HALLINGDAL"]))
17/34:
for a in area:
    print(a)
    display(les_brensel_enmd_fil(area[a]))
17/35: df = les_brensel_enmd_filer(area[a])
17/36: df = les_brensel_enmd_fil(area[a])
17/37: df
17/38: df.columns
17/39: from nve_sintef_model.io.enmd import les_brensel_enmd_filer, les_brensel_enmd_fil
17/40:
for a in area:
    print(a)
    display(les_brensel_enmd_fil(area[a]))
17/41: df = les_brensel_enmd_fil(area[a])
17/42: df
17/43:
for a in area:
    print(a)
    display(les_brensel_enmd_fil(area[a]))
17/44:
path = "/Users/mah/Library/CloudStorage/OneDrive-NTNU/Postdoc/Dataset/nve-datasett/detd-filer/DETD_EMPS-omrâ€ der"
path = "/Users/mah/Library/CloudStorage/OneDrive-NTNU/Postdoc/Dataset/1-Base Case Dataset"
17/45: from nve_sintef_model.io.enmd import get_fastkontrakter
17/46: root_folder = Path(path)
17/47:
area={}
for i in root_folder.glob("**/*"):
    area[i.stem] = i
    display(get_fastkontrakter(i))
17/48:
area={}
for i in root_folder.glob("*.detd"):
    area[i.stem] = i
    display(get_fastkontrakter(i))
17/49:
area={}
for i in root_folder.glob("*.detd"):
    print(i)
    area[i.stem] = i
    display(get_fastkontrakter(i))
17/50:
area={}
for i in root_folder.glob("*.enmd"):
    print(i)
    area[i.stem] = i
    display(get_fastkontrakter(i))
17/51:
area={}
for i in root_folder.glob("*.ENMD"):
    print(i)
    area[i.stem] = i
    display(get_fastkontrakter(i))
17/52:
def get_files(folder, file_extension:str =".ENMD"):
    return [i for i in folder.glob(f"*{file_extension}")]
17/53: get_files(root_folder, file_extension=".ENMD")
17/54:
def get_files(folder: Path, file_extension:str =".ENMD"):
    return [i for i in folder.glob(f"*{file_extension}")]
17/55:
area={}
enmd_files = get_files(root_folder, ".ENMD")
for i in enmd_files:
    area[i.stem] = i
    display(get_fastkontrakter(i))
17/56:
area={}
enmd_files = get_files(root_folder, ".ENMD")
17/57: [i.stem for i in enmd_files]
17/58: from nve_sintef_model.io.enmd import les_brensel_enmd_filer, les_brensel_enmd_fil
17/59:
for a in area:
    display(les_brensel_enmd_fil(area[a]))
17/60: area
17/61:
for i in enmd_files:
    display(les_brensel_enmd_fil(i))
17/62: i
17/63: les_brensel_enmd_fil(enmd_files[0])
17/64: les_brensel_enmd_fil(enmd_files[1])
17/65: les_brensel_enmd_fil(enmd_files[2])
17/66:
for i in enmd_files:
    try:
        display(les_brensel_enmd_fil(i))
    except Exception:
        print(i)
17/67: get_files(root_folder, ".pri")
17/68: get_files(root_folder, ".PRI")
17/69: from nve_sintef_model.io.emar import read_io_emar
17/70: read_io_emar(root_folder)
17/71: from nve_sintef_model.io.emar.read_io_emar import read_io_emar
17/72: read_io_emar(root_folder)
17/73: emar_files = get_files(root_folder, ".EMAR")
17/74: emar_files
17/75:
i = emar_files[0]
read_io_emar(i)
17/76:
i = emar_files[0]
read_io_emar(i)
17/77:
i = emar_files[0]
read_io_emar(i)
17/78:
i = emar_files[0]
df = read_io_emar(i)
df.head()
17/79: df.plot()
17/80: pd.options.plotting.backend = "plotly"
17/81:
import plotly.graph_objects as go
import plotly.express as px
17/82: df.plot()
17/83: df.set_index("uke")
17/84:
i = emar_files[0]
df = read_io_emar(i)
df.head()
17/85: df.plot()
17/86: read_io_emar?
17/87: i
17/88: i.suffix
17/89: i.stem
17/90: i.root
17/91: i.with_name
17/92: i.name
17/93: [i for i in root_folder.iterdir() if i.name == "ENMRES.DATA"][0]
17/94: enmres_file = [i for i in root_folder.iterdir() if i.name == "ENMRES.DATA"][0]
17/95: get_files(root_folder, file_extension=".ENMD")
17/96:
def get_file(folder: Path, file_name: str) -> Optional[Path]:
    """
    Returns the specified file from the given folder if it exists, otherwise returns None.

    Parameters
    ----------
    folder : Path
        The folder to search for the file.
    file_name : str
        The name of the file to search for.

    Returns
    -------
    Optional[Path]
        The matching Path object if the file is found, or None if no matching file is found.
    """

    files = [i for i in folder.iterdir() if i.is_file() and i.name == file_name]
    file_count = len(files)

    if files:
        return files[0]
    else:
        return None
17/97: from typing import Optional, Union
17/98: from typing import Optional, Union, List
17/99:
def get_files(folder: Path, file_extension: str = ".ENMD") -> List[Path]:
    """
    Returns a list of files in the given folder with the specified file extension.

    Parameters
    ----------
    folder : Path
        The folder to search for files.
    file_extension : str, optional
        The file extension to filter files by, including the dot (e.g., ".ENMD"). Case-insensitive. Default is ".ENMD".

    Returns
    -------
    List[Path]
        A list of Path objects representing the files with the specified extension.
    """
    file_extension = file_extension.lower()

    return [i for i in folder.glob("*") if i.is_file() and i.suffix.lower() == file_extension]
17/100:
def get_files(folder: Path, file_extension: str = ".ENMD") -> List[Path]:
    """
    Returns a list of files in the given folder with the specified file extension.

    Parameters
    ----------
    folder : Path
        The folder to search for files.
    file_extension : str, optional
        The file extension to filter files by, including the dot (e.g., ".ENMD"). Case-insensitive. Default is ".ENMD".

    Returns
    -------
    List[Path]
        A list of Path objects representing the files with the specified extension.
    """
    file_extension = file_extension.lower()

    return [i for i in folder.glob("*") if i.is_file() and i.suffix.lower() == file_extension]
17/101:
def get_file(folder: Path, file_name: str) -> Optional[Path]:
    """
    Returns the specified file from the given folder if it exists, otherwise returns None.

    Parameters
    ----------
    folder : Path
        The folder to search for the file.
    file_name : str
        The name of the file to search for.

    Returns
    -------
    Optional[Path]
        The matching Path object if the file is found, or None if no matching file is found.
    """

    files = [i for i in folder.iterdir() if i.is_file() and i.name == file_name]
    file_count = len(files)

    if files:
        return files[0]
    else:
        return None
17/102: enmres_file = get_file(root_folder, "ENMRES.DATA")
17/103: from nve_sintef_model.io.enmres import Enmres, read_io_enmres
17/104: enmres_file = get_file(root_folder, "ENMRES.DATA")
17/105: from nve_sintef_model.io.enmres import Enmres, read_io_enmres
17/106: Enmres.Enmres
17/107: from nve_sintef_model.io.enmres import Enmres, read_io_enmres
17/108: from nve_sintef_model.io.enmres import Enmres, read_io_enmres
17/109: from nve_sintef_model.io.enmres import Enmres, read_io_enmres
17/110: Enmres.Enmres
17/111: Enmres
17/112: from nve_sintef_model.io.enmres import Enmres, read_io_enmres
17/113: from nve_sintef_model.io.enmres import Enmres, read_io_enmres
17/114: Enmres
17/115: Enmres(enmres_file)
17/116: enmres = Enmres(enmres_file)
17/117: enmres["v"]
17/118: getattr(enmres,"v")
17/119: enmres.get_data("v")
17/120: enmres.v
17/121: enmres.v()
17/122: enmres.get_data("pkrv")
17/123: from nve_sintef_model.io.enmres import Enmres, read_io_enmres, EnmresType
17/124: [i for i in EnmresType]
17/125: [i.value for i in EnmresType]
17/126: [i.value for i in EnmresType]
17/127: from nve_sintef_model.io.enmres import Enmres, read_io_enmres, EnmresType
17/128: [i.value for i in EnmresType]
17/129: enmres.get_data("pkrv")
17/130: get_files(root_folder, "")
17/131: enmres = Enmres(enmres_file)
17/132: getattr(enmres,"v")
17/133: enmres.get_data("v")
17/134: from nve_sintef_model.io.enmres import Enmres, read_io_enmres, EnmresProperty
17/135: from nve_sintef_model.io.enmres import Enmres, read_io_enmres, EnmresProperty
17/136: enmres_file = get_file(root_folder, "ENMRES.DATA")
17/137: enmres = Enmres(enmres_file)
17/138: enmres.get_data(EnmresProperty.ELPUMP)
17/139: enmres.get_data(EnmresProperty.ELPUMP)
17/140: EnmresProperty.ELPUMP
17/141: EnmresProperty.ELPUMP.value
17/142: enmres.get_data(EnmresProperty.ELPUMP)
17/143: enmres.get_data(EnmresProperty.ELPUMP)
17/144: read_io_enmres(enmres_file)
17/145: from nve_sintef_model.io.enmres import Enmres, read_io_enmres, EnmresProperty
17/146: enmres_file = get_file(root_folder, "ENMRES.DATA")
17/147: enmres = Enmres(enmres_file)
17/148: enmres.get_data(EnmresProperty.ELPUMP)
17/149: read_io_enmres(enmres_file)
17/150: rows = {k.value: [] for k in EnmresType}
17/151: rows
17/152: from nve_sintef_model.io.enmres import Enmres, read_io_enmres, EnmresProperty
17/153: read_io_enmres(enmres_file)
17/154:
from typing import Union, Dict, List
from pathlib import Path
import struct

from .Enmres import EnmresType

def read_io_enmres(
    path: Union[str, Path]
) -> Dict[str, Union[str, int, float, List[List[Union[int, float]]]]]:
    """
    Reads the binary data from the ENMRES file and returns it as a dictionary.

    Parameters
    ----------
    path : Union[str, Path]
        Path to the ENMRES file.

    Returns
    -------
    Dict[str, Union[str, int, float, List[List[Union[int, float]]]]]
        A dictionary containing the data read from the ENMRES file.
    """
    encoding = "cp865"

    with open(path, "rb") as f:
        enmfil = b"".join([i for i in struct.unpack("c" * 60, f.read(60))]).decode(encoding).strip()
        enmtid = struct.unpack("i" * 7, f.read(7 * 4))
        runcode = struct.unpack("c" * 40, f.read(40))
        npre, nsim, naar, staar, innaar, nuke, ntrinn, jstart, jslutt, serie, npenm = struct.unpack(
            "i" * 11, f.read(11 * 4)
        )
        ntimen = struct.unpack("i" * npenm, f.read(npenm * 4))
        (stmag,) = struct.unpack("f", f.read(4))
        hist = struct.unpack("i" * nsim, f.read(nsim * 4))
        (tprog,) = struct.unpack("i", f.read(4))
        tprog = int(tprog != 0)
        (realrente,) = struct.unpack("f", f.read(4))

        blokklengde = max(12 * 4 * npenm * (jslutt - jstart + 1), 100 + (npenm + nsim + 19 + 2) * 4)

        blokknr = lambda n: n

        data = dict()

        data["enmfil"] = enmfil
        data["enmtid"] = enmtid
        data["runcode"] = runcode
        data["npre"] = npre
        data["nsim"] = nsim
        data["naar"] = naar
        data["staar"] = staar
        data["innaar"] = innaar
        data["nuke"] = nuke
        data["ntrinn"] = ntrinn
        data["jstart"] = jstart
        data["jslutt"] = jslutt
        data["serie"] = serie
        data["npenm"] = npenm
        data["ntimen"] = ntimen
        data["stmag"] = stmag
        data["hist"] = hist
        data["tprog"] = tprog
        data["realrente"] = realrente
        data["blokklengde"] = blokklengde

        rows = {k.value: [] for k in EnmresType}

        for n, aar in enumerate(hist, start=1):
            f.seek(blokklengde * blokknr(n))

            for uke in range(jstart, jslutt + 1):
                for tsnitt in range(1, npenm + 1):
                    values = struct.unpack("f" * 12, f.read(4 * 12))
                    for k, value in zip(EnmresType, values):
                        rows[k.value].append([aar, uke, tsnitt, value])

        data.update(rows)

        return data
17/155: read_io_enmres(enmres_file)
17/156: enmres.get_data(EnmresProperty.ELPUMP)
17/157: enmres.get_data(EnmresProperty.QPUMP)
17/158: enmres._set_output()
17/159: enmres._output
17/160: enmres._output.keys()
17/161: enmres._output.pop(EnmresProperty.ELPUMP)
17/162: EnmresProperty.ELPUMP
17/163: enmres._output.pop(EnmresProperty.ELPUMP.value)
17/164: enmres.get_data(EnmresProperty.QPUMP)
17/165: enmres.get_data(EnmresProperty.ELPUMP)
17/166: enmres.get_data(EnmresProperty.ELPUMP)
17/167: enmres = Enmres(enmres_file)
17/168: enmres.get_data(EnmresProperty.ELPUMP)
17/169: enmres.get_data(EnmresProperty.V)
17/170:
if "v" in [i.value for i in EnmresProperty]:
    print("gg")
17/171: from nve_sintef_model.io.enmres import Enmres, read_io_enmres, EnmresProperty
17/172: enmres_file = get_file(root_folder, "ENMRES.DATA")
17/173: enmres = Enmres(enmres_file)
17/174: enmres.get_data(EnmresProperty.V)
17/175: enmres.
17/176: enmres.v
17/177: enmres.v
17/178: enmres.flom
18/1: import configparser
18/2:
config = configparser.ConfigParser()
config.read("../config.ini")
18/3: os.environ["OPENAI_API_KEY"] = config["API"]["OpenAI_API_Key"]
18/4: import os
18/5: os.environ["OPENAI_API_KEY"] = config["API"]["OpenAI_API_Key"]
18/6:
from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader('data').load_data()
index = GPTSimpleVectorIndex.from_documents(documents)
18/7:
from gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader('data').load_data()
index = GPTSimpleVectorIndex.from_documents(documents)
18/8:
from llama_index import (
    GPTKeywordTableIndex,
    SimpleDirectoryReader,
    LLMPredictor,
    ServiceContext
)
from langchain import OpenAI

documents = SimpleDirectoryReader('data').load_data()

# define LLM
llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="text-davinci-002"))
service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)

# build index
index = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)

# get response from query
response = index.query("What did the author do after his time at Y Combinator?")
18/9:
from gpt_index import (
    GPTKeywordTableIndex,
    SimpleDirectoryReader,
    LLMPredictor,
    ServiceContext
)
from langchain import OpenAI

documents = SimpleDirectoryReader('data').load_data()

# define LLM
llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="text-davinci-002"))
service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)

# build index
index = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)

# get response from query
response = index.query("What did the author do after his time at Y Combinator?")
18/10:
# get response from query
response = index.query("What did the author do after his time at Y Combinator?")
18/11: response
18/12:

response.get_formatted_sources
18/13:

response.get_formatted_sources()
18/14: OpenAI?
20/1:
from configparser import ConfigParser
from typing import Optional
from pathlib import Path
from getpass import getpass
from io import StringIO
from azure.storage.fileshare import ShareServiceClient, ShareDirectoryClient
from configparser import RawConfigParser
import pandas as pd
20/2:

class DataLoader:
    """
    DataLoader loads offshore wind energy data from local or Azure file share storage.

    Args:
        data_folder_path (str, optional): Local path to data folder. Defaults to None, in which case
        data is loaded from Azure file storage, read from the 'config.ini' file.

    Attributes:
        df (pandas.DataFrame): Concatenated dataframe of offshore wind energy data.
        df_locations(pandas.DataFrame): Locations' dataframe with columns: latitude, longitude, and location name.
        df_nve_wind_locations (pandas.DataFrame): NVE's offshore wind area dataframe with columns: latitude, longitude, and location ids.

    """

    def __init__(self, data_folder_path: Optional[str] = None):

        if data_folder_path:
            df_wind_locations = pd.read_csv(
                Path(data_folder_path) / "offshore_wind_locations.csv")
            df_nve_wind_locations = pd.read_csv(
                Path(data_folder_path) / "nve_offshore_wind_areas.csv", index_col=0)
            df_nve_wind_locations = df_nve_wind_locations.sort_values(
                by="lat")  # Sort by south to north

            df_locations = pd.concat(
                [df_wind_locations, df_nve_wind_locations], axis=0)
            df_locations = df_locations.reset_index(drop=True)
            df_locations = df_locations.sort_values(
                by="lat")  # Sort by south to north

            # Load data
            data = []
            for l in df_locations["location"].values:
                data.append(pd.read_csv(Path(data_folder_path) /
                            f"{l}.csv", index_col=0, parse_dates=True))

            df = pd.concat(data, axis=1)
            df = df[df_locations["location"]]  # Sort by south to north

            self.df = df
            self.df_locations = df_locations
            self.df_nve_wind_locations = df_nve_wind_locations

        else:
            parser = ConfigParser()
            parser.read("config.ini")
            account_url = parser.get("Azure", "account_url")
            directory_path = parser.get("Azure", "directory_path")
            share_name = parser.get("Azure", "share_name")
            sas_token_url = parser.get(
                "Azure", "sas_token") or getpass("sas taken and url: ")

            dir_client = ShareDirectoryClient(
                account_url=account_url, directory_path=directory_path, share_name=share_name
            )
            file_client = dir_client.get_file_client(
                "offshore_wind_locations.csv")

            dir_client = ShareDirectoryClient(
                account_url=account_url, directory_path=f"{directory_path}/nve/profiler/Wind and solar", share_name=share_name
            )

            file_client = dir_client.get_file_client(
                "offshore_wind_locations.csv")
            df_wind_locations = pd.read_csv(
                StringIO(file_client.download_file().readall()))

            file_client = dir_client.get_file_client(
                "nve_offshore_wind_areas.csv")
            df_nve_wind_locations = pd.read_csv(
                StringIO(file_client.download_file().readall()))

            df_nve_wind_locations = df_nve_wind_locations.sort_values(
                by="lat")  # Sort by south to north

            df_locations = pd.concat(
                [df_wind_locations, df_nve_wind_locations], axis=0)
            df_locations = df_locations.reset_index(drop=True)
            df_locations = df_locations.sort_values(
                by="lat")  # Sort by south to north

            # Load data
            data = []
            for l in df_locations["location"].values:
                file_client = dir_client.get_file_client(f"{l}.csv")
                df_temp = pd.read_csv(
                    StringIO(file_client.download_file().readall()), index_col=0, parse_dates=True
                )
                data.append(df_temp)\n\n            df = pd.concat(data, axis=1)
            df = df[df_locations["location"]]  # Sort by south to north

            self.df = df
            self.df_locations = df_locations
            self.df_nve_wind_locations = df_nve_wind_locations
20/3:
class DataLoader:
    """
    DataLoader loads offshore wind energy data from local or Azure file share storage.
    Args:
        data_folder_path (str, optional): Local path to data folder. Defaults to None, in which case
        data is loaded from Azure file storage, read from the 'config.ini' file.
    Attributes:
        df (pandas.DataFrame): Concatenated dataframe of offshore wind energy data.
        df_locations(pandas.DataFrame): Locations' dataframe with columns: latitude, longitude, and location name.
        df_nve_wind_locations (pandas.DataFrame): NVE's offshore wind area dataframe with columns: latitude, longitude, and location ids.
    """
    def __init__(self, data_folder_path: Optional[str] = None):
        if data_folder_path:
            df_wind_locations = pd.read_csv(
                Path(data_folder_path) / "offshore_wind_locations.csv")
            df_nve_wind_locations = pd.read_csv(
                Path(data_folder_path) / "nve_offshore_wind_areas.csv", index_col=0)
            df_nve_wind_locations = df_nve_wind_locations.sort_values(
                by="lat")  # Sort by south to north
            df_locations = pd.concat(
                [df_wind_locations, df_nve_wind_locations], axis=0)
            df_locations = df_locations.reset_index(drop=True)
            df_locations = df_locations.sort_values(
                by="lat")  # Sort by south to north
            # Load data
            data = []
            for l in df_locations["location"].values:
                data.append(pd.read_csv(Path(data_folder_path) /
                            f"{l}.csv", index_col=0, parse_dates=True))
            df = pd.concat(data, axis=1)
            df = df[df_locations["location"]]  # Sort by south to north
            self.df = df
            self.df_locations = df_locations
            self.df_nve_wind_locations = df_nve_wind_locations
        else:
            parser = ConfigParser()
            parser.read("config.ini")
            account_url = parser.get("Azure", "account_url")
            directory_path = parser.get("Azure", "directory_path")
            share_name = parser.get("Azure", "share_name")
            sas_token_url = parser.get(
                "Azure", "sas_token") or getpass("sas taken and url: ")
            dir_client = ShareDirectoryClient(
                account_url=account_url, directory_path=directory_path, share_name=share_name
            )
            file_client = dir_client.get_file_client(
                "offshore_wind_locations.csv")
            dir_client = ShareDirectoryClient(
                account_url=account_url, directory_path=f"{directory_path}/nve/profiler/Wind and solar", share_name=share_name
            )
            file_client = dir_client.get_file_client(
                "offshore_wind_locations.csv")
            df_wind_locations = pd.read_csv(
                StringIO(file_client.download_file().readall()))
            file_client = dir_client.get_file_client(
                "nve_offshore_wind_areas.csv")
            df_nve_wind_locations = pd.read_csv(
                StringIO(file_client.download_file().readall()))
            df_nve_wind_locations = df_nve_wind_locations.sort_values(
                by="lat")  # Sort by south to north
            df_locations = pd.concat(
                [df_wind_locations, df_nve_wind_locations], axis=0)
            df_locations = df_locations.reset_index(drop=True)
            df_locations = df_locations.sort_values(
                by="lat")  # Sort by south to north
            # Load data
            data = []
            for l in df_locations["location"].values:
                file_client = dir_client.get_file_client(f"{l}.csv")
                df_temp = pd.read_csv(
                    StringIO(file_client.download_file().readall()), index_col=0, parse_dates=True
                )
                data.append(df_temp)
            df = pd.concat(data, axis=1)
            df = df[df_locations["location"]]  # Sort by south to north
            self.df = df
            self.df_locations = df_locations
            self.df_nve_wind_locations = df_nve_wind_locations
20/4:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
torch.cuda.is_available()
20/5:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
torch.cuda.is_available()
20/6: torch.cuda.is_available()
20/7: device = torch.device("cpu")
20/8:
def update_identity_rho(x, I, rho, mu, n_weights):
    zero = torch.tensor(0.0, dtype=torch.float64)
    for i in range(n_weights):
        if (-x[i]) < zero and torch.isclose(mu[i], zero):
            I[i, i] = 0
        else:
            I[i, i] = rho

def update_dual_variables(lmbda, mu, rho, x):
    with torch.no_grad():
        lmbda = lmbda + rho * (x.sum() - 1)
        mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))

def are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4):
    # dx L = 0
    dx = torch.autograd.grad(lf(x, lmbda, mu), x)[0]
    if torch.isclose(dx, torch.zeros_like(dx), atol=atol).all():
        # c(x) = 0 | x.sum()-1 = 0
        if torch.isclose((x.sum() - 1), torch.tensor(0.0, dtype=torch.float64), atol=atol):
            # h(x) <= 0 | (-x) <= 0
            if ((-x) <= 0.0).all():
                # mu >= 0
                if (mu >= 0.0).all():
                    # mu*.h(x) = 0
                    if torch.isclose((-x) * mu, torch.zeros_like(mu), atol=atol).all():
                        return True
    return False

def f(x, loss_fn="Var", alp=0.5):
    if loss_fn == "Var":
        y = torch.matmul(x, Y)
        return y.var()
    elif loss_fn == "Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return torch.triu(cov_matrix, diagonal=1).sum()
    elif loss_fn == "-Mean+Var":
        y = x[:, None] * Y
        return -alp * y.mean() + (1 - alp) * y.var()
    elif loss_fn == "-Mean+Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return -alp * y.mean() + (1 - alp) * torch.triu(cov_matrix, diagonal=1).sum()

def lf(x, lmbda, mu, loss_fn="Var"):
    return f(x, loss_fn) + lmbda * (x.sum() - 1) + torch.matmul(-x, mu)

def lf_rho(x, lmbda, mu, rho, I_rho, loss_fn="Var"):
    return lf(x, lmbda, mu, loss_fn) + rho / 2 * (x.sum() - 1) ** 2 + 1 / 2 * torch.matmul(torch.matmul(-x, I_rho), (-x))
20/9: from Wind.load_data import DataLoader
20/10:
from pathlib import Path
from Wind.load_data import DataLoader
20/11: data_loader = DataLoader()
20/12:
config = RawConfigParser()
config.read("config.ini")
21/1:
import numpy as np
import pandas as pd
import plotly.express as px
21/2: !cwd
21/3:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
torch.cuda.is_available()
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
def update_identity_rho(x, I, rho, mu, n_weights):
    zero = torch.tensor(0.0, dtype=torch.float64)
    for i in range(n_weights):
        if (-x[i]) < zero and torch.isclose(mu[i], zero):
            I[i, i] = 0
        else:
            I[i, i] = rho

def update_dual_variables(lmbda, mu, rho, x):
    with torch.no_grad():
        lmbda = lmbda + rho * (x.sum() - 1)
        mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))

def are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4):
    # dx L = 0
    dx = torch.autograd.grad(lf(x, lmbda, mu), x)[0]
    if torch.isclose(dx, torch.zeros_like(dx), atol=atol).all():
        # c(x) = 0 | x.sum()-1 = 0
        if torch.isclose((x.sum() - 1), torch.tensor(0.0, dtype=torch.float64), atol=atol):
            # h(x) <= 0 | (-x) <= 0
            if ((-x) <= 0.0).all():
                # mu >= 0
                if (mu >= 0.0).all():
                    # mu*.h(x) = 0
                    if torch.isclose((-x) * mu, torch.zeros_like(mu), atol=atol).all():
                        return True
    return False

def f(x, loss_fn="Var", alp=0.5):
    if loss_fn == "Var":
        y = torch.matmul(x, Y)
        return y.var()
    elif loss_fn == "Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return torch.triu(cov_matrix, diagonal=1).sum()
    elif loss_fn == "-Mean+Var":
        y = x[:, None] * Y
        return -alp * y.mean() + (1 - alp) * y.var()
    elif loss_fn == "-Mean+Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return -alp * y.mean() + (1 - alp) * torch.triu(cov_matrix, diagonal=1).sum()

def lf(x, lmbda, mu, loss_fn="Var"):
    return f(x, loss_fn) + lmbda * (x.sum() - 1) + torch.matmul(-x, mu)

def lf_rho(x, lmbda, mu, rho, I_rho, loss_fn="Var"):
    return lf(x, lmbda, mu, loss_fn) + rho / 2 * (x.sum() - 1) ** 2 + 1 / 2 * torch.matmul(torch.matmul(-x, I_rho), (-x))
### Generate test dataset

N = 1000
s = np.linspace(0, 10, N)
y1 = np.sin(s)
y2 = np.cos(s)
plt.plot(y1)
plt.plot(y2)
Y = np.stack((y1, y2), axis=-1).T
Y = torch.from_numpy(Y)
Y = Y.to(device)
### Use Wind dataset

from pathlib import Path
from Wind.load_data import DataLoader
21/4: data_loader = DataLoader()
21/5:
config = RawConfigParser()
config.read("config.ini")
21/6: from configparser import RawConfigParser
21/7:
config = RawConfigParser()
config.read("config.ini")
21/8: config.read("../config.ini")
22/1: config = RawConfigParser()
22/2: from configparser import RawConfigParser
22/3:
config = RawConfigParser()
config.read("../config.ini")
22/4: sas_token_url = config["File Storage"]["sas_token"]
22/5: sas_token_url
22/6:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
torch.cuda.is_available()
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
def update_identity_rho(x, I, rho, mu, n_weights):
    zero = torch.tensor(0.0, dtype=torch.float64)
    for i in range(n_weights):
        if (-x[i]) < zero and torch.isclose(mu[i], zero):
            I[i, i] = 0
        else:
            I[i, i] = rho

def update_dual_variables(lmbda, mu, rho, x):
    with torch.no_grad():
        lmbda = lmbda + rho * (x.sum() - 1)
        mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))

def are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4):
    # dx L = 0
    dx = torch.autograd.grad(lf(x, lmbda, mu), x)[0]
    if torch.isclose(dx, torch.zeros_like(dx), atol=atol).all():
        # c(x) = 0 | x.sum()-1 = 0
        if torch.isclose((x.sum() - 1), torch.tensor(0.0, dtype=torch.float64), atol=atol):
            # h(x) <= 0 | (-x) <= 0
            if ((-x) <= 0.0).all():
                # mu >= 0
                if (mu >= 0.0).all():
                    # mu*.h(x) = 0
                    if torch.isclose((-x) * mu, torch.zeros_like(mu), atol=atol).all():
                        return True
    return False

def f(x, loss_fn="Var", alp=0.5):
    if loss_fn == "Var":
        y = torch.matmul(x, Y)
        return y.var()
    elif loss_fn == "Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return torch.triu(cov_matrix, diagonal=1).sum()
    elif loss_fn == "-Mean+Var":
        y = x[:, None] * Y
        return -alp * y.mean() + (1 - alp) * y.var()
    elif loss_fn == "-Mean+Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return -alp * y.mean() + (1 - alp) * torch.triu(cov_matrix, diagonal=1).sum()

def lf(x, lmbda, mu, loss_fn="Var"):
    return f(x, loss_fn) + lmbda * (x.sum() - 1) + torch.matmul(-x, mu)

def lf_rho(x, lmbda, mu, rho, I_rho, loss_fn="Var"):
    return lf(x, lmbda, mu, loss_fn) + rho / 2 * (x.sum() - 1) ** 2 + 1 / 2 * torch.matmul(torch.matmul(-x, I_rho), (-x))
### Generate test dataset

N = 1000
s = np.linspace(0, 10, N)
y1 = np.sin(s)
y2 = np.cos(s)
plt.plot(y1)
plt.plot(y2)
Y = np.stack((y1, y2), axis=-1).T
Y = torch.from_numpy(Y)
Y = Y.to(device)
### Use Wind dataset

from pathlib import Path
from Wind.load_data import DataLoader
23/1: !pwd
23/2:
from Wind.load_data import DataLoader
data_loader = DataLoader()
23/3: Y = data_loader.df.values.T
23/4:
Y = torch.from_numpy(Y)
Y = Y.to(device)
23/5:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
torch.cuda.is_available()
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
def update_identity_rho(x, I, rho, mu, n_weights):
    zero = torch.tensor(0.0, dtype=torch.float64)
    for i in range(n_weights):
        if (-x[i]) < zero and torch.isclose(mu[i], zero):
            I[i, i] = 0
        else:
            I[i, i] = rho

def update_dual_variables(lmbda, mu, rho, x):
    with torch.no_grad():
        lmbda = lmbda + rho * (x.sum() - 1)
        mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))

def are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4):
    # dx L = 0
    dx = torch.autograd.grad(lf(x, lmbda, mu), x)[0]
    if torch.isclose(dx, torch.zeros_like(dx), atol=atol).all():
        # c(x) = 0 | x.sum()-1 = 0
        if torch.isclose((x.sum() - 1), torch.tensor(0.0, dtype=torch.float64), atol=atol):
            # h(x) <= 0 | (-x) <= 0
            if ((-x) <= 0.0).all():
                # mu >= 0
                if (mu >= 0.0).all():
                    # mu*.h(x) = 0
                    if torch.isclose((-x) * mu, torch.zeros_like(mu), atol=atol).all():
                        return True
    return False

def f(x, loss_fn="Var", alp=0.5):
    if loss_fn == "Var":
        y = torch.matmul(x, Y)
        return y.var()
    elif loss_fn == "Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return torch.triu(cov_matrix, diagonal=1).sum()
    elif loss_fn == "-Mean+Var":
        y = x[:, None] * Y
        return -alp * y.mean() + (1 - alp) * y.var()
    elif loss_fn == "-Mean+Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return -alp * y.mean() + (1 - alp) * torch.triu(cov_matrix, diagonal=1).sum()

def lf(x, lmbda, mu, loss_fn="Var"):
    return f(x, loss_fn) + lmbda * (x.sum() - 1) + torch.matmul(-x, mu)

def lf_rho(x, lmbda, mu, rho, I_rho, loss_fn="Var"):
    return lf(x, lmbda, mu, loss_fn) + rho / 2 * (x.sum() - 1) ** 2 + 1 / 2 * torch.matmul(torch.matmul(-x, I_rho), (-x))
### Generate test dataset

N = 1000
s = np.linspace(0, 10, N)
y1 = np.sin(s)
y2 = np.cos(s)
plt.plot(y1)
plt.plot(y2)
Y = np.stack((y1, y2), axis=-1).T
Y = torch.from_numpy(Y)
Y = Y.to(device)
23/6:
Y = data_loader.df.values.T
Y = torch.from_numpy(Y)
Y = Y.to(device)
23/7:
def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float64)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float64)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float64)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float64, device=device)
    I_rho = np.eye(Y.shape[0])
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    objs = []
    xs = [x.cpu().detach().numpy().copy()]
    lmbdas = [lmbda.item()]
    mus = [mu.cpu().detach().numpy().copy()]
    rhos = [rho.item()]
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            xs.append(x.cpu().detach().numpy().copy())
            objs.append([obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), f(x, loss_fn=loss_fn).item()])
        objs.append(
            [
                lf_rho(x, lmbda, mu, rho, I_rho,loss_fn=loss_fn).item(),
                lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                f(x, loss_fn=loss_fn).item(),
            ]
        )
        mus.append(mu.cpu().detach().numpy().copy())
        lmbdas.append(lmbda.item())
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        rhos.append(rho.item())
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    return objs, lmbdas, mus, rhos, xs

objs, lmbdas, mus, rhos, xs = train(Y, loss_fn="Var")
23/8:
data = [
    go.Scatter(y=[i[0] for i in objs], name="lf_rho"),
    go.Scatter(y=[i[1] for i in objs], name="lf"),
    go.Scatter(y=[i[2] for i in objs], name="f"),
]

go.Figure(data=data).show()
px.line(xs)
px.line(rhos)
px.line(mus)
px.line(lmbdas)
23/9:
data = [
    go.Scatter(y=[i[0] for i in objs], name="lf_rho"),
    go.Scatter(y=[i[1] for i in objs], name="lf"),
    go.Scatter(y=[i[2] for i in objs], name="f"),
]

go.Figure(data=data).show()
px.line(xs)
px.line(rhos)
px.line(mus)
px.line(lmbdas)
23/10: px.line(xs)
24/1: print(torch.backends.mps.is_available())
24/2: import torch
24/3: print(torch.backends.mps.is_available())
24/4: print(torch.backends.mps.is_built())
24/5: torch.has_mps()
24/6: torch.has_mps
24/7: torch.backends.mps.device_count()
24/8: torch.device("mps")
24/9:
if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")
24/10:

N = 1000
s = np.linspace(0, 10, N)
y1 = np.sin(s)
y2 = np.cos(s)
plt.plot(y1)
plt.plot(y2)
Y = np.stack((y1, y2), axis=-1).T
Y = torch.from_numpy(Y)
Y = Y.to(device)
24/11:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
24/12:
N = 1000
s = np.linspace(0, 10, N)
y1 = np.sin(s)
y2 = np.cos(s)
plt.plot(y1)
plt.plot(y2)
Y = np.stack((y1, y2), axis=-1).T
Y = torch.from_numpy(Y)
Y = Y.to(device)
### Use Wind dataset

from Wind.load_data import DataLoader
data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y)
Y = Y.to(device)
24/13:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
# Use Apples Metal Performance Shaders (MPS) if available

if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")

def update_identity_rho(x, I, rho, mu, n_weights):
    zero = torch.tensor(0.0, dtype=torch.float32)
    for i in range(n_weights):
        if (-x[i]) < zero and torch.isclose(mu[i], zero):
            I[i, i] = 0
        else:
            I[i, i] = rho

def update_dual_variables(lmbda, mu, rho, x):
    with torch.no_grad():
        lmbda = lmbda + rho * (x.sum() - 1)
        mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))

def are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4):
    # dx L = 0
    dx = torch.autograd.grad(lf(x, lmbda, mu), x)[0]
    if torch.isclose(dx, torch.zeros_like(dx), atol=atol).all():
        # c(x) = 0 | x.sum()-1 = 0
        if torch.isclose((x.sum() - 1), torch.tensor(0.0, dtype=torch.float32), atol=atol):
            # h(x) <= 0 | (-x) <= 0
            if ((-x) <= 0.0).all():
                # mu >= 0
                if (mu >= 0.0).all():
                    # mu*.h(x) = 0
                    if torch.isclose((-x) * mu, torch.zeros_like(mu), atol=atol).all():
                        return True
    return False

def f(x, loss_fn="Var", alp=0.5):
    if loss_fn == "Var":
        y = torch.matmul(x, Y)
        return y.var()
    elif loss_fn == "Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return torch.triu(cov_matrix, diagonal=1).sum()
    elif loss_fn == "-Mean+Var":
        y = x[:, None] * Y
        return -alp * y.mean() + (1 - alp) * y.var()
    elif loss_fn == "-Mean+Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return -alp * y.mean() + (1 - alp) * torch.triu(cov_matrix, diagonal=1).sum()

def loss_var(x, Y, **kwargs):
    y = torch.matmul(x, Y)
    return y.var()

def loss_cov(x, Y, **kwargs):
    y = x[:, None] * Y
    cov_matrix = y.cov()
    return torch.triu(cov_matrix, diagonal=1).sum()

def loss_mean_var(x, Y, alp):
    y = x[:, None] * Y
    return -alp * y.mean() + (1 - alp) * y.var()

def loss_mean_cov(x,Y, alp):
    y = x[:, None] * Y
    cov_matrix = y.cov()
    return -alp * y.mean() + (1 - alp) * torch.triu(cov_matrix, diagonal=1).sum()

def lf(x, lmbda, mu, loss_fn):
    return loss_fn(x) + lmbda * (x.sum() - 1) + torch.matmul(-x, mu)

def lf_rho(x, lmbda, mu, rho, I_rho, loss_fn):
    return lf(x, lmbda, mu, loss_fn) + rho / 2 * (x.sum() - 1) ** 2 + 1 / 2 * torch.matmul(torch.matmul(-x, I_rho), (-x))
### Generate test dataset

N = 1000
s = np.linspace(0, 10, N)
y1 = np.sin(s)
y2 = np.cos(s)
plt.plot(y1)
plt.plot(y2)
Y = np.stack((y1, y2), axis=-1).T
Y = torch.from_numpy(Y)
Y = Y.to(device)
24/14:

N = 1000
s = np.linspace(0, 10, N, dtype=np.float32)
y1 = np.sin(s)
y2 = np.cos(s)
plt.plot(y1)
plt.plot(y2)
Y = np.stack((y1, y2), axis=-1).T
Y = torch.from_numpy(Y)
Y = Y.to(device)
24/15:

from Wind.load_data import DataLoader
data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y)
Y = Y.to(device)
# %%%

def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    objs = []
    xs = [x.cpu().detach().numpy().copy()]
    lmbdas = [lmbda.item()]
    mus = [mu.cpu().detach().numpy().copy()]
    rhos = [rho.item()]
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            xs.append(x.cpu().detach().numpy().copy())
            objs.append([obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), f(x, loss_fn=loss_fn).item()])
        objs.append(
            [
                lf_rho(x, lmbda, mu, rho, I_rho,loss_fn=loss_fn).item(),
                lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                f(x, loss_fn=loss_fn).item(),
            ]
        )
        mus.append(mu.cpu().detach().numpy().copy())
        lmbdas.append(lmbda.item())
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        rhos.append(rho.item())
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    return objs, lmbdas, mus, rhos, xs

objs, lmbdas, mus, rhos, xs = train(Y, loss_fn=lambda x: loss_var(x,Y))
24/16: Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
24/17:

Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
24/18: objs, lmbdas, mus, rhos, xs = train(Y, loss_fn=lambda x: loss_var(x,Y))
24/19:
def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    objs = []
    xs = [x.cpu().detach().numpy().copy()]
    lmbdas = [lmbda.item()]
    mus = [mu.cpu().detach().numpy().copy()]
    rhos = [rho.item()]
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            xs.append(x.cpu().detach().numpy().copy())
            objs.append([obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), f(x, loss_fn=loss_fn).item()])
        objs.append(
            [
                lf_rho(x, lmbda, mu, rho, I_rho,loss_fn=loss_fn).item(),
                lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                f(x, loss_fn=loss_fn).item(),
            ]
        )
        mus.append(mu.cpu().detach().numpy().copy())
        lmbdas.append(lmbda.item())
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        rhos.append(rho.item())
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    return objs, lmbdas, mus, rhos, xs
24/20: objs, lmbdas, mus, rhos, xs = train(Y, loss_fn=lambda x: loss_var(x,Y))
24/21:

def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    objs = []
    xs = [x.cpu().detach().numpy().copy()]
    lmbdas = [lmbda.item()]
    mus = [mu.cpu().detach().numpy().copy()]
    rhos = [rho.item()]
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            xs.append(x.cpu().detach().numpy().copy())
            objs.append([obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), f(x, loss_fn=loss_fn).item()])
        objs.append(
            [
                lf_rho(x, lmbda, mu, rho, I_rho,loss_fn=loss_fn).item(),
                lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                f(x, loss_fn=loss_fn).item(),
            ]
        )
        mus.append(mu.cpu().detach().numpy().copy())
        lmbdas.append(lmbda.item())
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        rhos.append(rho.item())
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    return objs, lmbdas, mus, rhos, xs

objs, lmbdas, mus, rhos, xs = train(Y, loss_fn=lambda x: loss_var(x,Y))
24/22:

def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    objs = []
    xs = [x.cpu().detach().numpy().copy()]
    lmbdas = [lmbda.item()]
    mus = [mu.cpu().detach().numpy().copy()]
    rhos = [rho.item()]
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            xs.append(x.cpu().detach().numpy().copy())
            objs.append([obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), f(x, loss_fn=loss_fn).item()])
        objs.append(
            [
                lf_rho(x, lmbda, mu, rho, I_rho,loss_fn=loss_fn).item(),
                lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                f(x, loss_fn=loss_fn).item(),
            ]
        )
        mus.append(mu.cpu().detach().numpy().copy())
        lmbdas.append(lmbda.item())
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        rhos.append(rho.item())
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    return objs, lmbdas, mus, rhos, xs

objs, lmbdas, mus, rhos, xs = train(Y, loss_fn=lambda x: loss_var(x,Y))
24/23:
n_weights = Y.shape[0]
x = np.ones(n_weights, dtype=np.float32)
x = x / x.sum(axis=-1)
x = torch.from_numpy(x)
x.requires_grad = True
x = x.to(device)
24/24:
rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
step_size = torch.tensor(1e-3, dtype=torch.float32)
n_steps = 250
n_iterations = 100
# Langrange multipliers

lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
I_rho = np.eye(n_weights, dtype=np.float32)
I_rho = torch.from_numpy(I_rho) * rho
I_rho = I_rho.to(device)
24/25: objs = torch.tensor([], device=device)
24/26: xs = torch.tensor([x.detach().numpy().copy()], device=device)
24/27: xs = torch.tensor([x.detach()], device=device)
24/28: x
24/29: x.detach()
24/30: x.detach().view()
24/31: x.detach().view(1)
24/32: x.detach().view(1,-1)
24/33: xs = torch.tensor([x.detach().view(1,-1)], device=device)
24/34: xs = torch.tensor(x.detach().view(1,-1), device=device)
24/35: xs = torch.tensor(x.clone().detach().view(1,-1), device=device)
24/36: torch.tensor(x.detach().clone().view(1,-1), device=device)
24/37: xs = torch.tensor([], device=device)
24/38: xs = torch.cat(xs, torch.tensor(x.detach().clone().view(1,-1)))
24/39: xs = torch.cat((xs, torch.tensor(x.detach().clone().view(1,-1))))
24/40: xs = torch.cat((xs, x.detach().clone().view(1,-1)))
24/41: xs
24/42: lmbdas = torch.tensor([lmbda.item()], device=device)
24/43:

mus = torch.tensor([], device=device)
mus = torch.cat((mus, mu.detach().clone().view(1,-1)))
24/44: mus
24/45: lmbdas
24/46: rhos = torch.tensor([rho.item()], device=device)
24/47:
objs= torch.cat((
    objs,
    torch.cat((obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), f(x, loss_fn=loss_fn).item()))
))
24/48: obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
24/49: loss_fn=lambda x: loss_var(x,Y)
24/50: obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
24/51:

objs= torch.cat((
    objs,
    torch.cat((obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), f(x, loss_fn=loss_fn).item()))
))
24/52: obj.item()
24/53: lf(x, lmbda, mu, loss_fn=loss_fn).item()
24/54: f(x, loss_fn=loss_fn).item()
24/55: loss_fn(x)
24/56: loss_fn(x).item()
24/57:

objs= torch.cat((
    objs,
    torch.cat((obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), loss_fn(x).item()))
))
24/58: torch.cat((obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), loss_fn(x).item()))
24/59: (obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), loss_fn(x).item())
24/60: lmbdas = torch.cat((lmbdas, lmbda.item()))
24/61: lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
24/62: lmbdas = torch.tensor([lmbda.item()], device=device)
24/63: lmbdas = torch.cat((lmbdas, lmbda.item()))
24/64: torch.tensor((obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), loss_fn(x).item()))
24/65:

objs= torch.cat((
    objs,
    torch.tensor((obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), loss_fn(x).item()))
))
24/66: torch.tensor((obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), loss_fn(x).item()), device=device)
24/67:

objs= torch.cat((
    objs,
    torch.tensor((obj.item(), lf(x, lmbda, mu, loss_fn=loss_fn).item(), loss_fn(x).item()), device=device)
))
24/68: SummaryWriter?
24/69: from torch.utils.tensorboard import SummaryWriter
24/70: SummaryWriter?
24/71: writer = SummaryWriter(log_dir="logs/")
24/72: from datetime import datetime
24/73: datetime.now().isoformat()
24/74:
def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    # objs = torch.tensor([], device=device)
    # xs = torch.tensor([], device=device)
    # xs = torch.cat((xs, x.detach().clone().view(1,-1)))
    # lmbdas = torch.tensor([lmbda.item()], device=device)
    # mus = torch.tensor([], device=device)
    # mus = torch.cat((mus, mu.detach().clone().view(1,-1)))
    # rhos = torch.tensor([rho.item()], device=device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            xs = torch.cat((xs, x.detach().clone().view(1,-1)))
            # Log scalar values for each step
            writer.add_scalar('Objective Function', obj.item(), it * n_steps + i)
            writer.add_scalar('Lf Function', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalar('Loss Function', loss_fn(x).item(), it * n_steps + i)
        # Log vector values for each iteration
        writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    writer.close()
24/75: train(Y, loss_fn=lambda x: loss_var(x,Y))
24/76:


def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    # objs = torch.tensor([], device=device)
    # xs = torch.tensor([], device=device)
    # xs = torch.cat((xs, x.detach().clone().view(1,-1)))
    # lmbdas = torch.tensor([lmbda.item()], device=device)
    # mus = torch.tensor([], device=device)
    # mus = torch.cat((mus, mu.detach().clone().view(1,-1)))
    # rhos = torch.tensor([rho.item()], device=device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalar('Objective Function', obj.item(), it * n_steps + i)
            writer.add_scalar('Lf Function', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalar('Loss Function', loss_fn(x).item(), it * n_steps + i)
        # Log vector values for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    writer.close()
24/77: train(Y, loss_fn=lambda x: loss_var(x,Y))
24/78:


def are_kkt_conditions_verified(x, lmbda, mu, loss_fn, atol=1e-4):
    # dx L = 0
    dx = torch.autograd.grad(lf(x, lmbda, mu, loss_fn), x)[0]
    if torch.isclose(dx, torch.zeros_like(dx), atol=atol).all():
        # c(x) = 0 | x.sum()-1 = 0
        if torch.isclose((x.sum() - 1), torch.tensor(0.0, dtype=torch.float32), atol=atol):
            # h(x) <= 0 | (-x) <= 0
            if ((-x) <= 0.0).all():
                # mu >= 0
                if (mu >= 0.0).all():
                    # mu*.h(x) = 0
                    if torch.isclose((-x) * mu, torch.zeros_like(mu), atol=atol).all():
                        return True
    return False
24/79:


def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    # objs = torch.tensor([], device=device)
    # xs = torch.tensor([], device=device)
    # xs = torch.cat((xs, x.detach().clone().view(1,-1)))
    # lmbdas = torch.tensor([lmbda.item()], device=device)
    # mus = torch.tensor([], device=device)
    # mus = torch.cat((mus, mu.detach().clone().view(1,-1)))
    # rhos = torch.tensor([rho.item()], device=device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalar('Objective Function', obj.item(), it * n_steps + i)
            writer.add_scalar('Lf Function', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalar('Loss Function', loss_fn(x).item(), it * n_steps + i)
        # Log vector values for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    writer.close()
24/80: train(Y, loss_fn=lambda x: loss_var(x,Y))
24/81:

def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    # objs = torch.tensor([], device=device)
    # xs = torch.tensor([], device=device)
    # xs = torch.cat((xs, x.detach().clone().view(1,-1)))
    # lmbdas = torch.tensor([lmbda.item()], device=device)
    # mus = torch.tensor([], device=device)
    # mus = torch.cat((mus, mu.detach().clone().view(1,-1)))
    # rhos = torch.tensor([rho.item()], device=device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalar('Objective Function', obj.item(), it * n_steps + i)
            writer.add_scalar('Lf Function', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalar('Loss Function', loss_fn(x).item(), it * n_steps + i)
        # Log vector values for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, loss_fn, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    writer.close()
24/82:

def train(Y, loss_fn):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(2.0, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(1.0, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(1e-3, dtype=torch.float32)
    n_steps = 250
    n_iterations = 100
    # Langrange multipliers
    lmbda = torch.tensor(-0.05, requires_grad=True, device=device)
    mu = torch.tensor([0.5 for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    # objs = torch.tensor([], device=device)
    # xs = torch.tensor([], device=device)
    # xs = torch.cat((xs, x.detach().clone().view(1,-1)))
    # lmbdas = torch.tensor([lmbda.item()], device=device)
    # mus = torch.tensor([], device=device)
    # mus = torch.cat((mus, mu.detach().clone().view(1,-1)))
    # rhos = torch.tensor([rho.item()], device=device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalar('Objective Function', obj.item(), it * n_steps + i)
            writer.add_scalar('Lf Function', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalar('Loss Function', loss_fn(x).item(), it * n_steps + i)
        # Log vector values for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, loss_fn, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    writer.close()
24/83: train(Y, loss_fn=lambda x: loss_var(x,Y))
24/84:
log_dir = 'logs'
# Create an EventAccumulator object

ea = event_accumulator.EventAccumulator(log_dir)
24/85:
from tensorboard.backend.event_processing import event_accumulator
# Define the log directory

log_dir = 'logs'
# Create an EventAccumulator object

ea = event_accumulator.EventAccumulator(log_dir)
# Load the data from the event files

ea.Reload()
24/86: scalar_data = ea.Scalars('Lf Function')
24/87: ea.accumulated_attrs
24/88: ea.histograms
24/89: ea.histograms()
24/90: ea.histograms.size
24/91: ea.histograms.Keys
24/92: ea.histograms.Keys()
24/93: ea.histograms.Items
24/94: ea.histograms.Items()
24/95:
log_dir = 'logs/2023-04-04T11:43:36.026434'
# Create an EventAccumulator object

ea = event_accumulator.EventAccumulator(log_dir)
24/96: ea.Reload()
24/97: scalar_data = ea.Scalars('Lf Function')
24/98:
values = [scalar.value for scalar in scalar_data]
timestamps = [scalar.timestamp for scalar in scalar_data]
24/99: values
24/100: writer.log_dir
24/101: writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
24/102: writer.log_dir
24/103: writer.close()
24/104: return writer.log_dir
24/105: writer.log_dir
24/106: from dataclasses import dataclass
24/107: hparams = Hyperparameters(**hparams_dict)
24/108:
hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
24/109:
@dataclass
class Hyperparameters:
    n_steps: int
    n_iterations: int
    rho: float
    rho_scaling: float
    step_size: float
    init_lmbda: float
    init_mu: float

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
24/110: hparams
24/111:
def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values',
                               {'f': loss_fn(x).item(),
                                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                                   'Lf_rho': obj.item()},
                               it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams = Hyperparameters(**{
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
# Create an EventAccumulator object

ea = event_accumulator.EventAccumulator(log_dir)
# Load the data from the event files

ea.Reload()
24/112: print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item();.3f}")
24/113: print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
24/114: it=2
24/115: print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
24/116: log_dir
24/117:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values',
                               {'f': loss_fn(x).item(),
                                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                                   'Lf_rho': obj.item()},
                               it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir
24/118:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values',
                               {'f': loss_fn(x).item(),
                                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                                   'Lf_rho': obj.item()},
                               it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir
24/119:
hparams = Hyperparameters(**{
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/120:


@dataclass
class Hyperparameters:
    n_steps: int
    n_iterations: int
    rho: float
    rho_scaling: float
    step_size: float
    init_lmbda: float
    init_mu: float

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values/',
                               {'f': loss_fn(x).item(),
                                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                                   'Lf_rho': obj.item()},
                               it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir
24/121:

hparams = Hyperparameters(**{
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/122:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)

    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations

    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)

    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)

    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")

    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]

            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)

            # Log scalar values for each step
            writer.add_scalars('Function values/f': loss_fn(x).item(), it * n_steps + i)
            writer.add_scalars('Function values/Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalars('Function values/Lf_rho': obj.item(), it * n_steps + i)

        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)

        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)

        rho = rho * rho_scaling

        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)

        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break

    writer.close()
    return writer.log_dir

hparams = Hyperparameters(**{
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

hparams = Hyperparameters(**{
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/123:

@dataclass
class Hyperparameters:
    n_steps: int
    n_iterations: int
    rho: float
    rho_scaling: float
    step_size: float
    init_lmbda: float
    init_mu: float

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values/f', loss_fn(x).item(), it * n_steps + i)
            writer.add_scalars('Function values/Lf', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalars('Function values/Lf_rho', obj.item(), it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams = Hyperparameters(**{
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

hparams = Hyperparameters(**{
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/124:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalar('Function values/f', loss_fn(x).item(), it * n_steps + i)
            writer.add_scalar('Function values/Lf', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalar('Function values/Lf_rho', obj.item(), it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams = Hyperparameters(**{
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

hparams = Hyperparameters(**{
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/125:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values/',{'f': loss_fn(x).item()}, it * n_steps + i)
            writer.add_scalars('Function values/',{'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item()}, it * n_steps + i)
            writer.add_scalars('Function values/',{'Lf_rho': obj.item()}, it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams = Hyperparameters(**{
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

hparams = Hyperparameters(**{
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
})

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/126: hparams = Hyperparameters(**hparams_dict)
24/127: from dataclasses import dataclass, asdict
24/128: asdict(hparams)
24/129:


def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_hparams(asdict(hparams))
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            with writer:
                writer.add_scalars('Function values/',{'f': loss_fn(x).item()}, it * n_steps + i)
                writer.add_scalars('Function values/',{'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item()}, it * n_steps + i)
                writer.add_scalars('Function values/',{'Lf_rho': obj.item()}, it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir
24/130:

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/131:
def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)

    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations

    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)

    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)

    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")

    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]

            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)

            # Log scalar values for each step
            with writer:
                writer.add_scalars('Function values/',{'f': loss_fn(x).item()}, it * n_steps + i)
                writer.add_scalars('Function values/',{'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item()}, it * n_steps + i)
                writer.add_scalars('Function values/',{'Lf_rho': obj.item()}, it * n_steps + i)

        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)

        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)

        rho = rho * rho_scaling

        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)

        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break

    writer.add_hparams(asdict(hparams),{"loss_fn": loss_fn(x).item()}
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/132:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            with writer:
                writer.add_scalars('Function values/',{'f': loss_fn(x).item()}, it * n_steps + i)
                writer.add_scalars('Function values/',{'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item()}, it * n_steps + i)
                writer.add_scalars('Function values/',{'Lf_rho': obj.item()}, it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.add_hparams(asdict(hparams),{"loss_fn": loss_fn(x).item()})
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/133:

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/134:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}/")
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values/',{'f': loss_fn(x).item()}, it * n_steps + i)
            writer.add_scalars('Function values/',{'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item()}, it * n_steps + i)
            writer.add_scalars('Function values/',{'Lf_rho': obj.item()}, it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.add_hparams(asdict(hparams),{"loss_fn": loss_fn(x).item()})
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/135:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    layout = {
    "Function values": {
            "loss": ["Multiline", ["loss/f", "loss/Lf", "loss/Lf_rho"]],
        },
    }
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}/")
    writer.add_custom_scalars(layout=layout)
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalar('Function values/f', loss_fn(x).item(), it * n_steps + i)
            writer.add_scalar('Function values/Lf', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalar('Function values/Lf_rho', obj.item(), it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.add_hparams(asdict(hparams),{"loss_fn": loss_fn(x).item()})
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/136:


def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    layout = {
    "Function values": {
            "loss": ["Multiline", ["loss/f", "loss/Lf", "loss/Lf_rho"]],
        },
    }
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_custom_scalars(layout=layout)
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalar('loss/f', loss_fn(x).item(), it * n_steps + i)
            writer.add_scalar('loss/Lf', lf(x, lmbda, mu, loss_fn=loss_fn).item(), it * n_steps + i)
            writer.add_scalar('loss/Lf_rho', obj.item(), it * n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.add_hparams(asdict(hparams),{"loss_fn": loss_fn(x).item()})
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/137:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values',{
                    'f': loss_fn(x).item(),
                    'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                    'Lf_rho': obj.item()
                })
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/138:

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/139:


@dataclass
class Hyperparameters:
    n_steps: int
    n_iterations: int
    rho: float
    rho_scaling: float
    step_size: float
    init_lmbda: float
    init_mu: float

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
# Create an EventAccumulator object

ea = event_accumulator.EventAccumulator(log_dir)
24/140:
def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/141:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values/', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().item(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().item(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/142:

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach(), it)
            # Log scalar values for each step
            writer.add_scalars('Function values/', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach(), it)
        writer.add_histogram('mu', mu.detach(), it)
        writer.add_histogram('rho', rho.detach(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir
24/143:

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/144: lmbda.detach()
24/145: mu.detach()
24/146:
@dataclass
class Hyperparameters:
    n_steps: int
    n_iterations: int
    rho: float
    rho_scaling: float
    step_size: float
    init_lmbda: float
    init_mu: float

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it*n_steps+i)
            # Log scalar values for each step
            writer.add_scalars('Function values/', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().cpu().numpy(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.3f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir
24/147:

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
24/148: lmbda.clone().cpu().numpy()
24/149: lmbda.detach().clone().cpu().numpy()
24/150: mu.detach().clone().cpu().numpy()
25/1:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
# Use Apples Metal Performance Shaders (MPS) if available

if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")

def update_identity_rho(x, I, rho, mu, n_weights):
    zero = torch.tensor(0.0, dtype=torch.float32)
    for i in range(n_weights):
        if (-x[i]) < zero and torch.isclose(mu[i], zero):
            I[i, i] = 0
        else:
            I[i, i] = rho

def update_dual_variables(lmbda, mu, rho, x):
    with torch.no_grad():
        lmbda = lmbda + rho * (x.sum() - 1)
        mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))

def are_kkt_conditions_verified(x, lmbda, mu, loss_fn, atol=1e-4):
    # dx L = 0
    dx = torch.autograd.grad(lf(x, lmbda, mu, loss_fn), x)[0]
    if torch.isclose(dx, torch.zeros_like(dx), atol=atol).all():
        # c(x) = 0 | x.sum()-1 = 0
        if torch.isclose((x.sum() - 1), torch.tensor(0.0, dtype=torch.float32), atol=atol):
            # h(x) <= 0 | (-x) <= 0
            if ((-x) <= 0.0).all():
                # mu >= 0
                if (mu >= 0.0).all():
                    # mu*.h(x) = 0
                    if torch.isclose((-x) * mu, torch.zeros_like(mu), atol=atol).all():
                        return True
    return False

def f(x, loss_fn="Var", alp=0.5):
    if loss_fn == "Var":
        y = torch.matmul(x, Y)
        return y.var()
    elif loss_fn == "Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return torch.triu(cov_matrix, diagonal=1).sum()
    elif loss_fn == "-Mean+Var":
        y = x[:, None] * Y
        return -alp * y.mean() + (1 - alp) * y.var()
    elif loss_fn == "-Mean+Cov":
        y = x[:, None] * Y
        cov_matrix = y.cov()
        return -alp * y.mean() + (1 - alp) * torch.triu(cov_matrix, diagonal=1).sum()

def loss_var(x, Y, **kwargs):
    y = torch.matmul(x, Y)
    return y.var()

def loss_cov(x, Y, **kwargs):
    y = x[:, None] * Y
    cov_matrix = y.cov()
    return torch.triu(cov_matrix, diagonal=1).sum()

def loss_mean_var(x, Y, alp):
    y = x[:, None] * Y
    return -alp * y.mean() + (1 - alp) * y.var()

def loss_mean_cov(x, Y, alp):
    y = x[:, None] * Y
    cov_matrix = y.cov()
    return -alp * y.mean() + (1 - alp) * torch.triu(cov_matrix, diagonal=1).sum()

def lf(x, lmbda, mu, loss_fn):
    return loss_fn(x) + lmbda * (x.sum() - 1) + torch.matmul(-x, mu)

def lf_rho(x, lmbda, mu, rho, I_rho, loss_fn):
    return lf(x, lmbda, mu, loss_fn) + rho / 2 * (x.sum() - 1) ** 2 + 1 / 2 * torch.matmul(torch.matmul(-x, I_rho), (-x))
# Generate test dataset

N = 1000
s = np.linspace(0, 10, N, dtype=np.float32)
y1 = np.sin(s)
y2 = np.cos(s)
plt.plot(y1)
plt.plot(y2)
Y = np.stack((y1, y2), axis=-1).T
Y = torch.from_numpy(Y)
Y = Y.to(device)
# Use Wind dataset

data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
# %%%

@dataclass
class Hyperparameters:
    n_steps: int
    n_iterations: int
    rho: float
    rho_scaling: float
    step_size: float
    init_lmbda: float
    init_mu: float

def train(Y, loss_fn, hparams: Hyperparameters):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram('x', x.detach().clone().cpu().numpy(), it*n_steps+i)
            # Log scalar values for each step
            writer.add_scalars('Function values/', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().cpu().numpy(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
25/2:

log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams
)
25/3:
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams,
    writer=writer
)
25/4:
def train(Y, loss_fn, hparams: Hyperparameters, writer):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram(
                'x', x.detach().clone().cpu().numpy(), it*n_steps+i)
            # Log scalar values for each step
            writer.add_scalars('Function values/', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().cpu().numpy(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams,
    writer=writer
)
25/5: writer.all_writers?
25/6: writer.all_writers()
25/7:
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
writer.all_writers()
25/8: writer.all_writers
25/9:
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams,
    writer=writer
)
25/10: writer.all_writers
25/11: writer.all_writers
25/12: a=writer.all_writers
25/13: a
25/14: from Wind.summary_writer import SummaryWriter
25/15:

@dataclass
class Hyperparameters:
    n_steps: int
    n_iterations: int
    rho: float
    rho_scaling: float
    step_size: float
    init_lmbda: float
    init_mu: float

def train(Y, loss_fn, hparams: Hyperparameters, writer):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram(
                'x', x.detach().clone().cpu().numpy(), it*n_steps+i)
            # Log scalar values for each step
            writer.add_scalars('Function values/', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().cpu().numpy(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir

hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
a=writer.all_writers
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams,
    writer=writer
)
# Create an EventAccumulator object
25/16: writer.add_hparams(asdict(hparams), {}, run_name='.')
25/17: from torch.utils.tensorboard import SummaryWriter
25/18:
ea = event_accumulator.EventAccumulator(log_dir)
# Load the data from the event files

ea.Reload()
25/19: ea.accumulated_attrs
25/20: ea.scalars
25/21: ea.scalars.Items
25/22: ea.scalars.Items()
25/23: ea.summary_metadata
25/24: ea.tensors
25/25: scalar_data = ea.Scalars('Function values')
25/26: scalar_data = ea.Scalars('Function values__Lf')
25/27: scalar_data = ea.Scalars('Lf')
25/28: event_acc.Tags()['scalars']
25/29:
event_acc = event_accumulator.EventAccumulator(log_dir)
# Load the data from the event files

event_acc.Reload()
# Extract the "scalar" values and their timestamps

scalar_data = event_acc.Scalars('Lf')
event_acc.Tags()['scalars']
25/30: event_acc.Tags()['scalars']
25/31: log_dir
25/32: event_acc = event_accumulator.EventAccumulator(log_dir+"Function values__f")
25/33:

event_acc.Reload()
# Extract the "scalar" values and their timestamps
25/34:
event_acc = event_accumulator.EventAccumulator(log_dir+"/Function values__f")
# Load the data from the event files

event_acc.Reload()
25/35: event_acc.Tags()['scalars']
25/36: scalar_data = event_acc.Scalars('Fuction values/')
25/37: event_acc.Tags()['scalars']
25/38: scalar_data = event_acc.Scalars('Function values/')
25/39: scalar_data
25/40: values = [scalar.value for scalar in scalar_data]
25/41: values
25/42: timestamps = [scalar.timestamp for scalar in scalar_data]
25/43:
# %%%


@dataclass
class Hyperparameters:
    n_steps: int
    n_iterations: int
    rho: float
    rho_scaling: float
    step_size: float
    init_lmbda: float
    init_mu: float


def train(Y, loss_fn, hparams: Hyperparameters, writer):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)

    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations

    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)

    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)

    writer.add_hparams(asdict(hparams), {})

    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]

            writer.add_histogram(
                'x', x.detach().clone().cpu().numpy(), it*n_steps+i)

            # Log scalar values for each step
            writer.add_scalars('Function values/', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)

        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().cpu().numpy(), it)

        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)

        rho = rho * rho_scaling

        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)

        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break

    writer.close()
    return writer.log_dir


hparams_dict = {
    "n_steps": 250, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams_dict = {
    "n_steps": 25, "n_iterations": 10, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
a=writer.all_writers
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams,
    writer=writer
)

# Create an EventAccumulator object
event_acc = event_accumulator.EventAccumulator(log_dir+"/Function values__f")

# Load the data from the event files
event_acc.Reload()

# Extract the "scalar" values and their timestamps
scalar_data = event_acc.Scalars('Function values/')

event_acc.Tags()['scalars']

# Extract the values and timestamps for each scalar
values = [scalar.value for scalar in scalar_data]
timestamps = [scalar.timestamp for scalar in scalar_data]

scalar_data

data = [
    go.Scatter(y=[i[0] for i in objs], name="lf_rho"),
    go.Scatter(y=[i[1] for i in objs], name="lf"),
    go.Scatter(y=[i[2] for i in objs], name="f"),
]
go.Figure(data=data).show()
px.line(xs)
px.line(rhos)

px.line(mus)
px.line(lmbdas)


weights = []
loss_fns = ["Var", "Cov", "-Mean+Var", "-Mean+Cov"]
for loss_fn in tqdm(loss_fns):
    objs, lmbdas, mus, rhos, xs = train(Y, loss_fn=loss_fn)
    weights.append(xs[-1])
25/44: scalar_data
25/45: [scalar.wall_time for scalar in scalar_data]
25/46: timestamps = [scalar.wall_time for scalar in scalar_data]
25/47: vals = get_data_from_summary_writer(log_dir+"/Function values__f")
25/48:
def get_data_from_summary_writer(log_dir):
    # Create an EventAccumulator object
    event_acc = event_accumulator.EventAccumulator(log_dir)
    # Load the data from the event files
    event_acc.Reload()
    # Extract the "scalar" values and their timestamps
    scalar_data = event_acc.Scalars('Function values')
    # Extract the values and timestamps for each scalar
    values = [scalar.value for scalar in scalar_data]
    timestamps = [scalar.wall_time for scalar in scalar_data]
    return values

vals = get_data_from_summary_writer(log_dir+"/Function values__f")
25/49:
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams,
    writer=writer
)
25/50:
def get_data_from_summary_writer(log_dir):
    # Create an EventAccumulator object
    event_acc = event_accumulator.EventAccumulator(log_dir)
    # Load the data from the event files
    event_acc.Reload()
    # Extract the "scalar" values and their timestamps
    scalar_data = event_acc.Scalars('Function values')
    # Extract the values and timestamps for each scalar
    values = [scalar.value for scalar in scalar_data]
    timestamps = [scalar.wall_time for scalar in scalar_data]
    return values

vals = get_data_from_summary_writer(log_dir+"/Function values__f")
25/51: event_acc = event_accumulator.EventAccumulator(log_dir+"/Function values__f")
25/52: event_acc.Reload()
25/53: event_acc.Tags()['scalars']
25/54:

def train(Y, loss_fn, hparams: Hyperparameters, writer):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer.add_hparams(asdict(hparams), {})
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram(
                'x', x.detach().clone().cpu().numpy(), it*n_steps+i)
            # Log scalar values for each step
            writer.add_scalars('Function values', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().cpu().numpy(), it)
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir
25/55:
log_dir = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams,
    writer=writer
)
25/56:
event_acc = event_accumulator.EventAccumulator(log_dir+"/Function values__f")
# Load the data from the event files

event_acc.Reload()
event_acc.Tags()['scalars']
# Extract the "scalar" values and their timestamps

scalar_data = event_acc.Scalars('Function values')
# Extract the values and timestamps for each scalar

values = [scalar.value for scalar in scalar_data]
25/57: event_acc = event_accumulator.EventAccumulator(log_dir+"/Function values__f")
25/58: event_acc.Reload()
25/59: event_acc = event_accumulator.EventAccumulator(log_dir+"/Function values_f")
25/60: event_acc.Reload()
25/61: event_acc.Tags()['scalars']
25/62: scalar_data = event_acc.Scalars('Function values')
25/63: values = [scalar.value for scalar in scalar_data]
25/64: values
25/65: scalar="f"
25/66: summary_dir = log_dir+ "/Function values_" + scalar
25/67: summary_dir
25/68: event_acc = event_accumulator.EventAccumulator(summary_dir)
25/69:
event_acc = event_accumulator.EventAccumulator(summary_dir)
# Load the data from the event files

event_acc.Reload()
25/70: scalar_data = event_acc.Scalars('Function values')
25/71: values = [scalar.value for scalar in scalar_data]
25/72: vals = get_data_from_summary_writer(log_dir, "f")
25/73:

def get_data_from_summary_writer(log_dir, scalar="f"):
    summary_dir = log_dir+ "/Function values_" + scalar
    # Create an EventAccumulator object
    event_acc = event_accumulator.EventAccumulator(summary_dir)
    # Load the data from the event files
    event_acc.Reload()
    # event_acc.Tags()['scalars']
    # Extract the "scalar" values and their timestamps
    scalar_data = event_acc.Scalars('Function values')
    # Extract the values and timestamps for each scalar
    values = [scalar.value for scalar in scalar_data]
    timestamps = [scalar.wall_time for scalar in scalar_data]
    return values

vals = get_data_from_summary_writer(log_dir, "f")
25/74:

df = pd.DataFrame({
    "f": get_data_from_summary_writer(log_dir, "f"),
    "Lf": get_data_from_summary_writer(log_dir, "Lf"),
    "Lf_rho": get_data_from_summary_writer(log_dir, "Lf_rho"),
})
25/75: df.plot()
25/76: pd.options.plotting.backend = "plotly"
25/77:
df = pd.DataFrame({
    "f": get_data_from_summary_writer(log_dir, "f"),
    "Lf": get_data_from_summary_writer(log_dir, "Lf"),
    "Lf_rho": get_data_from_summary_writer(log_dir, "Lf_rho"),
})

df.plot()
25/78: event_acc.Tags()['histograms']
25/79:
summary_dir = log_dir
# Create an EventAccumulator object

event_acc = event_accumulator.EventAccumulator(summary_dir)
# Load the data from the event files

event_acc.Reload()
25/80: event_acc.Tags()['histograms']
25/81: vars = event_acc.Tags()['histograms']
25/82: scalar_data = event_acc.Histograms(vars[0])
25/83: scalar_data
25/84: values = [scalar.value for scalar in scalar_data]
25/85:
for i in scalar_data:
    print(i)
25/86: dir(i)
25/87: i.histogram_value
25/88: Y.shape
25/89: dir(i.histogram_value)
25/90: [i for i in dir(i.histogram_value) if "__" not in i]
25/91: i.histogram_value.bucket
25/92: vars = event_acc.Tags()['distributions']
25/93: vars
25/94: scalar_data = event_acc.Histograms(vars[0])
25/95:
for i in scalar_data:
    print(i)
25/96:

xs = torch.tensor([], device=device)
xs = torch.cat(xs, x.detach().view(1,-1))
25/97:
n_weights = Y.shape[0]
x = np.ones(n_weights, dtype=np.float32)
x = x / x.sum(axis=-1)
x = torch.from_numpy(x)
x.requires_grad = True
x = x.to(device)
25/98:
xs = torch.tensor([], device=device)
xs = torch.cat(xs, x.detach().view(1,-1))
25/99: xs = torch.cat((xs, x.detach().view(1,-1)))
25/100: xs
25/101: xs = torch.cat((xs, x.detach().view(1,-1)))
25/102: xs
25/103:

def train(Y, loss_fn, hparams: Hyperparameters, writer):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    writer.add_hparams(asdict(hparams), {})
    xs = torch.tensor([], device=device)
    xs = torch.cat((xs, x.detach().view(1,-1)))
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram(
                'x', x.detach().clone().cpu().numpy(), it*n_steps+i)
            # Log scalar values for each step
            writer.add_scalars('Function values', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().cpu().numpy(), it)
        # Log x
        xs = torch.cat((xs, x.detach().view(1,-1)))
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.close()
    return writer.log_dir, xs
25/104:

log_dir, xs = train(
    Y,
    loss_fn=lambda x: loss_var(x, Y),
    hparams=hparams,
    writer=writer
)
25/105: xs
25/106: xs.numpy()
25/107: xs.cpu().numpy()
25/108: px.line(xs)
25/109: xs=xs.cpu().numpy()
25/110: px.line(xs)
25/111:

def train(Y, loss_fn, hparams: Hyperparameters, writer):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(
        hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor([hparams.init_mu for i in range(n_weights)],
                      requires_grad=True, dtype=torch.float32, device=device)
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    xs = torch.tensor([], device=device)
    xs = torch.cat((xs, x.detach().view(1,-1)))
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram(
                'x', x.detach().clone().cpu().numpy(), it*n_steps+i)
            # Log scalar values for each step
            writer.add_scalars('Function values', {
                'f': loss_fn(x).item(),
                'Lf': lf(x, lmbda, mu, loss_fn=loss_fn).item(),
                'Lf_rho': obj.item()
            }, it*n_steps + i)
        # Log multipliers and rho for each iteration
        writer.add_histogram('lmbda', lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram('mu', mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram('rho', rho.detach().clone().cpu().numpy(), it)
        # Log x
        xs = torch.cat((xs, x.detach().view(1,-1)))
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(
            x, lmbda, mu, loss_fn, atol=1e-4)
        print(
            f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.add_hparams(asdict(hparams), {"loss": obj.item()})
    writer.close()
    return writer.log_dir, xs.cpu().numpy()
25/112: asdict(hparams)
25/113: output_string = ", ".join([f"{key}: {value}" for key, value in asdict(hparams).items()])
25/114: output_string
25/115: "_".join([f"{key}={value}" for key, value in asdict(hparams).items()])
25/116: "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
25/117:

step_sizes = [5e-3, 1e-3, 5e-4]
for step_size in step_sizes:
    hparams.step_size = step_size
    hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
    log_dir, xs = train(
        Y,
        loss_fn=lambda x: loss_var(x, Y),
        hparams=hparams,
        writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}")
    )
25/118:

step_sizes = [5e-3, 1e-3, 5e-4, 1e-4]
for step_size in step_sizes:
    hparams.step_size = step_size
    hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
    log_dir, xs = train(
        Y,
        loss_fn=lambda x: loss_var(x, Y),
        hparams=hparams,
        writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}")
    )
25/119:

hparams_dict = {
    "n_steps": 25, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/
25/120:

hparams_dict = {
    "n_steps": 25, "n_iterations": 100, "rho": 2.0, "rho_scaling": 1.0, "step_size": 1e-3, "init_lmbda": -0.05, "init_mu": 0.5
}

hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
25/121:

step_sizes = [5e-3, 1e-3, 5e-4, 1e-4]
for step_size in step_sizes:
    hparams.step_size = step_size
    hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
    log_dir, xs = train(
        Y,
        loss_fn=lambda x: loss_var(x, Y),
        hparams=hparams,
        writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}")
    )
25/122:

step_sizes = [1e-2, 5e-3, 1e-3, 5e-4]
rhos = [1.5, 2, 2.5]
rho_scalings = [0.9, 1, 1.1]
25/123:
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            hparams.rho_scaling = rho_scaling
            hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
            log_dir, xs = train(
                Y,
                loss_fn=lambda x: loss_var(x, Y),
                hparams=hparams,
                writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}")
            )
25/124: rho_scaling
25/125:
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            hparams.rho_scaling = rho_scaling
            hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
            log_dir, xs = train(
                Y,
                loss_fn=lambda x: loss_var(x, Y),
                hparams=hparams,
                writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}")
            )
25/126: rho_scalings = [0.9, 1]
25/127:

step_sizes = [1e-2, 5e-3, 1e-3, 5e-4]
rhos = [1.5, 2, 2.5]
rho_scalings = [0.9, 0.99]
25/128:

for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            hparams.rho_scaling = rho_scaling
            hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
            log_dir, xs = train(
                Y,
                loss_fn=lambda x: loss_var(x, Y),
                hparams=hparams,
                writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}")
            )
25/129:
step_sizes = [1e-1, 8e-2, 4e-2, 1.5e-2, 8e-3]
rhos = [1.6, 2, 2.4]
rho_scalings = [0.9, 0.99]
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            print(f"{hparams_as_str}")
            hparams.rho_scaling = rho_scaling
            hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
            log_dir, xs = train(
                Y,
                loss_fn=lambda x: loss_var(x, Y),
                hparams=hparams,
                writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}"),
            )
25/130:
step_sizes = [1e-1, 8e-2, 4e-2, 1.5e-2, 8e-3]
rhos = [1.6, 2, 2.4]
rho_scalings = [0.9, 0.99]
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            print(f"{hparams_as_str}")
            hparams.rho_scaling = rho_scaling
            hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
            log_dir, xs = train(
                Y,
                loss_fn=lambda x: loss_var(x, Y),
                hparams=hparams,
                writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}"),
            )
25/131: log_dir, xs = train(Y, loss_fn=lambda x: loss_var(x, Y), hparams=hparams, writer=writer)
25/132:

def train(Y, loss_fn, hparams: Hyperparameters, writer):
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor(
        [hparams.init_mu for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device
    )
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    xs = torch.tensor([], device=device)
    xs = torch.cat((xs, x.detach().view(1, -1)))
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram("x", x.detach().clone().cpu().numpy(), it * n_steps + i)
            # Log scalar values for each step
            writer.add_scalars(
                "Function values",
                {"f": loss_fn(x).item(), "Lf": lf(x, lmbda, mu, loss_fn=loss_fn).item(), "Lf_rho": obj.item()},
                it * n_steps + i,
            )
        # Log multipliers and rho for each iteration
        writer.add_histogram("lmbda", lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram("mu", mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram("rho", rho.detach().clone().cpu().numpy(), it)
        # Log x
        xs = torch.cat((xs, x.detach().view(1, -1)))
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.add_hparams(asdict(hparams), {"loss": obj.item()})
    writer.close()
    return writer.log_dir, xs.cpu().numpy()
25/133:

hparams_dict = {
    "n_steps": 25,
    "n_iterations": 100,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.5,
}

hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
log_dir, xs = train(Y, loss_fn=lambda x: loss_var(x, Y), hparams=hparams, writer=writer)
25/134:
step_sizes = [1e-1, 8e-2, 4e-2, 1.5e-2, 8e-3]
rhos = [1.6, 2, 2.4]
rho_scalings = [0.9, 0.99]
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            print(f"{hparams_as_str}")
            hparams.rho_scaling = rho_scaling
            hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
            log_dir, xs = train(
                Y,
                loss_fn=lambda x: loss_var(x, Y),
                hparams=hparams,
                writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}"),
            )
25/135:
step_sizes = [8e-2, 4e-2, 1.5e-2, 8e-3]
rhos = [1.6, 2, 2.4]
rho_scalings = [0.9, 0.99]
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            print(f"{hparams_as_str}")
            hparams.rho_scaling = rho_scaling
            hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
            log_dir, xs = train(
                Y,
                loss_fn=lambda x: loss_var(x, Y),
                hparams=hparams,
                writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}"),
            )
25/136:
step_sizes = [5e-2, 4e-2, 1.5e-2, 8e-3]
rhos = [1.6, 2, 2.4]
rho_scalings = [0.9, 0.99]
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            print(f"{hparams_as_str}")
            hparams.rho_scaling = rho_scaling
            hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
            log_dir, xs = train(
                Y,
                loss_fn=lambda x: loss_var(x, Y),
                hparams=hparams,
                writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}"),
            )
25/137:
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            try:
                print(f"{hparams_as_str}")
                hparams.rho_scaling = rho_scaling
                hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
                log_dir, xs = train(
                    Y,
                    loss_fn=lambda x: loss_var(x, Y),
                    hparams=hparams,
                    writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}"),
                )
            except Exception:
                print("failed")
25/138:
step_sizes = [0.05, 2e-2, 1.5e-2, 8e-3]
rhos = [1.6, 2, 2.4]
rho_scalings = [0.9]
hparams.n_iterations = 200
for step_size in step_sizes:
    hparams.step_size = step_size
    for rho in rhos:
        hparams.rho = rho
        for rho_scaling in rho_scalings:
            try:
                print(f"{hparams_as_str}")
                hparams.rho_scaling = rho_scaling
                hparams_as_str = "|".join([f"{key}={value}" for key, value in asdict(hparams).items()])
                log_dir, xs = train(
                    Y,
                    loss_fn=lambda x: loss_var(x, Y),
                    hparams=hparams,
                    writer=SummaryWriter(log_dir=f"logs/{hparams_as_str}/{datetime.now().isoformat()}"),
                )
            except Exception:
                print("failed")
25/139: log_dir = "logs/n_steps=25|n_iterations=10|rho=2.0|rho_scaling=1.0|step_size=0.0001|init_lmbda=-0.05|init_mu=0.5/2023-04-04T14:58:12.625100/1680613095.617959
25/140: log_dir = "logs/n_steps=25|n_iterations=10|rho=2.0|rho_scaling=1.0|step_size=0.0001|init_lmbda=-0.05|init_mu=0.5/2023-04-04T14:58:12.625100/1680613095.617959"
25/141:

df = pd.DataFrame(
    {
        "f": get_scalar_data_from_summary_writer(log_dir, "f"),
        "Lf": get_scalar_data_from_summary_writer(log_dir, "Lf"),
        "Lf_rho": get_scalar_data_from_summary_writer(log_dir, "Lf_rho"),
    }
)

df.plot()
25/142:
def get_scalar_data_from_summary_writer(log_dir, scalar="f"):
    summary_dir = log_dir + "/Function values_" + scalar
    # Create an EventAccumulator object
    event_acc = event_accumulator.EventAccumulator(summary_dir)
    # Load the data from the event files
    event_acc.Reload()
    # event_acc.Tags()['scalars']
    # Extract the "scalar" values and their timestamps
    scalar_data = event_acc.Scalars("Function values")
    # Extract the values and timestamps for each scalar
    values = [scalar.value for scalar in scalar_data]
    timestamps = [scalar.wall_time for scalar in scalar_data]
    return values
25/143:

df = pd.DataFrame(
    {
        "f": get_scalar_data_from_summary_writer(log_dir, "f"),
        "Lf": get_scalar_data_from_summary_writer(log_dir, "Lf"),
        "Lf_rho": get_scalar_data_from_summary_writer(log_dir, "Lf_rho"),
    }
)

df.plot()
25/144: log_dir = "logs/n_steps=25|n_iterations=10|rho=2.0|rho_scaling=1.0|step_size=0.0001|init_lmbda=-0.05|init_mu=0.5/2023-04-04T14:58:12.625100"
25/145: vals = get_scalar_data_from_summary_writer(log_dir, "f")
25/146:
df = pd.DataFrame(
    {
        "f": get_scalar_data_from_summary_writer(log_dir, "f"),
        "Lf": get_scalar_data_from_summary_writer(log_dir, "Lf"),
        "Lf_rho": get_scalar_data_from_summary_writer(log_dir, "Lf_rho"),
    }
)

df.plot()
25/147: log_dir = "logs/n_steps=25|n_iterations=100|rho=2.5|rho_scaling=0.9|step_size=0.001|init_lmbda=-0.05|init_mu=0.5/2023-04-04T21:01:34.989612"
25/148:

df = pd.DataFrame(
    {
        "f": get_scalar_data_from_summary_writer(log_dir, "f"),
        "Lf": get_scalar_data_from_summary_writer(log_dir, "Lf"),
        "Lf_rho": get_scalar_data_from_summary_writer(log_dir, "Lf_rho"),
    }
)

df.plot()
25/149:
log_dir = "logs/n_steps=25|n_iterations=100|rho=2|rho_scaling=0.9|step_size=0.0005|init_lmbda=-0.05|init_mu=0.5/2023-04-04T21:03:32.803743"
vals = get_scalar_data_from_summary_writer(log_dir, "f")
df = pd.DataFrame(
    {
        "f": get_scalar_data_from_summary_writer(log_dir, "f"),
        "Lf": get_scalar_data_from_summary_writer(log_dir, "Lf"),
        "Lf_rho": get_scalar_data_from_summary_writer(log_dir, "Lf_rho"),
    }
)

df.plot()
25/150:
log_dir = "logs/n_steps=25|n_iterations=100|rho=2.0|rho_scaling=1.0|step_size=0.005|init_lmbda=-0.05|init_mu=0.5/2023-04-04T20:15:03.327843"
vals = get_scalar_data_from_summary_writer(log_dir, "f")
df = pd.DataFrame(
    {
        "f": get_scalar_data_from_summary_writer(log_dir, "f"),
        "Lf": get_scalar_data_from_summary_writer(log_dir, "Lf"),
        "Lf_rho": get_scalar_data_from_summary_writer(log_dir, "Lf_rho"),
    }
)

df.plot()
25/151:
hparams_dict = {
    "n_steps": 250,
    "n_iterations": 100,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.5,
}
25/152: hparams = Hyperparameters(**hparams_dict)
25/153: log_dir, xs = train(Y, loss_fn=lambda x: loss_var(x, Y), hparams=hparams, writer=writer)
25/154: log_dir
25/155: px.line(xs)
25/156:
def train(Y, loss_fn, hparams: Hyperparameters, writer):
    # Initialize x evenly
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor(
        [hparams.init_mu for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device
    )
    # Penalty matrix
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    # Log x and mu values
    xs = torch.tensor([], device=device)
    xs = torch.cat((xs, x.detach().view(1, -1)))
    mus = torch.tensor([], device=device)
    mus = torch.cat((mus, mu.detach().view(1, -1)))
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram("x", x.detach().clone().cpu().numpy(), it * n_steps + i)
            # Log scalar values for each step
            writer.add_scalars(
                "Function values",
                {"f": loss_fn(x).item(), "Lf": lf(x, lmbda, mu, loss_fn=loss_fn).item(), "Lf_rho": obj.item()},
                it * n_steps + i,
            )
        # Log multipliers and rho for each iteration
        writer.add_histogram("lmbda", lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram("mu", mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram("rho", rho.detach().clone().cpu().numpy(), it)
        # Log x and mu
        xs = torch.cat((xs, x.detach().view(1, -1)))
        mus = torch.cat((mus, mu.detach().view(1, -1)))
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.add_hparams(asdict(hparams), {"loss": obj.item()})
    writer.close()
    return writer.log_dir, xs.cpu().numpy()
25/157:
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
25/158:

def train(Y, loss_fn, hparams: Hyperparameters, writer):
    # Initialize x evenly
    n_weights = Y.shape[0]
    x = np.ones(n_weights, dtype=np.float32)
    x = x / x.sum(axis=-1)
    x = torch.from_numpy(x)
    x.requires_grad = True
    x = x.to(device)
    # Hyperparamters
    rho = torch.tensor(hparams.rho, dtype=torch.float32)  # Testdata = 1.1
    rho_scaling = torch.tensor(hparams.rho_scaling, dtype=torch.float32)  # Testdata= 1.1
    step_size = torch.tensor(hparams.step_size, dtype=torch.float32)
    n_steps = hparams.n_steps
    n_iterations = hparams.n_iterations
    # Langrange multipliers
    lmbda = torch.tensor(hparams.init_lmbda, requires_grad=True, device=device)
    mu = torch.tensor(
        [hparams.init_mu for i in range(n_weights)], requires_grad=True, dtype=torch.float32, device=device
    )
    # Penalty matrix
    I_rho = np.eye(n_weights, dtype=np.float32)
    I_rho = torch.from_numpy(I_rho) * rho
    I_rho = I_rho.to(device)
    # Log x and mu values
    xs = torch.tensor([], device=device)
    xs = torch.cat((xs, x.detach().view(1, -1)))
    mus = torch.tensor([], device=device)
    mus = torch.cat((mus, mu.detach().view(1, -1)))
    # Training loop
    for it in range(n_iterations):
        # solve for current lagrangian multipliers
        for i in range(n_steps):
            obj = lf_rho(x, lmbda, mu, rho, I_rho, loss_fn=loss_fn)
            dx = torch.autograd.grad(obj, x)
            with torch.no_grad():
                x -= step_size * dx[0]
            writer.add_histogram("x", x.detach().clone().cpu().numpy(), it * n_steps + i)
            # Log scalar values for each step
            writer.add_scalars(
                "Function values",
                {"f": loss_fn(x).item(), "Lf": lf(x, lmbda, mu, loss_fn=loss_fn).item(), "Lf_rho": obj.item()},
                it * n_steps + i,
            )
        # Log multipliers and rho for each iteration
        writer.add_histogram("lmbda", lmbda.detach().clone().cpu().numpy(), it)
        writer.add_histogram("mu", mu.detach().clone().cpu().numpy(), it)
        writer.add_histogram("rho", rho.detach().clone().cpu().numpy(), it)
        # Log x and mu
        xs = torch.cat((xs, x.detach().view(1, -1)))
        mus = torch.cat((mus, mu.detach().view(1, -1)))
        # Update lagrangian multipliers and rho
        with torch.no_grad():
            lmbda = lmbda + rho * (x.sum() - 1)
            mu = torch.maximum(torch.zeros_like(x), mu + rho * (-x))
        update_identity_rho(x, I_rho, rho, mu, n_weights)
        rho = rho * rho_scaling
        # Assert KKT Conditions
        converged = are_kkt_conditions_verified(x, lmbda, mu, loss_fn, atol=1e-4)
        print(f"Iteration: {it} - objective value lagrangian: {lf(x, lmbda, mu, loss_fn=loss_fn).item():.5f}")
        if converged:
            print("KKT conditions met")
            break
    writer.add_hparams(asdict(hparams), {"loss": obj.item()})
    writer.close()
    return writer.log_dir, xs.cpu().numpy(), mus.cpu().numpy()
25/159: log_dir, xs, mus = train(Y, loss_fn=lambda x: loss_var(x, Y), hparams=hparams, writer=writer)
25/160:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
25/161:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
26/1:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
26/2:
pd.options.plotting.backend = "plotly"
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
# Use Apples Metal Performance Shaders (MPS) if available

if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")

hparams_dict = {
    "n_steps": 25,
    "n_iterations": 100,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.5,
}
# Use test dataset
# Y = torch.from_numpy(get_test_dataset()).to(device=device)
# Use Wind dataset

data_loader = DataLoader()
26/3:
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
26/4:
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
26/5: model = AugmentedLagrangian(hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
26/6:

model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.fit(writer=writer)
26/7:

from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
26/8:

model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
27/1:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
pd.options.plotting.backend = "plotly"
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
# Use Apples Metal Performance Shaders (MPS) if available

if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")

hparams_dict = {
    "n_steps": 25,
    "n_iterations": 100,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.5,
}
# Use test dataset
# Y = torch.from_numpy(get_test_dataset()).to(device=device)
# Use Wind dataset

data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
27/2:
%reload_ext autoreload
%autoreload 2
27/3:

from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
27/4:
model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
28/1:
%reload_ext autoreload
%autoreload 2
28/2:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
pd.options.plotting.backend = "plotly"
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
# Use Apples Metal Performance Shaders (MPS) if available

if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")

hparams_dict = {
    "n_steps": 25,
    "n_iterations": 100,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.5,
}
# Use test dataset
# Y = torch.from_numpy(get_test_dataset()).to(device=device)
# Use Wind dataset

data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
28/3: writer.log_dir
28/4: from time import time
28/5: time.now()
28/6: time()
28/7:
import time
time.now()
28/8: time.perf_counter?
28/9:

model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
28/10:
device="cpu"
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
28/11:

hparams_dict = {
    "n_steps": 250,
    "n_iterations": 100,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.5,
}
28/12:
data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
28/13:
df = pd.DataFrame(
    {
        "f": get_scalar_data_from_summary_writer(log_dir, "f"),
        "Lf": get_scalar_data_from_summary_writer(log_dir, "Lf"),
        "Lf_rho": get_scalar_data_from_summary_writer(log_dir, "Lf_rho"),
    }
)
28/14:
model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
28/15:
df = pd.DataFrame(
    {
        "f": model.get_scalar_data_from_summary_writer("f"),
        "Lf": model.get_scalar_data_from_summary_writer("Lf"),
        "Lf_rho": model.get_scalar_data_from_summary_writer("Lf_rho"),
    }
)

df.plot()
28/16: px.line(model.xs)
28/17: px.line(model.xs, labels=data_loader.df.columns)
28/18: data_loader.df.columns
28/19: data_loader.df.columns.to_list()
28/20: px.line(model.xs, labels=data_loader.df.columns.to_list())
28/21: df_xs = pd.DataFrame(xs, columns=data_loader.df.columns)
28/22: df_xs = pd.DataFrame(model.xs, columns=data_loader.df.columns)
28/23: px.line(df_xs)
28/24:
df_mus = pd.DataFrame(model.mus, columns=data_loader.df.columns)
px.line(df_mus)
28/25:
hparams_dict = {
    "n_steps": 250,
    "n_iterations": 100,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.001,
}
28/26:

hparams_dict = {
    "n_steps": 250,
    "n_iterations": 1000,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.001,
}
# Use test dataset
# Y = torch.from_numpy(get_test_dataset()).to(device=device)
# Use Wind dataset

data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
model.train()
df = pd.DataFrame(
    {
        "f": model.get_scalar_data_from_summary_writer("f"),
        "Lf": model.get_scalar_data_from_summary_writer("Lf"),
        "Lf_rho": model.get_scalar_data_from_summary_writer("Lf_rho"),
    }
)

df.plot()
df_xs = pd.DataFrame(model.xs, columns=data_loader.df.columns)
px.line(df_xs)
df_mus = pd.DataFrame(model.mus, columns=data_loader.df.columns)
px.line(df_mus)
28/27: df.plot()
28/28:
df_xs = pd.DataFrame(model.xs, columns=data_loader.df.columns)
px.line(df_xs)
28/29:
breakpoint()
# %%
weights = []
loss_fns = ["Var", "Cov", "-Mean+Var", "-Mean+Cov"]
for loss_fn in tqdm(loss_fns):
    objs, lmbdas, mus, rhos, xs = train(Y, loss_fn=loss_fn)
    weights.append(xs[-1])
28/30: hparams
28/31:
weights = []
loss_fns = ["Var", "Cov", "-Mean+Var", "-Mean+Cov"]
loss_fns_dict = {
    "Var": lambda x: loss_var(x, Y),
    "Cov": lambda x: loss_cov(x, Y),
    "-Mean+Var": lambda x: loss_mean_var(x, Y,alp=0.5),
    "-Mean+Cov": lambda x: loss_mean_cov(x, Y, alp=0.5),
}

models = {}
for loss_fn in tqdm(loss_fns):
    writer = SummaryWriter(log_dir=f"logs/{loss_fn}/{datetime.now().isoformat()}")
    models[loss_fn] = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=loss_fns_dict[loss_fn], writer=writer, device=device)
    weights.append(models[loss_fn].xs[-1])
28/32:
df_weights = pd.DataFrame(np.around(100 * np.array(weights).T, 2), index=data_loader.df.columns, columns=loss_fns).iloc[
    ::-1
]
28/33: weights
28/34: models[loss_fn].xs[-1].numpy()
28/35:
models = {}
for loss_fn in tqdm(loss_fns):
    writer = SummaryWriter(log_dir=f"logs/{loss_fn}/{datetime.now().isoformat()}")
    models[loss_fn] = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=loss_fns_dict[loss_fn], writer=writer, device=device)
    weights.append(models[loss_fn].xs[-1].numpy())
28/36: weights
28/37: weights = []
28/38:
models = {}
for loss_fn in tqdm(loss_fns):
    writer = SummaryWriter(log_dir=f"logs/{loss_fn}/{datetime.now().isoformat()}")
    models[loss_fn] = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=loss_fns_dict[loss_fn], writer=writer, device=device)
    weights.append(models[loss_fn].xs[-1].numpy())
28/39:
df_weights = pd.DataFrame(np.around(100 * np.array(weights).T, 2), index=data_loader.df.columns, columns=loss_fns).iloc[
    ::-1
]
28/40: df_weights
28/41:
# %%
weights = []
loss_fns = ["Var", "Cov", "-Mean+Var", "-Mean+Cov"]

loss_fns_dict = {
    "Var": lambda x: loss_var(x, Y),
    "Cov": lambda x: loss_cov(x, Y),
    "-Mean+Var": lambda x: loss_mean_var(x, Y,alp=0.5),
    "-Mean+Cov": lambda x: loss_mean_cov(x, Y, alp=0.5),
}

models = {}
for loss_fn in tqdm(loss_fns):
    writer = SummaryWriter(log_dir=f"logs/{loss_fn}/{datetime.now().isoformat()}")
    models[loss_fn] = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=loss_fns_dict[loss_fn], writer=writer, device=device)
    models[loss_fn].train()
    weights.append(models[loss_fn].xs[-1].numpy())
29/1:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
pd.options.plotting.backend = "plotly"
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
# Use Apples Metal Performance Shaders (MPS) if available
# if torch.backends.mps.is_available() and torch.backends.mps.is_built():
#     device = torch.device("mps")

hparams_dict = {
    "n_steps": 250,
    "n_iterations": 1000,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 5e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.001,
}
# Use test dataset
# Y = torch.from_numpy(get_test_dataset()).to(device=device)
# Use Wind dataset

data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
hparams = Hyperparameters(**hparams_dict)
step_sizes = [0.05, 2e-2, 1.5e-2, 8e-3]
rhos = [2]
rho_scalings = [1]
hparams.n_iterations = 200
29/2:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
pd.options.plotting.backend = "plotly"
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device("cpu")
# Use Apples Metal Performance Shaders (MPS) if available
# if torch.backends.mps.is_available() and torch.backends.mps.is_built():
#     device = torch.device("mps")

hparams_dict = {
    "n_steps": 250,
    "n_iterations": 1000,
    "rho": 2.0,
    "rho_scaling": 1.0,
    "step_size": 1e-3,
    "init_lmbda": -0.05,
    "init_mu": 0.001,
}
# Use test dataset
# Y = torch.from_numpy(get_test_dataset()).to(device=device)
# Use Wind dataset

data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
hparams = Hyperparameters(**hparams_dict)
writer = SummaryWriter(log_dir=f"logs/{datetime.now().isoformat()}")
29/3: model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
29/4: x = model.x
29/5: y = torch.matmul(x, Y)
29/6: y
29/7: y.var()
29/8: y = x[:, None] * Y
29/9: y
29/10: cov_matrix = y.cov()
29/11: cov_matrix
29/12: torch.triu(cov_matrix, diagonal=1).sum()
29/13: y = x[:, None] * Y
29/14: y
29/15: y.mean()
29/16: y.shape
29/17: torch.matmul(x, Y).mean()
29/18: y = torch.matmul(x, Y)
29/19: y.mean()
29/20: y = x[:, None] * Y
29/21: y.mean()
29/22: y = x[:, None] * Y
29/23: y.shape
29/24: y.sum().mean()
29/25: y.sum(axis=0)
29/26: y.sum(axis=0).mean()
29/27: Y = torch.from_numpy(get_test_dataset()).to(device=device)
29/28: model = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=lambda x: loss_var(x, Y), writer=writer, device=device)
29/29: x = model.x
29/30: y = torch.matmul(x, Y)
29/31: y
29/32: y.mean()
29/33: y = x[:, None] * Y
29/34: y.sum(axis=0).mean()
29/35: y = x[:, None] * Y
29/36: y.cov()
29/37: torch.triu(cov_matrix, diagonal=1).sum()
29/38: torch.triu(cov_matrix, diagonal=1)
29/39: cov_matrix = y.cov()
29/40: cov_matrix
29/41: torch.triu(cov_matrix, diagonal=1)
29/42: torch.triu(cov_matrix, diagonal=1).sum()
29/43: y = torch.matmul(x, Y)
29/44: y
29/45: y.var()
29/46: y.mean()
29/47: datetime.now().strftime("%y-%m-%dT%H:%M:%S")
29/48: datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
29/49: SummaryWriter(log_dir=f"logs/{loss_fn}/{datetime.now().strftime("%Y-%m-%dT%H:%M:%S")}")
29/50: SummaryWriter(log_dir=f"logs/{loss_fn}/{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}")
30/1:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_cov, loss_mean_cov, loss_mean_var
device = "cpu"
# Use Wind dataset

data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
loss_fns_dict = {
    "-0.5*Mean+0.5*Var": lambda x: loss_mean_var(x, Y,alp=0.5),
    "-0.3*Mean+0.7*Var": lambda x: loss_mean_var(x, Y,alp=0.3),
    "-0.1*Mean+0.9*Var": lambda x: loss_mean_var(x, Y,alp=0.1),
    "-0.0*Mean+1.0*Var": lambda x: loss_mean_var(x, Y,alp=0.1)
}

hparams = Hyperparameters(**{
            "n_steps": 250,
            "n_iterations": 1000,
            "rho": 2.0,
            "rho_scaling": 1.0,
            "step_size": 0.02,
            "init_lmbda": -0.05,
            "init_mu": 0.001,
        }
    )

weights = []
models = {}
for loss_fn in tqdm(loss_fns_dict):
    writer = SummaryWriter(log_dir=f"logs/{loss_fn}/{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}")
    models[loss_fn] = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=loss_fns_dict[loss_fn], writer=writer, device=device)
    models[loss_fn].train()
    weights.append(models[loss_fn].xs[-1].numpy())
30/2:
from tensorboard.backend.event_processing import event_accumulator
from Wind.load_data import DataLoader, get_test_dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import torch
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from dataclasses import dataclass, asdict
from Wind.AugmentedLagrangian.augmented_lagrangian import Hyperparameters, AugmentedLagrangian
from Wind.AugmentedLagrangian.loss_functions import loss_var, loss_mean_var
device = "cpu"
# Use Wind dataset

data_loader = DataLoader()
Y = data_loader.df.values.T
Y = torch.from_numpy(Y).to(dtype=torch.float32, device=device)
loss_fns_dict = {
    "-0.5*Mean+0.5*Var": lambda x: loss_mean_var(x, Y,alp=0.5),
    "-0.3*Mean+0.7*Var": lambda x: loss_mean_var(x, Y,alp=0.3),
    "-0.1*Mean+0.9*Var": lambda x: loss_mean_var(x, Y,alp=0.1),
    "-0.0*Mean+1.0*Var": lambda x: loss_mean_var(x, Y,alp=0.1)
}

hparams = Hyperparameters(**{
            "n_steps": 250,
            "n_iterations": 1000,
            "rho": 2.0,
            "rho_scaling": 1.0,
            "step_size": 0.02,
            "init_lmbda": -0.05,
            "init_mu": 0.001,
        }
    )

weights = []
models = {}
for loss_fn in tqdm(loss_fns_dict):
    writer = SummaryWriter(log_dir=f"logs/{loss_fn}/{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}")
    models[loss_fn] = AugmentedLagrangian(Y=Y, hyperparameters=hparams, loss_fn=loss_fns_dict[loss_fn], writer=writer, device=device)
    models[loss_fn].train()
    weights.append(models[loss_fn].xs[-1].numpy())
30/3:
df_weights = pd.DataFrame(np.around(100 * np.array(weights).T, 2), index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]
30/4:
properties = {}
for loss_fn in loss_fns_dict:
    properties[loss_fn] = {}
    y=models[loss_fn].xs[:, None] * Y
    properties[loss_fn]["Mean"] = y.mean()
    properties[loss_fn]["Variance"] =  y.std()
30/5: Y
30/6: models[loss_fn].xs[:, None]
30/7: models[loss_fn].xs[-1, None].shape
30/8: models[loss_fn].xs[-1, None].shape * Y
30/9: models[loss_fn].xs[-1, None] * Y
30/10:
# %%
df_weights = pd.DataFrame(np.around(100 * np.array(weights).T, 2), index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]

properties = {}
for loss_fn in loss_fns_dict:
    properties[loss_fn] = {}

    y=models[loss_fn].xs[-1, None] * Y.shape
    properties[loss_fn]["Mean"] = y.mean()
    properties[loss_fn]["Variance"] =  y.std()

print(
    df_weights.style.to_latex(
        hrules=True,
        label="tab:developments-by-loss-fn",
        # escape=False,
        column_format="|" + "|".join(["l" for _ in df_weights.columns]) + "|",
        caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    )
)
30/11: Y.shape
30/12: models[loss_fn].xs[-1] * Y.shape
30/13: models[loss_fn].xs[-1] * Y
30/14: torch.matmul(models[loss_fn].xs[-1], Y)
30/15: y.shape
30/16: y=torch.matmul(models[loss_fn].xs[-1], Y)
30/17: y.shape
30/18:
properties = {}
for loss_fn in loss_fns_dict:
    properties[loss_fn] = {}
    y=torch.matmul(models[loss_fn].xs[-1], Y)
    properties[loss_fn]["Mean"] = y.mean()
    properties[loss_fn]["Variance"] =  y.std()
30/19: pd.DataFrame(properties)
30/20: y.mean().numpy()
30/21:

properties = {}
for loss_fn in loss_fns_dict:
    properties[loss_fn] = {}
    y=torch.matmul(models[loss_fn].xs[-1], Y)
    properties[loss_fn]["Mean"] = y.mean().numpy()
    properties[loss_fn]["Variance"] =  y.std().numpy()
30/22: pd.DataFrame(properties)
30/23: df_weights
30/24:
df_properties = pd.DataFrame(properties)
pd.concat((df_weights,df_properties))
30/25: data_loader.df_nve_wind_locations.columns
30/26: data_loader.df_nve_wind_locations
30/27: data_loader.df_nve_wind_locations["locataion"]
30/28: data_loader.df_nve_wind_locations["location"]
30/29: data_loader.df_nve_wind_locations["location"].to_list()
30/30: data_loader.df_locations
30/31: all_areas = data_loader.df_locations["location"].to_list()
30/32: all_areas
30/33:
data_loader = DataLoader()
norwegian_areas = data_loader.df_nve_wind_locations["location"].to_list()
30/34: norwegian_areas
30/35: all_areas
30/36: data_loader.df
30/37:
df_properties = pd.DataFrame(properties)
df_table = pd.concat((df_weights,df_properties))
print(
    df_table.style.to_latex(
        hrules=True,
        label="tab:developments-by-loss-fn",
        # escape=False,
        column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
        caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    )
)
30/38:
print(
    df_table.round(2).style.to_latex(
        hrules=True,
        label="tab:developments-by-loss-fn",
        # escape=False,
        column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
        caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    )
)
# %%
30/39:
print(
    df_table.round(2).style.to_latex(
        hrules=True,
        label="tab:developments-by-loss-fn",
        # escape=False,
        column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
        caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    )
)
30/40: df_table.round(2)
30/41:
print(
    df_table.style.to_latex(
        hrules=True,
        label="tab:developments-by-loss-fn",
        # escape=False,
        column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
        caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    )
)
30/42: df_table
30/43: df_table.round(2)
30/44: df_table.info()
30/45: weights
30/46: df_weights
30/47:
df_weights = pd.DataFrame(np.around(100 * np.array(weights).T, 2), index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]
30/48: df_weights.info()
30/49: df_properties.info()
30/50: y.mean().numpy()
30/51: df_properties = pd.DataFrame(properties)
30/52: df_properties
30/53: df_properties.info()
30/54: y.std().numpy()[0]
30/55: properties
30/56: df_properties = pd.DataFrame(properties, dtype=np.float32)
30/57: df_properties.info()
30/58: df_table = pd.concat((df_weights,df_properties))
30/59: df_table.info()
30/60:
df_weights = pd.DataFrame(np.array(weights).T, index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]

df_weights.info()
30/61: df_weights
30/62:
print(
    df_table.round(2).style.to_latex(
        hrules=True,
        label="tab:developments-by-loss-fn",
        # escape=False,
        column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
        caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    )
)
30/63:
print(
    df_table.round(2).style.to_latex(
        hrules=True,
        label="tab:developments-by-loss-fn",
        # escape=False,
        column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
        caption="Overview of how the model would weight the different wind farms given the different loss functions.",
        float_format="%.2f"
    )
)
30/64: df_table
30/65:
print(df_table.to_latex(
    index=True,
    column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
    caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    float_format="{:.2f}".format,
))
30/66: col_sums = df_table.sum()
30/67: col_sums
30/68:
df_weights = pd.DataFrame(np.array(weights).T, index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]
30/69:

properties = {}
for loss_fn in loss_fns_dict:
    properties[loss_fn] = {}
    y=torch.matmul(models[loss_fn].xs[-1], Y)
    properties[loss_fn]["Mean"] = y.mean().numpy()
    properties[loss_fn]["Variance"] =  y.std().numpy()

df_properties = pd.DataFrame(properties, dtype=np.float32)
df_properties.info()
df_table = pd.concat((df_weights,df_properties))
30/70: col_sums = df_table.sum()
30/71: col_sums
30/72: col_sums = df_weights.sum()
30/73: col_sums
30/74: col_sums = df_weights.sum()
30/75: col_sums
30/76:
df_weights = df_weights.apply(lambda x: round(x/col_sums[x.name]*100,2) if col_sums[x.name]>0 else x)
# Calculate the sum of each column after rounding

new_col_sums = df_weights.sum()
# Adjust the final value in each column to ensure that the sum adds up to 100

df_weights = df_weights.apply(lambda x: round(x+100-new_col_sums[x.name],2) if new_col_sums[x.name] != 100 else x)
30/77: df_weights.sum()
30/78: df_weights
30/79:
df_weights = pd.DataFrame(np.array(weights).T, index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]
30/80: df_weights.sum()
30/81: df_weights.apply(lambda x: round(x/col_sums[x.name],2))
30/82: df_weights.apply(lambda x: round(x/col_sums[x.name],2)).sum()
30/83: df_weights_rounded = df_weights.round(2)
30/84: df_weights_rounded-df_weights_rounded
30/85: df_weights_rounded-df_weights
30/86: (df_weights_rounded-df_weights).round(3)
30/87: df_diff.sum()
30/88:
df_diff = (df_weights_rounded-df_weights)
df_diff.sum()
30/89:
df_weights = pd.DataFrame(np.array(weights).T, index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]
30/90: df = df_weights.copy()
30/91:
df = df_weights.copy()
# Get the sum of each column

col_sums = df.sum()
30/92: col_sums[x.name]
30/93: df = df.apply(lambda x: round(x/col_sums[x.name],2)).sum()
30/94: df
30/95: df = df_weights.copy()
30/96: df.sum()
30/97: df.round(2)
30/98: df.apply(lambda x: round(x/col_sums[x.name],2)).sum()
30/99: df_rounded
30/100: df_rounded = df.apply(lambda x: round(x/col_sums[x.name],2)).sum()
30/101: df_rounded
30/102: df_rounded.sum()
30/103:
col_sums = df.sum()
col_rounded_sums = df.round(2).sum()
col_sums - col_rounded_sums
30/104: df.sum()
30/105: df + (col_sums - col_rounded_sums)/df.sum()
30/106: (df + (col_sums - col_rounded_sums)/df.sum()).round(2).sum()
30/107: df_weights.sum()-df_weights.round(2).sum()
30/108: (df_weights-df_weights.round(2))
30/109: (df_weights-df_weights.round(2)).round(3)
30/110: df_weights
30/111: df_weights.round(2).sum()
30/112:
print(df_table.to_latex(
    index=True,
    column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
    caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    float_format="{:.3f}".format,
    label="tab:developments-by-loss-fn"
))
30/113: df_weights.round(3)
30/114: df_weights.round(3).sum()
30/115: df_table = pd.concat((df_weights,df_properties))
30/116:
print(df_table.to_latex(
    index=True,
    column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
    caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    float_format="{:.3f}".format,
    label="tab:developments-by-loss-fn"
))
30/117:

df_weights = pd.DataFrame(np.array(weights)*100.T, index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]

properties = {}
for loss_fn in loss_fns_dict:
    properties[loss_fn] = {}

    y=torch.matmul(models[loss_fn].xs[-1], Y)*100
    properties[loss_fn]["Mean"] = y.mean().numpy()
    properties[loss_fn]["Variance"] =  y.std().numpy()

df_properties = pd.DataFrame(properties, dtype=np.float32)
df_properties.info()

df_table = pd.concat((df_weights,df_properties))

print(df_table.to_latex(
    index=True,
    column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
    caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    float_format="{:.3f}".format,
    label="tab:developments-by-loss-fn"
))
30/118:
df_weights = pd.DataFrame(np.array(weights).T*100, index=data_loader.df.columns, columns=loss_fns_dict.keys()).iloc[
    ::-1
]

properties = {}
for loss_fn in loss_fns_dict:
    properties[loss_fn] = {}
    y=torch.matmul(models[loss_fn].xs[-1], Y)*100
    properties[loss_fn]["Mean"] = y.mean().numpy()
    properties[loss_fn]["Variance"] =  y.std().numpy()

df_properties = pd.DataFrame(properties, dtype=np.float32)
df_properties.info()
df_table = pd.concat((df_weights,df_properties))
print(df_table.to_latex(
    index=True,
    column_format="|" + "|".join(["l" for _ in df_table.columns]) + "|",
    caption="Overview of how the model would weight the different wind farms given the different loss functions.",
    float_format="{:.3f}".format,
    label="tab:developments-by-loss-fn"
))
31/1: client_path = "/Users/mah/gitsource/wind-covariation/Wind/rninja_client.py"
31/2: import sys
31/3: client_path = "/Users/mah/gitsource/wind-covariation/Wind"
31/4: import sys
31/5: sys.path.append(client_path)
31/6: from rninja_client import NinjaClient
31/7: NinjaClient?
31/8:
from configparser import ConfigParser
config = ConfigParser()
config.read("config.ini")
web_token = config["Renewables Ninja"]["token"]
31/9: client = NinjaClient(web_token= )
31/10: client = NinjaClient(web_token)
31/11: df_wind, meta = ninja_client.get_wind_dataframe(lat=lat, long=lon, date_from=date_from, date_to=date_to)
31/12:
date_from="2022-01-01"
date_to="2022-12-31"
lat="63.200580"
lon="8.060748"
31/13: df_wind, meta = client.get_wind_dataframe(lat=lat, long=lon, date_from=date_from, date_to=date_to)
31/14:
date_from="2021-01-01"
date_to="2020-12-31"
lat="63.200580"
lon="8.060748"
31/15: df_wind, meta = client.get_wind_dataframe(lat=lat, long=lon, date_from=date_from, date_to=date_to)
31/16:
date_from="2020-01-01"
date_to="2020-12-31"
lat="63.200580"
lon="8.060748"
31/17: df_wind, meta = client.get_wind_dataframe(lat=lat, long=lon, date_from=date_from, date_to=date_to)
31/18: df_wind
31/19: df_wind.plot()
31/20: from entsoe import EntsoePandasClient
31/21: from entsoe import EntsoePandasClient
31/22: pd.set_option("plotting.backend", "plotly")
31/23:
import sys
import pandas as pd
31/24: pd.set_option("plotting.backend", "plotly")
31/25:
from configparser import ConfigParser
config = ConfigParser()
config.read("config.ini")
web_token = config["Renewables Ninja"]["token"]
31/26: from entsoe import EntsoePandasClient
31/27: web_token = config["entsoe"]["security_token"]
31/28: client = EntsoePandasClient(api_key=web_token)
31/29: df_price = client.query_day_ahead_prices("BZN|NO3", start=date_from, end=date_to)
31/30: date_from = pd.Timestamp('2020-01-01 00:00:00', tz='Europe/Osloe')
31/31: date_from = pd.Timestamp('2020-01-01 00:00:00', tz='Europe/Oslo')
31/32:
date_from = pd.Timestamp('2020-01-01 00:00:00', tz='Europe/Oslo')
date_to = pd.Timestamp('2020-12-31 23:00:00', tz='Europe/Oslo')
31/33: df_wind, meta = client.get_wind_dataframe(lat=lat, long=lon, date_from=date_from, date_to=date_to)
31/34:
from configparser import ConfigParser
config = ConfigParser()
config.read("config.ini")
web_token = config["Renewables Ninja"]["token"]
31/35: pd.set_option("plotting.backend", "plotly")
31/36: ninja_client = NinjaClient(web_token)
31/37:
date_from = pd.Timestamp('2020-01-01 00:00:00', tz='Europe/Oslo')
date_to = pd.Timestamp('2020-12-31 23:00:00', tz='Europe/Oslo')
31/38: df_wind, meta = client.get_wind_dataframe(lat=lat, long=lon, date_from=date_from, date_to=date_to)
31/39: df_wind, meta = ninja_client.get_wind_dataframe(lat=lat, long=lon, date_from=date_from, date_to=date_to)
31/40: date_from.isoformat()
31/41:
df_wind, meta = ninja_client.get_wind_dataframe(
    lat=lat,
    long=lon,
    date_from=date_from.isoformat(),
    date_to=date_to.isoformat()
)
31/42: date_from.date
31/43: date_from.date()
31/44:
df_wind, meta = ninja_client.get_wind_dataframe(
    lat=lat,
    long=lon,
    date_from=date_from.date().isoformat(),
    date_to=date_to.date().isoformat()
)
31/45: df_wind.plot()
31/46: df_wind
31/47: from entsoe import EntsoePandasClient
31/48: web_token = config["entsoe"]["security_token"]
31/49: entsoe_client = EntsoePandasClient(api_key=web_token)
31/50: df_price = entsoe_client.query_day_ahead_prices("BZN|NO3", start=date_from, end=date_to)
31/51:
df_price = entsoe_client.query_day_ahead_prices?#(
    "BZN|NO3", start=date_from, end=date_to
)
31/52:
df_price = entsoe_client.query_day_ahead_prices(
    "BZN|NO3", start=date_from, end=date_to
)
31/53:
df_price = entsoe_client.query_day_ahead_prices(
    "BZN|NO1", start=date_from, end=date_to
)
31/54:
df_price = entsoe_client.query_day_ahead_prices(
    "NO", start=date_from, end=date_to
)
31/55: entsoe_client.query_day_ahead_prices?
31/56: from entsoe.mappings import Area
31/57: Area?
31/58: Area.NO_3
31/59:
df_price = entsoe_client.query_day_ahead_prices(
    Area.NO_3, start=date_from, end=date_to
)
31/60: df_price.head)=
31/61: df_price.head()
31/62: df_price.plot()
33/1:
from urllib.request import urlretrieve
from os.path import basename

urls = [
    "https://tubcloud.tu-berlin.de/s/mxgaA7wH8NsmdDi/download/Natura2000_end2021-PT.gpkg",
    "https://tubcloud.tu-berlin.de/s/XoDpBcweJHmYKgF/download/GEBCO_2014_2D-PT.nc",
    "https://tubcloud.tu-berlin.de/s/Mw5dwbwdsDY8zfH/download/U2018_CLC2018_V2020_20u1-PT.tif",
    "https://tubcloud.tu-berlin.de/s/2oogpgBfM5n4ssZ/download/PORTUGAL-2013-01-era5.nc",
]
for url in urls:
    urlretrieve(url, basename(url))
33/2: import geopandas as gpd
33/3: url = "https://tubcloud.tu-berlin.de/s/7bpHrAkjMT3ADSr/download/country_shapes.geojson"
33/4: countries = gpd.read_file(url).set_index('name')
33/5: countries.tail()
33/6: countries.plot(edgecolor='k', facecolor='lightgrey')
33/7:
import matplotlib.pyplot as plt
import cartopy
import cartopy.crs as ccrs
33/8:
crs = ccrs.EqualEarth()

fig = plt.figure(figsize=(10,5))

ax = plt.axes(projection=crs)

countries.to_crs(crs.proj4_init).plot(
    ax=ax,
    edgecolor='k',
    facecolor='lightgrey'
)
33/9: import rasterio
33/10: clc = rasterio.open('U2018_CLC2018_V2020_20u1-PT.tif')
33/11: band = clc.read(1)
33/12: type(band)
33/13: band.shape
33/14: import numpy as np
33/15: np.unique(band)
33/16: plt.imshow(band, cmap='tab20')
33/17: band = clc.read(2)
33/18: band = clc.read(1)
33/19: clc.count
33/20: type(band)
33/21: band.shape
33/22: import numpy as np
33/23: np.unique(band)
33/24: plt.imshow(band, cmap='tab20')
33/25:
from rasterio.plot import show
show(band, cmap='tab20')
33/26: clc.crs
33/27: clc.bounds
33/28: clc.transform
33/29:
from rasterio.plot import show
show(band, transform=clc.transform, cmap='tab20')
33/30: gebco = rasterio.open('GEBCO_2014_2D-PT.nc')
33/31: band = gebco.read(1)
33/32: np.unique(band)
33/33:
fig, ax = plt.subplots(figsize=(4,8))

countries.loc[["PT", "ES"]].plot(ax=ax, color='none')

show(band, transform=gebco.transform, cmap='RdBu_r', ax=ax)
33/34: from atlite.gis import ExclusionContainer
33/35: excluder = ExclusionContainer(crs=3035)
33/36: excluder.add_geometry('Natura2000_end2021-PT.gpkg')
33/37: from atlite.gis import shape_availability
33/38:
shape = countries.to_crs(excluder.crs).loc[["PT"]].geometry
shape[0]
33/39: band, transform = shape_availability(shape, excluder)
33/40: band
33/41: band.any()
33/42: transform
33/43:
fig, ax = plt.subplots(figsize=(4,8))
shape.plot(ax=ax, color='none')
show(band, transform=transform, cmap='Greens', ax=ax)
33/44: eligible_cells = band.sum()
33/45: cell_area = excluder.res**2
33/46: eligible_area = cell_area * eligible_cells
33/47: country_area = shape.geometry.area[0]
33/48: eligible_area / country_area * 100
33/49:
excluder = ExclusionContainer()
excluder.add_geometry('Natura2000_end2021-PT.gpkg')
33/50: fn = 'U2018_CLC2018_V2020_20u1-PT.tif'
33/51: codes = [12, 13, 18, 19, 20, 21]
33/52: excluder.add_raster(fn, codes=codes, crs=3035, invert=True)
33/53: codes = [1, 2, 3, 4, 5, 6]
33/54: excluder.add_raster(fn, codes=codes, buffer=800, crs=3035)
33/55:
band, transform = shape_availability(shape, excluder)

fig, ax = plt.subplots(figsize=(4,8))
shape.plot(ax=ax, color='none')
show(band, transform=transform, cmap='Greens', ax=ax)
33/56: import atlite
33/57: cutout = atlite.Cutout("PORTUGAL-2013-01-era5.nc")
33/58: cutout
33/59: cutout.data
33/60:
ax = shape.to_crs(4326).plot()
cutout.grid.plot(ax=ax, edgecolor='grey', color='None')
33/61: wnd100m = cutout.data.wnd100m.mean(dim='time')
33/62:
ax = plt.axes(projection=ccrs.PlateCarree())

wnd100m.plot(ax=ax, vmin=0, vmax=10)

shape.to_crs(4326).plot(ax=ax, edgecolor='k', color='none')
33/63:
z_r = cutout.data.roughness
wnd100m = cutout.data.wnd100m
33/64: wnd10m = wnd100m * np.log(10 / z_r) / np.log(100 / z_r)
33/65:
ax = plt.axes(projection=ccrs.PlateCarree())

wnd10m.mean(dim='time').plot(ax=ax, vmin=0, vmax=10)

shape.to_crs(4326).plot(ax=ax, edgecolor='k', color='none')
33/66: cutout.data.mean(dim=["x", "y"]).influx_direct.plot()
33/67: A = cutout.availabilitymatrix(shape, excluder)
33/68:
fig, ax = plt.subplots()
A.sel(name='PT').plot(cmap='Greens')
shape.to_crs(4326).plot(ax=ax, edgecolor='k', color='none')
cutout.grid.plot(ax=ax, color='none', edgecolor='grey', ls=':')
33/69:
import xarray as xr

cap_per_sqkm = 2
area = cutout.grid.set_index(['y', 'x']).to_crs(3035).area / 1e6
area = xr.DataArray(area, dims=('spatial'))

capacity_matrix = A.stack(spatial=['y', 'x']) * area * cap_per_sqkm
33/70:
cutout.prepare()
wind = cutout.wind(matrix=capacity_matrix, turbine="Vestas_V90_3MW", index=shape.index)
33/71: wind.plot()
34/1:
fuel_cost = dict(
    coal=8,
    gas=100,
    oil=48,
)
34/2:
efficiency = dict(
    coal=0.33,
    gas=0.58,
    oil=0.35,
)
34/3:
# t/MWh thermal
emissions = dict(
    coal=0.34,
    gas=0.2,
    oil=0.26,
    hydro=0,
)
34/4:
power_plants = {
    "SA": {"coal": 35000, "wind": 3000, "gas": 8000, "oil": 2000},
    "MZ": {"hydro": 1200},
}
34/5:
loads = {
    "SA": 42000,
    "MZ": 650,
}
34/6: import pypsa
34/7: n = pypsa.Network()
34/8:
n.add("Bus", "SA", y=-30.5, x=25, v_nom=400, carrier='AC')
n.add("Bus", "MZ", y=-18.5, x=35.5, v_nom=400, carrier='AC')
34/9: n.buses
34/10: emissions
34/11:
n.madd(
    "Carrier",
    ["coal", "gas", "oil", "hydro", "wind"],
    co2_emissions=emissions,
    nice_name=["Coal", "Gas", "Oil", "Hydro", "Onshore Wind"],
    color=["grey", "indianred", "black", "aquamarine", "dodgerblue"],
)
34/12:
n.add("Generator",
    "MZ hydro",
    bus="MZ",
    carrier='hydro',
    p_nom=1200, # MW
    marginal_cost=0, # default
)
34/13:
for tech, p_nom in power_plants["SA"].items():
    n.add("Generator",
        f"SA {tech}",
        bus="SA",
        carrier=tech,
        efficiency=efficiency.get(tech, 1),
        p_nom=p_nom,
        marginal_cost=fuel_cost.get(tech, 0) / efficiency.get(tech, 1),
    )
34/14: n.generators
34/15:
n.add("Load",
    "SA electricity demand",
    bus="SA",
    p_set=loads["SA"],
    carrier='electricity',
)
34/16:
n.add("Load",
    "MZ electricity demand",
    bus="MZ",
    p_set=loads["MZ"],
    carrier='electricity',
)
34/17: n.loads
34/18:
n.add("Line",
    "SA-MZ",
    bus0="SA",
    bus1="MZ",
    s_nom=500,
    x=1,
)
34/19: n.lines
34/20: n.plot(bus_sizes=1, margin=1);
34/21: n.lopf(solver_name='cbc')
34/22: n.lopf(solver_name='gurobi')
34/23: n.generators_t.p
34/24: n.generators_t.p / n.generators.p_nom
34/25: n.lines_t.p0
34/26: n.lines_t.p1
34/27: n.buses_t.marginal_price
34/28:
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
34/29: n.plot(margin=1, bus_sizes=2)
34/30: n.plot(geomap=False);
34/31:
n.plot(
    margin=1,
    bus_sizes=2,
    bus_colors='orange',
    bus_alpha=0.7,
    color_geomap=True,
    line_colors='orchid',
    line_widths=3,
    title='Test',

);
34/32:
fig = plt.figure(figsize=(5,5))
ax = plt.axes(projection=ccrs.EqualEarth())

n.plot(ax=ax, margin=1, bus_sizes=2);
34/33: s = n.loads.groupby('bus').p_set.sum() / 1e4
34/34: s
34/35: n.plot(margin=1, bus_sizes=s);
34/36: n.generators_t.p.loc["now"]
34/37:
s = n.generators_t.p.loc["now"].groupby(
    [n.generators.bus, n.generators.carrier]
).sum()
34/38: s
34/39: n.plot(margin=1, bus_sizes=s/3000);
34/40: n.lines.loc["SA-MZ", "s_nom"] = 400
34/41: n.lines
34/42: n.lopf()
34/43: n.generators_t.p
34/44:
e = n.generators_t.p / n.generators.efficiency * n.generators.carrier.map(n.carriers.co2_emissions)
e
34/45: e.sum().sum()
34/46:
n.add("GlobalConstraint",
      "emission_limit",
      carrier_attribute="co2_emissions",
      sense="<=",
      constant=e.sum().sum() * 0.9
     )
34/47: n.lopf()
34/48: n.generators_t.p
34/49: n.generators_t.p / n.generators.p_nom
34/50: n.global_constraints.mu
34/51: n.global_constraints.loc["emission_limit", "constant"] = 0.85
34/52: n.lopf()
34/53: n.export_to_csv_folder('tmp')
34/54: n_csv = pypsa.Network('tmp')
34/55: n.export_to_netcdf("tmp.nc");
34/56: n_nc = pypsa.Network("tmp.nc")
34/57: n = pypsa.examples.scigrid_de(from_master=True)
34/58:
n.lines.s_max_pu = 0.7
n.lines.loc[["316", "527", "602"], "s_nom"] = 1715
34/59: n.snapshots
34/60: n.loads_t.p_set.head(3)
34/61: n.loads_t.p_set.sum(axis=1).div(1e3).plot(ylim=[0,60], ylabel='MW')
34/62: n.generators_t.p_max_pu.groupby(n.generators.carrier, axis=1).mean().plot(ylabel='p.u.')
34/63:
n.generators.groupby("carrier").p_nom.sum().div(1e3).plot.barh()
plt.xlabel('GW')
34/64: load = n.loads_t.p_set.sum(axis=0).groupby(n.loads.bus).sum()
34/65:
fig = plt.figure()
ax = plt.axes(projection=ccrs.EqualEarth())

n.plot(
    ax=ax,
    bus_sizes=load / 2e5,
);
34/66: capacities = n.generators.groupby(['bus', 'carrier']).p_nom.sum()
34/67:
import random
carriers = n.generators.carrier.unique()
colors = ["#%06x" % random.randint(0, 0xFFFFFF) for _ in carriers]
n.madd("Carrier", carriers, color=colors)
34/68:
from pypsa.plot import add_legend_patches

fig = plt.figure()
ax = plt.axes(projection=ccrs.EqualEarth())

n.plot(
    ax=ax,
    bus_sizes=capacities / 2e4,
)

add_legend_patches(
    ax,
    colors,
    carriers,
    legend_kw=dict(frameon=False, bbox_to_anchor=(0,1))
)
34/69: n.storage_units.head(3)
34/70: n.lopf(solver_name='cbc')
34/71: n.lopf(solver_name='gurobi')
34/72: line_loading = n.lines_t.p0.iloc[0].abs() / n.lines.s_nom / n.lines.s_max_pu * 100 # %
34/73: norm = plt.Normalize(vmin=0, vmax=100)
34/74:
fig = plt.figure(figsize=(7,7))
ax = plt.axes(projection=ccrs.EqualEarth())

n.plot(
    ax=ax,
    bus_sizes=0,
    line_colors=line_loading,
    line_norm=norm,
    line_cmap='plasma',
    line_widths=n.lines.s_nom / 1000
);

plt.colorbar(
    plt.cm.ScalarMappable(cmap='plasma', norm=norm),
    label='Relative line loading [%]',
    shrink=0.6
)
34/75: p_by_carrier = n.generators_t.p.groupby(n.generators.carrier, axis=1).sum().div(1e3)
34/76:
fig, ax = plt.subplots(figsize=(11, 4))

p_by_carrier.plot(
    kind="area",
    ax=ax,
    linewidth=0,
    cmap='tab20b',
)

ax.legend(
    ncol=5,
    loc="upper left",
    frameon=False
)

ax.set_ylabel("GW")

ax.set_ylim(0,80);
34/77:
fig, ax = plt.subplots()

p_storage = n.storage_units_t.p.sum(axis=1).div(1e3)
state_of_charge = n.storage_units_t.state_of_charge.sum(axis=1).div(1e3)

p_storage.plot(label="Pumped hydro dispatch [GW]", ax=ax)
state_of_charge.plot(label="State of charge [GWh]", ax=ax)

ax.grid()
ax.legend()
ax.set_ylabel("MWh or MW")
34/78:
fig = plt.figure(figsize=(7,7))
ax = plt.axes(projection=ccrs.EqualEarth())

norm = plt.Normalize(vmin=0, vmax=100) # â‚¬/MWh

n.plot(
    ax=ax,
    bus_colors=n.buses_t.marginal_price.mean(),
    bus_cmap='plasma',
    bus_norm=norm,
);

plt.colorbar(
    plt.cm.ScalarMappable(cmap='plasma', norm=norm),
    label='LMP [â‚¬/MWh]',
    shrink=0.6
)
35/1:
import pypsa
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('bmh')
35/2:
year = 2030
url = f"https://raw.githubusercontent.com/PyPSA/technology-data/master/outputs/costs_{year}.csv"
costs = pd.read_csv(url, index_col=[0,1])
35/3:
costs.loc[costs.unit.str.contains("/kW"), "value"] *= 1e3
costs.unit = costs.unit.str.replace("/kW", "/MW")

defaults = {
    "FOM": 0,
    "VOM": 0,
    "efficiency": 1,
    "fuel": 0,
    "investment": 0,
    "lifetime": 25,
    "CO2 intensity": 0,
    "discount rate": 0.07,
}
costs = costs.value.unstack().fillna(defaults)

costs.at["OCGT", "fuel"] = costs.at["gas", "fuel"]
costs.at["CCGT", "fuel"] = costs.at["gas", "fuel"]
costs.at["OCGT", "CO2 intensity"] = costs.at["gas", "CO2 intensity"]
costs.at["CCGT", "CO2 intensity"] = costs.at["gas", "CO2 intensity"]
35/4:
def annuity(r, n):
    return r / (1.0 - 1.0 / (1.0 + r) ** n)
35/5: annuity(0.07, 20)
35/6: costs["marginal_cost"] = costs["VOM"] + costs["fuel"] / costs["efficiency"]
35/7: costs
35/8: costs["marginal_cost"] = costs["VOM"] + costs["fuel"] / costs["efficiency"]
35/9: annuity = costs.apply(lambda x: annuity(x["discount rate"], x["lifetime"]), axis=1)
35/10: costs["capital_cost"] = (annuity + costs["FOM"] / 100) * costs["investment"]
35/11: costs
35/12: costs.head(20)
35/13: costs.head(40)
35/14: costs.head(100)
35/15: costs.head(50)
35/16: costs.tail(50)
35/17:
url = "https://raw.githubusercontent.com/fneum/data-science-for-esm/main/data-science-for-esm/time-series-lecture-2.csv"
ts = pd.read_csv(url, index_col=0, parse_dates=True)
35/18: ts.head(3)
35/19: ts.load *= 1e3
35/20:
resolution = 4
ts = ts.resample(f"{resolution}H").first()
35/21: n = pypsa.Network()
35/22: n.add("Bus", "electricity")
35/23: n.set_snapshots(ts.index)
35/24: n.snapshots
35/25: n.snapshot_weightings.head(3)
35/26: n.snapshot_weightings.loc[:,:] = resolution
35/27: n.snapshot_weightings.head(3)
35/28:
carriers = ["onwind", "offwind", "solar", "OCGT", "hydrogen storage underground", "battery storage"]

n.madd(
    "Carrier",
    carriers,
    color=["dodgerblue", "aquamarine", "gold", "indianred", "magenta", "yellowgreen"],
    co2_emissions=[costs.at[c, "CO2 intensity"] for c in carriers]
)
35/29:
n.add(
    "Load",
    "demand",
    bus="electricity",
    p_set=ts.load,
)
35/30: n.loads_t.p_set.plot(figsize=(6,2), ylabel="MW")
35/31:
n.add(
    "Generator",
    "OCGT",
    bus='electricity',
    carrier="OCGT",
    capital_cost=costs.at["OCGT", "capital_cost"],
    marginal_cost=costs.at["OCGT", "marginal_cost"],
    efficiency=costs.at["OCGT", "efficiency"],
    p_nom_extendable=True,
)
35/32:
for tech in ["onwind", "offwind", "solar"]:
    n.add(
        "Generator",
        tech,
        bus='electricity',
        carrier=tech,
        p_max_pu=ts[tech],
        capital_cost=costs.at[tech, "capital_cost"],
        marginal_cost=costs.at[tech, "marginal_cost"],
        efficiency=costs.at[tech, "efficiency"],
        p_nom_extendable=True,
    )
35/33: n.generators_t.p_max_pu.loc["2015-03"].plot(figsize=(6,2), ylabel="CF")
35/34: n.lopf(solver_name='gurobi')
35/35: n.objective / 1e9
35/36: n.generators.p_nom_opt.div(1e3) # GW
35/37: n.snapshot_weightings.generators @ n.generators_t.p.div(1e6) # TWh
35/38: opex = n.snapshot_weightings.generators @ (n.generators_t.p * n.generators.marginal_cost).div(1e6) # Mâ‚¬/a
35/39: capex = (n.generators.p_nom_opt * n.generators.capital_cost).div(1e6) # Mâ‚¬/a
35/40: capex + opex
35/41: (n.statistics.capex() + n.statistics.opex(aggregate_time='sum')).div(1e6)
35/42: emissions = n.generators_t.p / n.generators.efficiency * n.generators.carrier.map(n.carriers.co2_emissions) # t/h
35/43: n.snapshot_weightings.generators @ emissions.sum(axis=1).div(1e6) # Mt
35/44:
def plot_dispatch(n, time="2015-07"):

    p_by_carrier = n.generators_t.p.groupby(n.generators.carrier, axis=1).sum().div(1e3)

    if not n.storage_units.empty:
        sto = n.storage_units_t.p.groupby(n.storage_units.carrier, axis=1).sum().div(1e3)
        p_by_carrier = pd.concat([p_by_carrier, sto], axis=1)

    fig, ax = plt.subplots(figsize=(6, 3))

    color = p_by_carrier.columns.map(n.carriers.color)

    p_by_carrier.where(p_by_carrier>0).loc[time].plot.area(
        ax=ax,
        linewidth=0,
        color=color,
    )

    charge = p_by_carrier.where(p_by_carrier<0).dropna(how='all', axis=1).loc[time]

    if not charge.empty:

        charge.plot.area(
            ax=ax,
            linewidth=0,
            color=charge.columns.map(n.carriers.color),
        )

    n.loads_t.p_set.sum(axis=1).loc[time].div(1e3).plot(ax=ax, c='k')

    plt.legend(loc=(1.05,0))
    ax.set_ylabel("GW")
    ax.set_ylim(-200, 200)
35/45: plot_dispatch(n)
35/46:
n.add(
    "StorageUnit",
    "battery storage",
    bus='electricity',
    carrier="battery storage",
    max_hours=6,
    capital_cost=costs.at["battery inverter", "capital_cost"] + 6 * costs.at["battery storage", "capital_cost"],
    efficiency_store=costs.at["battery inverter", "efficiency"],
    efficiency_dispatch=costs.at["battery inverter", "efficiency"],
    p_nom_extendable=True,
    cyclic_state_of_charge=True,
)
35/47:
capital_costs = (
    costs.at["electrolysis", "capital_cost"] +
    costs.at["fuel cell", "capital_cost"] +
    168 * costs.at["hydrogen storage underground", "capital_cost"]
)

n.add(
    "StorageUnit",
    "hydrogen storage underground",
    bus="electricity",
    carrier="hydrogen storage underground",
    max_hours=168,
    capital_cost=capital_costs,
    efficiency_store=costs.at["electrolysis", "efficiency"],
    efficiency_dispatch=costs.at["fuel cell", "efficiency"],
    p_nom_extendable=True,
    cyclic_state_of_charge=True,
)
35/48: n.lopf(solver_name='cbc')
35/49: n.storage_units.p_nom_opt # MW
35/50: n.lopf(solver_name='gurobi')
35/51: n.generators.p_nom_opt # MW
35/52: n.storage_units.p_nom_opt # MW
35/53:
n.add(
    "GlobalConstraint",
    "CO2Limit",
    carrier_attribute="co2_emissions",
    sense="<=",
    constant=0,
)
35/54: n.lopf(solver_name='cbc')
35/55: n.lopf(solver_name='gurobi')
35/56: n.generators.p_nom_opt # MW
35/57: n.storage_units.p_nom_opt # MW
35/58: n.storage_units.p_nom_opt.div(1e3) * n.storage_units.max_hours # GWh
35/59: plot_dispatch(n)
35/60:
def system_cost(n):
    tsc = n.statistics.capex() + n.statistics.opex(aggregate_time='sum')
    return tsc.droplevel(0).div(1e6) # million â‚¬/a
35/61: system_cost(n)
35/62: system_cost(n).plot.pie(figsize=(2,2))
35/63: demand = n.snapshot_weightings.generators @ n.loads_t.p_set.sum(axis=1)
35/64: system_cost(n).sum()*1e6 / demand.sum()
35/65:
sensitivity = {}
for co2 in [150,100,50,25,0]:
    n.global_constraints.loc["CO2Limit", "constant"] = co2 * 1e6
    n.lopf(solver_name='cbc')
    sensitivity[co2] = system_cost(n)
35/66:
sensitivity = {}
for co2 in [150,100,50,25,0]:
    n.global_constraints.loc["CO2Limit", "constant"] = co2 * 1e6
    n.lopf(solver_name='gurobi')
    sensitivity[co2] = system_cost(n)
35/67:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel=r"CO$_2$ emissions [Mt/a]",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
plt.legend(frameon=False, loc=(1.05, 0))
35/68:
sensitivity = {}
for solar_cost in [0, 20, 40, 60, 80, 100, 150]:
    n.generators.loc["solar", "capital_cost"] = solar_cost * 1e3
    n.lopf(solver_name='cbc')
    sensitivity[solar_cost] = system_cost(n)
n.generators.loc["solar", "capital_cost"] = 40 * 1e3
35/69:
sensitivity = {}
for solar_cost in [0, 20, 40, 60, 80, 100, 150]:
    n.generators.loc["solar", "capital_cost"] = solar_cost * 1e3
    n.lopf(solver_name='gurobi')
    sensitivity[solar_cost] = system_cost(n)
n.generators.loc["solar", "capital_cost"] = 40 * 1e3
35/70:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="Solar Capital Cost [â‚¬/MW/a]",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/71:
sensitivity = {}
for offwind_potential in [0, 50, 100, 150]:
    n.generators.loc["offwind", "p_nom_max"] = offwind_potential * 1e3
    n.lopf(solver_name='cbc')
    sensitivity[offwind_potential] = system_cost(n)
35/72:
sensitivity = {}
for offwind_potential in [0, 50, 100, 150]:
    n.generators.loc["offwind", "p_nom_max"] = offwind_potential * 1e3
    n.lopf(solver_name='gurobi')
    sensitivity[offwind_potential] = system_cost(n)
35/73:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="Offshore Wind Potential [GW]",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/74: n.export_to_netcdf("network-cem.nc");
35/75: n.export_to_netcdf("network-assignment-5.nc");
35/76: n.generators
35/77:
for tech in ["onwind", "offwind", "solar", "nuclear"]:
    n.add(
        "Generator",
        tech,
        bus='electricity',
        carrier=tech,
        p_max_pu=ts[tech],
        capital_cost=costs.at[tech, "capital_cost"],
        marginal_cost=costs.at[tech, "marginal_cost"],
        efficiency=costs.at[tech, "efficiency"],
        p_nom_extendable=True,
    )
35/78:
import pypsa
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('bmh')
35/79:
year = 2030
url = f"https://raw.githubusercontent.com/PyPSA/technology-data/master/outputs/costs_{year}.csv"
costs = pd.read_csv(url, index_col=[0,1])
35/80:
costs.loc[costs.unit.str.contains("/kW"), "value"] *= 1e3
costs.unit = costs.unit.str.replace("/kW", "/MW")

defaults = {
    "FOM": 0,
    "VOM": 0,
    "efficiency": 1,
    "fuel": 0,
    "investment": 0,
    "lifetime": 25,
    "CO2 intensity": 0,
    "discount rate": 0.07,
}
costs = costs.value.unstack().fillna(defaults)

costs.at["OCGT", "fuel"] = costs.at["gas", "fuel"]
costs.at["CCGT", "fuel"] = costs.at["gas", "fuel"]
costs.at["OCGT", "CO2 intensity"] = costs.at["gas", "CO2 intensity"]
costs.at["CCGT", "CO2 intensity"] = costs.at["gas", "CO2 intensity"]
35/81: costs.tail(50)
35/82:
def annuity(r, n):
    return r / (1.0 - 1.0 / (1.0 + r) ** n)
35/83: annuity(0.07, 20)
35/84: costs["marginal_cost"] = costs["VOM"] + costs["fuel"] / costs["efficiency"]
35/85: annuity = costs.apply(lambda x: annuity(x["discount rate"], x["lifetime"]), axis=1)
35/86: costs["capital_cost"] = (annuity + costs["FOM"] / 100) * costs["investment"]
35/87:
url = "https://raw.githubusercontent.com/fneum/data-science-for-esm/main/data-science-for-esm/time-series-lecture-2.csv"
ts = pd.read_csv(url, index_col=0, parse_dates=True)
35/88: ts.head(3)
35/89: ts.load *= 1e3
35/90:
resolution = 4
ts = ts.resample(f"{resolution}H").first()
35/91: n = pypsa.Network()
35/92: n.add("Bus", "electricity")
35/93: n.set_snapshots(ts.index)
35/94: n.snapshots
35/95: n.snapshot_weightings.head(3)
35/96: n.snapshot_weightings.loc[:,:] = resolution
35/97: n.snapshot_weightings.head(3)
35/98:
carriers = ["onwind", "offwind", "solar", "OCGT", "hydrogen storage underground", "battery storage"]

n.madd(
    "Carrier",
    carriers,
    color=["dodgerblue", "aquamarine", "gold", "indianred", "magenta", "yellowgreen"],
    co2_emissions=[costs.at[c, "CO2 intensity"] for c in carriers]
)
35/99:
n.add(
    "Load",
    "demand",
    bus="electricity",
    p_set=ts.load,
)
35/100: n.loads_t.p_set.plot(figsize=(6,2), ylabel="MW")
35/101:
n.add(
    "Generator",
    "OCGT",
    bus='electricity',
    carrier="OCGT",
    capital_cost=costs.at["OCGT", "capital_cost"],
    marginal_cost=costs.at["OCGT", "marginal_cost"],
    efficiency=costs.at["OCGT", "efficiency"],
    p_nom_extendable=True,
)
35/102:
n.add(
    "Generator",
    "OCGT",
    bus='electricity',
    carrier="OCGT",
    capital_cost=costs.at["OCGT", "capital_cost"],
    marginal_cost=costs.at["OCGT", "marginal_cost"],
    efficiency=costs.at["OCGT", "efficiency"],
    p_nom_extendable=True,
)
35/103: n = pypsa.Network()
35/104: n.add("Bus", "electricity")
35/105: n.set_snapshots(ts.index)
35/106: n.snapshots
35/107: n.snapshot_weightings.head(3)
35/108: n.snapshot_weightings.loc[:,:] = resolution
35/109: n.snapshot_weightings.head(3)
35/110:
carriers = ["onwind", "offwind", "solar", "OCGT", "hydrogen storage underground", "battery storage"]

n.madd(
    "Carrier",
    carriers,
    color=["dodgerblue", "aquamarine", "gold", "indianred", "magenta", "yellowgreen"],
    co2_emissions=[costs.at[c, "CO2 intensity"] for c in carriers]
)
35/111:
carriers = ["onwind", "offwind", "solar", "OCGT", "hydrogen storage underground", "battery storage", "nuclear"]

n.madd(
    "Carrier",
    carriers,
    color=["dodgerblue", "aquamarine", "gold", "indianred", "magenta", "yellowgreen"],
    co2_emissions=[costs.at[c, "CO2 intensity"] for c in carriers]
)
35/112:
carriers = ["onwind", "offwind", "solar", "OCGT", "hydrogen storage underground", "battery storage", "nuclear"]

n.madd(
    "Carrier",
    carriers,
    color=["dodgerblue", "aquamarine", "gold", "indianred", "magenta", "yellowgreen", "red"],
    co2_emissions=[costs.at[c, "CO2 intensity"] for c in carriers]
)
35/113:
n.add(
    "Load",
    "demand",
    bus="electricity",
    p_set=ts.load,
)
35/114: n.loads_t.p_set.plot(figsize=(6,2), ylabel="MW")
35/115:
n.add(
    "Generator",
    "OCGT",
    bus='electricity',
    carrier="OCGT",
    capital_cost=costs.at["OCGT", "capital_cost"],
    marginal_cost=costs.at["OCGT", "marginal_cost"],
    efficiency=costs.at["OCGT", "efficiency"],
    p_nom_extendable=True,
)
35/116:
for tech in ["onwind", "offwind", "solar", "nuclear"]:
    n.add(
        "Generator",
        tech,
        bus='electricity',
        carrier=tech,
        p_max_pu=ts[tech],
        capital_cost=costs.at[tech, "capital_cost"],
        marginal_cost=costs.at[tech, "marginal_cost"],
        efficiency=costs.at[tech, "efficiency"],
        p_nom_extendable=True,
    )
35/117: ts
35/118: ts.load.max()
35/119: ts["nuclear"] = ts.load.max()
35/120:
for tech in ["onwind", "offwind", "solar", "nuclear"]:
    n.add(
        "Generator",
        tech,
        bus='electricity',
        carrier=tech,
        p_max_pu=ts[tech],
        capital_cost=costs.at[tech, "capital_cost"],
        marginal_cost=costs.at[tech, "marginal_cost"],
        efficiency=costs.at[tech, "efficiency"],
        p_nom_extendable=True,
    )
35/121: n.add("Bus", "electricity")
35/122: n = pypsa.Network()
35/123: n.add("Bus", "electricity")
35/124: n.set_snapshots(ts.index)
35/125: n.snapshots
35/126: n.snapshot_weightings.head(3)
35/127: n.snapshot_weightings.loc[:,:] = resolution
35/128: n.snapshot_weightings.head(3)
35/129:
carriers = ["onwind", "offwind", "solar", "OCGT", "hydrogen storage underground", "battery storage", "nuclear"]

n.madd(
    "Carrier",
    carriers,
    color=["dodgerblue", "aquamarine", "gold", "indianred", "magenta", "yellowgreen", "red"],
    co2_emissions=[costs.at[c, "CO2 intensity"] for c in carriers]
)
35/130:
n.add(
    "Load",
    "demand",
    bus="electricity",
    p_set=ts.load,
)
35/131: n.loads_t.p_set.plot(figsize=(6,2), ylabel="MW")
35/132:
n.add(
    "Generator",
    "OCGT",
    bus='electricity',
    carrier="OCGT",
    capital_cost=costs.at["OCGT", "capital_cost"],
    marginal_cost=costs.at["OCGT", "marginal_cost"],
    efficiency=costs.at["OCGT", "efficiency"],
    p_nom_extendable=True,
)
35/133: ts.load.max()
35/134: ts["nuclear"] = ts.load.max()
35/135:
for tech in ["onwind", "offwind", "solar", "nuclear"]:
    n.add(
        "Generator",
        tech,
        bus='electricity',
        carrier=tech,
        p_max_pu=ts[tech],
        capital_cost=costs.at[tech, "capital_cost"],
        marginal_cost=costs.at[tech, "marginal_cost"],
        efficiency=costs.at[tech, "efficiency"],
        p_nom_extendable=True,
    )
35/136: n.generators_t.p_max_pu.loc["2015-03"].plot(figsize=(6,2), ylabel="CF")
35/137: n.lopf(solver_name='gurobi')
35/138: n.objective / 1e9
35/139: n.generators.p_nom_opt.div(1e3) # GW
35/140: n.snapshot_weightings.generators @ n.generators_t.p.div(1e6) # TWh
35/141: opex = n.snapshot_weightings.generators @ (n.generators_t.p * n.generators.marginal_cost).div(1e6) # Mâ‚¬/a
35/142: capex = (n.generators.p_nom_opt * n.generators.capital_cost).div(1e6) # Mâ‚¬/a
35/143: capex + opex
35/144: costs["nuclear"]
35/145: costs["nuclear",:]
35/146: costs
35/147: costs.index
35/148: costs.loc["nuclear", :]
35/149: ts["nuclear"] = 1
35/150: n.generators
35/151: n.generators["p_max_pu"]
35/152: ts
35/153: n.generators_t.p_max_pu.loc["2015-03"].plot(figsize=(6,2), ylabel="CF")
35/154: n = pypsa.Network()
35/155: n.add("Bus", "electricity")
35/156: n.set_snapshots(ts.index)
35/157: n.snapshots
35/158: n.snapshot_weightings.head(3)
35/159: n.snapshot_weightings.loc[:,:] = resolution
35/160: n.snapshot_weightings.head(3)
35/161:
carriers = ["onwind", "offwind", "solar", "OCGT", "hydrogen storage underground", "battery storage", "nuclear"]

n.madd(
    "Carrier",
    carriers,
    color=["dodgerblue", "aquamarine", "gold", "indianred", "magenta", "yellowgreen", "red"],
    co2_emissions=[costs.at[c, "CO2 intensity"] for c in carriers]
)
35/162:
n.add(
    "Load",
    "demand",
    bus="electricity",
    p_set=ts.load,
)
35/163: n.loads_t.p_set.plot(figsize=(6,2), ylabel="MW")
35/164:
n.add(
    "Generator",
    "OCGT",
    bus='electricity',
    carrier="OCGT",
    capital_cost=costs.at["OCGT", "capital_cost"],
    marginal_cost=costs.at["OCGT", "marginal_cost"],
    efficiency=costs.at["OCGT", "efficiency"],
    p_nom_extendable=True,
)
35/165: ts.load.max()
35/166: ts["nuclear"] = 1
35/167:
for tech in ["onwind", "offwind", "solar", "nuclear"]:
    n.add(
        "Generator",
        tech,
        bus='electricity',
        carrier=tech,
        p_max_pu=ts[tech],
        capital_cost=costs.at[tech, "capital_cost"],
        marginal_cost=costs.at[tech, "marginal_cost"],
        efficiency=costs.at[tech, "efficiency"],
        p_nom_extendable=True,
    )
35/168: ts
35/169: n.generators_t.p_max_pu.loc["2015-03"].plot(figsize=(6,2), ylabel="CF")
35/170: n.lopf(solver_name='gurobi')
35/171: n.objective / 1e9
35/172: n.generators.p_nom_opt.div(1e3) # GW
35/173: n.snapshot_weightings.generators @ n.generators_t.p.div(1e6) # TWh
35/174: opex = n.snapshot_weightings.generators @ (n.generators_t.p * n.generators.marginal_cost).div(1e6) # Mâ‚¬/a
35/175: capex = (n.generators.p_nom_opt * n.generators.capital_cost).div(1e6) # Mâ‚¬/a
35/176: capex + opex
35/177: (n.statistics.capex() + n.statistics.opex(aggregate_time='sum')).div(1e6)
35/178: emissions = n.generators_t.p / n.generators.efficiency * n.generators.carrier.map(n.carriers.co2_emissions) # t/h
35/179: n.snapshot_weightings.generators @ emissions.sum(axis=1).div(1e6) # Mt
35/180:
def plot_dispatch(n, time="2015-07"):

    p_by_carrier = n.generators_t.p.groupby(n.generators.carrier, axis=1).sum().div(1e3)

    if not n.storage_units.empty:
        sto = n.storage_units_t.p.groupby(n.storage_units.carrier, axis=1).sum().div(1e3)
        p_by_carrier = pd.concat([p_by_carrier, sto], axis=1)

    fig, ax = plt.subplots(figsize=(6, 3))

    color = p_by_carrier.columns.map(n.carriers.color)

    p_by_carrier.where(p_by_carrier>0).loc[time].plot.area(
        ax=ax,
        linewidth=0,
        color=color,
    )

    charge = p_by_carrier.where(p_by_carrier<0).dropna(how='all', axis=1).loc[time]

    if not charge.empty:

        charge.plot.area(
            ax=ax,
            linewidth=0,
            color=charge.columns.map(n.carriers.color),
        )

    n.loads_t.p_set.sum(axis=1).loc[time].div(1e3).plot(ax=ax, c='k')

    plt.legend(loc=(1.05,0))
    ax.set_ylabel("GW")
    ax.set_ylim(-200, 200)
35/181: plot_dispatch(n)
35/182:
n.add(
    "StorageUnit",
    "battery storage",
    bus='electricity',
    carrier="battery storage",
    max_hours=6,
    capital_cost=costs.at["battery inverter", "capital_cost"] + 6 * costs.at["battery storage", "capital_cost"],
    efficiency_store=costs.at["battery inverter", "efficiency"],
    efficiency_dispatch=costs.at["battery inverter", "efficiency"],
    p_nom_extendable=True,
    cyclic_state_of_charge=True,
)
35/183:
capital_costs = (
    costs.at["electrolysis", "capital_cost"] +
    costs.at["fuel cell", "capital_cost"] +
    168 * costs.at["hydrogen storage underground", "capital_cost"]
)

n.add(
    "StorageUnit",
    "hydrogen storage underground",
    bus="electricity",
    carrier="hydrogen storage underground",
    max_hours=168,
    capital_cost=capital_costs,
    efficiency_store=costs.at["electrolysis", "efficiency"],
    efficiency_dispatch=costs.at["fuel cell", "efficiency"],
    p_nom_extendable=True,
    cyclic_state_of_charge=True,
)
35/184: n.lopf(solver_name='gurobi')
35/185: n.generators.p_nom_opt # MW
35/186: n.storage_units.p_nom_opt # MW
35/187:
n.add(
    "GlobalConstraint",
    "CO2Limit",
    carrier_attribute="co2_emissions",
    sense="<=",
    constant=0,
)
35/188: n.lopf(solver_name='gurobi')
35/189: n.generators.p_nom_opt # MW
35/190: n.storage_units.p_nom_opt # MW
35/191: n.storage_units.p_nom_opt.div(1e3) * n.storage_units.max_hours # GWh
35/192: plot_dispatch(n)
35/193:
def system_cost(n):
    tsc = n.statistics.capex() + n.statistics.opex(aggregate_time='sum')
    return tsc.droplevel(0).div(1e6) # million â‚¬/a
35/194: system_cost(n)
35/195: system_cost(n).plot.pie(figsize=(2,2))
35/196: demand = n.snapshot_weightings.generators @ n.loads_t.p_set.sum(axis=1)
35/197: system_cost(n).sum()*1e6 / demand.sum()
35/198:
sensitivity = {}
for co2 in [150,100,50,25,0]:
    n.global_constraints.loc["CO2Limit", "constant"] = co2 * 1e6
    n.lopf(solver_name='gurobi')
    sensitivity[co2] = system_cost(n)
35/199:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel=r"CO$_2$ emissions [Mt/a]",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
plt.legend(frameon=False, loc=(1.05, 0))
35/200:
sensitivity = {}
for solar_cost in [0, 20, 40, 60, 80, 100, 150]:
    n.generators.loc["solar", "capital_cost"] = solar_cost * 1e3
    n.lopf(solver_name='gurobi')
    sensitivity[solar_cost] = system_cost(n)
n.generators.loc["solar", "capital_cost"] = 40 * 1e3
35/201:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="Solar Capital Cost [â‚¬/MW/a]",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/202:
sensitivity = {}
for offwind_potential in [0, 50, 100, 150]:
    n.generators.loc["offwind", "p_nom_max"] = offwind_potential * 1e3
    n.lopf(solver_name='gurobi')
    sensitivity[offwind_potential] = system_cost(n)
35/203:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="Offshore Wind Potential [GW]",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/204: n.generators
35/205:
sensitivity = {}
for offwind_potential in [1000*i for i in range(1,8)]:
    n.generators.loc["offwind", "p_nom_max"] = offwind_potential * 1e3
    n.lopf(solver_name='gurobi')
    sensitivity[offwind_potential] = system_cost(n)
35/206:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="Offshore Wind Potential [GW]",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/207: n.generators
35/208: n.generators.columns
35/209:
sensitivity = {}
for nuclear_cap_cost in [1000*i for i in range(1,8)]:
    n.generators.loc["nuclear", "capital_cost"] = nuclear_cap_cost
    n.lopf(solver_name='gurobi')
    sensitivity[nuclear_cap_cost] = system_cost(n)
35/210:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="nuclear_cap_cost",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/211: df
35/212: sensitivity
35/213: plot_dispatch(n)
35/214: n.generators_t.p_max_pu.loc["2015-03"].plot(figsize=(6,2), ylabel="CF")
35/215: n = pypsa.Network()
35/216: n.add("Bus", "electricity")
35/217: n.set_snapshots(ts.index)
35/218: n.snapshots
35/219: n.snapshot_weightings.head(3)
35/220: n.snapshot_weightings.loc[:,:] = resolution
35/221: n.snapshot_weightings.head(3)
35/222:
carriers = ["onwind", "offwind", "solar", "OCGT", "hydrogen storage underground", "battery storage", "nuclear"]

n.madd(
    "Carrier",
    carriers,
    color=["dodgerblue", "aquamarine", "gold", "indianred", "magenta", "yellowgreen", "red"],
    co2_emissions=[costs.at[c, "CO2 intensity"] for c in carriers]
)
35/223:
n.add(
    "Load",
    "demand",
    bus="electricity",
    p_set=ts.load,
)
35/224: n.loads_t.p_set.plot(figsize=(6,2), ylabel="MW")
35/225:
n.add(
    "Generator",
    "OCGT",
    bus='electricity',
    carrier="OCGT",
    capital_cost=costs.at["OCGT", "capital_cost"],
    marginal_cost=costs.at["OCGT", "marginal_cost"],
    efficiency=costs.at["OCGT", "efficiency"],
    p_nom_extendable=True,
)
35/226: ts.load.max()
35/227: ts["nuclear"] = 1
35/228:
for tech in ["onwind", "offwind", "solar", "nuclear"]:
    n.add(
        "Generator",
        tech,
        bus='electricity',
        carrier=tech,
        p_max_pu=ts[tech],
        capital_cost=costs.at[tech, "capital_cost"],
        marginal_cost=costs.at[tech, "marginal_cost"],
        efficiency=costs.at[tech, "efficiency"],
        p_nom_extendable=True,
    )
35/229: ts
35/230: n.generators_t.p_max_pu.loc["2015-03"].plot(figsize=(6,2), ylabel="CF")
35/231: n.lopf(solver_name='gurobi')
35/232: n.objective / 1e9
35/233: n.generators.p_nom_opt.div(1e3) # GW
35/234: n.snapshot_weightings.generators @ n.generators_t.p.div(1e6) # TWh
35/235: opex = n.snapshot_weightings.generators @ (n.generators_t.p * n.generators.marginal_cost).div(1e6) # Mâ‚¬/a
35/236: capex = (n.generators.p_nom_opt * n.generators.capital_cost).div(1e6) # Mâ‚¬/a
35/237: capex + opex
35/238: (n.statistics.capex() + n.statistics.opex(aggregate_time='sum')).div(1e6)
35/239: emissions = n.generators_t.p / n.generators.efficiency * n.generators.carrier.map(n.carriers.co2_emissions) # t/h
35/240: n.snapshot_weightings.generators @ emissions.sum(axis=1).div(1e6) # Mt
35/241:
def plot_dispatch(n, time="2015-07"):

    p_by_carrier = n.generators_t.p.groupby(n.generators.carrier, axis=1).sum().div(1e3)

    if not n.storage_units.empty:
        sto = n.storage_units_t.p.groupby(n.storage_units.carrier, axis=1).sum().div(1e3)
        p_by_carrier = pd.concat([p_by_carrier, sto], axis=1)

    fig, ax = plt.subplots(figsize=(6, 3))

    color = p_by_carrier.columns.map(n.carriers.color)

    p_by_carrier.where(p_by_carrier>0).loc[time].plot.area(
        ax=ax,
        linewidth=0,
        color=color,
    )

    charge = p_by_carrier.where(p_by_carrier<0).dropna(how='all', axis=1).loc[time]

    if not charge.empty:

        charge.plot.area(
            ax=ax,
            linewidth=0,
            color=charge.columns.map(n.carriers.color),
        )

    n.loads_t.p_set.sum(axis=1).loc[time].div(1e3).plot(ax=ax, c='k')

    plt.legend(loc=(1.05,0))
    ax.set_ylabel("GW")
    ax.set_ylim(-200, 200)
35/242: plot_dispatch(n)
35/243:
sensitivity = {}
for nuclear_cap_cost in [1000*i for i in range(1,8)]:
    n.generators.loc["nuclear", "capital_cost"] = nuclear_cap_cost
    n.lopf(solver_name='gurobi')
    sensitivity[nuclear_cap_cost] = system_cost(n)
35/244:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="nuclear_cap_cost",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/245: df
35/246: n.generators.loc[:, "capital_cost"]
35/247:
sensitivity = {}
for nuclear_cap_cost in [1000*i for i in range(1,8)]:
    n.generators.loc["nuclear", "capital_cost"] = nuclear_cap_cost*1e3
    n.lopf(solver_name='gurobi')
    sensitivity[nuclear_cap_cost] = system_cost(n)
35/248: n.generators.loc[:, "capital_cost"]
35/249:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="nuclear_cap_cost",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/250: df
35/251: costs.loc["nuclear"]
35/252: costs.loc["ofwind"]
35/253: costs.loc["offwind"]
35/254:
sensitivity = {}
for nuclear_cap_cost in [1000*i for i in range(1,8)]:
    n.generators.loc["nuclear", "capital_cost"] = nuclear_cap_cost*1e2
    n.lopf(solver_name='gurobi')
    sensitivity[nuclear_cap_cost] = system_cost(n)
35/255: n.generators.loc[:, "capital_cost"]
35/256:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="nuclear_cap_cost",
    ylabel="System cost [bnâ‚¬/a]",
    ylim=(0,100),
)
35/257:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
    xlim=(0,150),
    xlabel="nuclear_cap_cost",
    ylabel="System cost [bnâ‚¬/a]",
 #   ylim=(0,100),
)
35/258:
df = pd.DataFrame(sensitivity).T.div(1e3) # billion Euro/a
df.plot.area(
    stacked=True,
    linewidth=0,
    color=df.columns.map(n.carriers.color),
    figsize=(4,4),
  #  xlim=(0,150),
    xlabel="nuclear_cap_cost",
    ylabel="System cost [bnâ‚¬/a]",
 #   ylim=(0,100),
)
35/259: df
37/1:
import powerplantmatching as pm
pm.powerplants(from_url=True)
37/2: df = _
37/3: df.head()
37/4: df.columns()
37/5: df.columns
37/6: # Bigger dataset with the original data entries
37/7: df_more = pm.powerplants(reduced=False)
37/8: df_more = pm.powerplants(reduced=False)
37/9: df_more = pm.powerplants(reduced=False)
40/1: import powerplantmatching as pm
40/2: !pwd
40/3: !cwd
41/1:
import pathlib
import powerplantmatching as pm
41/2:
root = pathlib.Path(__file__).parent.parent.absolute()
df = pm.powerplants(update=True)
41/3: df.head()
41/4: df = pm.powerplants(reduced=False)
43/1:
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
44/1:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
44/2:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
44/3:
# start coding here
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
44/4: snakemake.input
44/5:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
44/6: # Snakemake inputs
44/7:

df_wind_locations = pd.read_csv(wind_locations_file)
df_nve_wind_locations = pd.read_csv(wind_nve_locations_file, index_col=0)
df_nve_wind_locations = df_nve_wind_locations.sort_values(by="lat")  # Sort by south to north

df_locations = pd.concat([df_wind_locations, df_nve_wind_locations], axis=0)
df_locations = df_locations.reset_index(drop=True)
df_locations = df_locations.sort_values(by="lat")  # Sort by south to north

df = pd.read_csv(wind_data_file, index_col=0)
df.index = pd.to_datetime(df.index)
44/8:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
44/9:

df_mean_wind_nve = df[df_nve_wind_locations["location"]].mean()
# df_mean_wind_nve.to_csv("data/mean_wind.csv")
fig = px.bar(df_mean_wind_nve, text_auto=".2", title="")
fig.update_traces(textfont_size=12, textangle=0, textposition="inside", cliponaxis=False)
fig.update_layout(
    xaxis_title="Wind farm", yaxis_title="Mean wind power output", showlegend=False, template="plotly_white", width=1000
)
fig.show()
44/10:

# Plot correlation
fig = get_corr_figure(df)
fig.show()

fig = get_corr_distance_figure(df, df_locations)
fig.show()
44/11:

# Plot short-term variation
n_shifts = 25
quantile = 0.8

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile)
fig.update_layout(width=900)
fig.show()
44/12:

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile=0.9999)
fig.update_layout(width=900)
fig.show()
44/13:

fig = px.area(df.resample("1D").mean())
fig.show()
44/14:
resample_period = "7D"
fig = get_mean_std_wind_figure(df, resample_period)
fig.show()
44/15:
df["BE"].sort_values().plot()
vals = df["BE"].values
x = np.linspace(0, 1, len(vals))
px.line(x=x, y=np.sort(vals))
px.line(df["BE"].sample(10000).sort_values().values)
44/16:
df["BE"].sort_values().plot()
vals = df["BE"].values
x = np.linspace(0, 1, len(vals))
px.line(x=x, y=np.sort(vals))
px.line(df["BE"].sample(10000).sort_values().values)
44/17:
df["BE"].sort_values().plot()
vals = df["BE"].values
x = np.linspace(0, 1, len(vals))
px.line(x=x, y=np.sort(vals))
44/18:

px.line(df["BE"].sample(10000).sort_values().values)
44/19:

### Line plots
area = "Utsira nord"
resample_period = "7D"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
44/20:
resample_period = "1H"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
44/21: df.mean(axis=1)
44/22: df.mean(axis=1).values
44/23:
ind = df.index.map(lambda x: x - pd.Timestamp(year=x.year, month=1, day=1, hour=0, minute=0))

dff = pd.DataFrame(df[area].values, index=ind, columns=[area])
dff["Year"] = df.index.year
dff = dff.reset_index().pivot(index="index", columns="Year").dropna()
dff.columns = dff.columns.droplevel()
dff.index = dff.index + pd.Timestamp("1970-1-1")  # Hack to show dates in figure
years = dff.columns

dff = dff.resample(resample_period).mean()

dff = dff.reset_index(names="date")

fig = px.line(
    dff,
    x="date",
    y=years,
    hover_data={"date": "|%d. %B, %H:%M"},
    #     color_discrete_sequence=px.colors.sequential.Blues,
    color_discrete_sequence=sns.color_palette("mako", len(dff.columns)).as_hex(),
)
fig.update_traces(opacity=0.5)
44/24:
import seaborn as sns
ind = df.index.map(lambda x: x - pd.Timestamp(year=x.year, month=1, day=1, hour=0, minute=0))

dff = pd.DataFrame(df[area].values, index=ind, columns=[area])
dff["Year"] = df.index.year
dff = dff.reset_index().pivot(index="index", columns="Year").dropna()
dff.columns = dff.columns.droplevel()
dff.index = dff.index + pd.Timestamp("1970-1-1")  # Hack to show dates in figure
years = dff.columns

dff = dff.resample(resample_period).mean()

dff = dff.reset_index(names="date")

fig = px.line(
    dff,
    x="date",
    y=years,
    hover_data={"date": "|%d. %B, %H:%M"},
    #     color_discrete_sequence=px.colors.sequential.Blues,
    color_discrete_sequence=sns.color_palette("mako", len(dff.columns)).as_hex(),
)
fig.update_traces(opacity=0.5)
44/25: dff
44/26:
import seaborn as sns
ind = df.index.map(lambda x: x - pd.Timestamp(year=x.year, month=1, day=1, hour=0, minute=0))

dff = pd.DataFrame(df[area].values, index=ind, columns=[area])
44/27: dff.head()
44/28:
dff["Year"] = df.index.year
dff = dff.reset_index().pivot(index="index", columns="Year").dropna()
dff.columns = dff.columns.droplevel()
dff.index = dff.index + pd.Timestamp("1970-1-1")  # Hack to show dates in figure
years = dff.columns
44/29: dff.head()
44/30: dff = dff.resample(resample_period).mean()
44/31: dff.head()
44/32: dff = dff.reset_index(names="date")
44/33: dff.head()
44/34:
import seaborn as sns
ind = df.index.map(lambda x: x - pd.Timestamp(year=x.year, month=1, day=1, hour=0, minute=0))

dff = pd.DataFrame(df[area].values, index=ind, columns=[area])
44/35: dff.head()
44/36:
dff["Year"] = df.index.year
dff = dff.reset_index().pivot(index="index", columns="Year").dropna()
dff.columns = dff.columns.droplevel()
dff.index = dff.index + pd.Timestamp("1970-1-1")  # Hack to show dates in figure
years = dff.columns
44/37: dff = dff.resample(resample_period).mean()
44/38: dff.head()
44/39:
import seaborn as sns
ind = df.index.map(lambda x: x - pd.Timestamp(year=x.year, month=1, day=1, hour=0, minute=0))

dff = pd.DataFrame(df[area].values, index=ind, columns=[area])
44/40:
dff["Year"] = df.index.year
dff = dff.reset_index().pivot(index="index", columns="Year").dropna()
dff.columns = dff.columns.droplevel()
dff.index = dff.index + pd.Timestamp("1970-1-1")  # Hack to show dates in figure
dff.index.name = "date"
years = dff.columns
44/41: dff.head()
44/42:

fig = px.line(
    dff,
    x="date",
    y=years,
    hover_data={"date": "|%d. %B, %H:%M"},
    #     color_discrete_sequence=px.colors.sequential.Blues,
    color_discrete_sequence=sns.color_palette("mako", len(dff.columns)).as_hex(),
)
fig.update_traces(opacity=0.5)
44/43:
dff["Year"] = df.index.year
dff = dff.reset_index().pivot(index="index", columns="Year").dropna()
dff.columns = dff.columns.droplevel()
dff.index = dff.index + pd.Timestamp("1970-1-1")  # Hack to show dates in figure
years = dff.columns
44/44:
import seaborn as sns
ind = df.index.map(lambda x: x - pd.Timestamp(year=x.year, month=1, day=1, hour=0, minute=0))

dff = pd.DataFrame(df[area].values, index=ind, columns=[area])
44/45: dff.head()
44/46:
dff["Year"] = df.index.year
dff = dff.reset_index().pivot(index="index", columns="Year").dropna()
dff.columns = dff.columns.droplevel()
dff.index = dff.index + pd.Timestamp("1970-1-1")  # Hack to show dates in figure
years = dff.columns
44/47: dff = dff.resample(resample_period).mean()
44/48:

fig = px.line(
    dff,
    x="date",
    y=years,
    hover_data={"date": "|%d. %B, %H:%M"},
    #     color_discrete_sequence=px.colors.sequential.Blues,
    color_discrete_sequence=sns.color_palette("mako", len(dff.columns)).as_hex(),
)
fig.update_traces(opacity=0.5)
44/49: dff.head()
44/50: dff = dff.reset_index(names="date")
44/51:

fig = px.line(
    dff,
    x="date",
    y=years,
    hover_data={"date": "|%d. %B, %H:%M"},
    #     color_discrete_sequence=px.colors.sequential.Blues,
    color_discrete_sequence=sns.color_palette("mako", len(dff.columns)).as_hex(),
)
fig.update_traces(opacity=0.5)
44/52: dff.head()
44/53:
x = list(dff["date"])
y = dff.drop("date").mean(axis=1).values

y_upper = y + dff.std(axis=1).values
y_lower = y - dff.std(axis=1).values
44/54:
x = list(dff["date"])
y = dff.drop("date", axis=1).mean(axis=1).values

y_upper = y + dff.std(axis=1).values
y_lower = y - dff.std(axis=1).values
44/55: dff = dff.set_index(names="date")
44/56: dff = dff.set_index("date")
44/57: dff.head()
44/58:
x = list(dff.index)
y = dff.mean(axis=1).values

y_upper = y + dff.std(axis=1).values
y_lower = y - dff.std(axis=1).values
44/59:

### Line plots
area = "Utsira nord"
resample_period = "7D"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
44/60:
%load_ext autoreload
%autoreload 2
46/1:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
46/2:
%load_ext autoreload
%autoreload 2
46/3:
# start coding here
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
46/4:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
46/5:

df_wind_locations = pd.read_csv(wind_locations_file)
df_nve_wind_locations = pd.read_csv(wind_nve_locations_file, index_col=0)
df_nve_wind_locations = df_nve_wind_locations.sort_values(by="lat")  # Sort by south to north

df_locations = pd.concat([df_wind_locations, df_nve_wind_locations], axis=0)
df_locations = df_locations.reset_index(drop=True)
df_locations = df_locations.sort_values(by="lat")  # Sort by south to north

df = pd.read_csv(wind_data_file, index_col=0)
df.index = pd.to_datetime(df.index)
46/6:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
46/7:

df_mean_wind_nve = df[df_nve_wind_locations["location"]].mean()
# df_mean_wind_nve.to_csv("data/mean_wind.csv")
fig = px.bar(df_mean_wind_nve, text_auto=".2", title="")
fig.update_traces(textfont_size=12, textangle=0, textposition="inside", cliponaxis=False)
fig.update_layout(
    xaxis_title="Wind farm", yaxis_title="Mean wind power output", showlegend=False, template="plotly_white", width=1000
)
fig.show()
46/8:

# Plot correlation
fig = get_corr_figure(df)
fig.show()

fig = get_corr_distance_figure(df, df_locations)
fig.show()
46/9:

# Plot short-term variation
n_shifts = 25
quantile = 0.8

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile)
fig.update_layout(width=900)
fig.show()
46/10:

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile=0.9999)
fig.update_layout(width=900)
fig.show()
46/11:

fig = px.area(df.resample("1D").mean())
fig.show()
46/12:
resample_period = "7D"
fig = get_mean_std_wind_figure(df, resample_period)
fig.show()
46/13:
df["BE"].sort_values().plot()
vals = df["BE"].values
x = np.linspace(0, 1, len(vals))
px.line(x=x, y=np.sort(vals))
46/14:

px.line(df["BE"].sample(10000).sort_values().values)
46/15:

### Line plots
area = "Utsira nord"
resample_period = "7D"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
46/16:
resample_period = "1H"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
46/17:
## Scatter plots
df.columns
area_a = "SÃ¸rlige NordsjÃ¸ II"  # "Utsira nord"
area_b = "DE West"  # "AuvÃ¦r"

fig = get_scatter_2d_figure(df.sample(10000), area_a, area_b)
fig.show()
46/18:

fig = get_histogram_2d_figure(df, area_a, area_b)
fig.show()
46/19:

## Kernel density plots
N = 50
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="epanechnikov"
)
fig.show()
46/20:

## Kernel density plots
N = 50
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="epanechnikov"
)
fig.show()
46/21:

fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
46/22:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "Nordmela"  # "Utsira nord"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
46/23:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "Nordmela"  # "Utsira nord"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
46/24:
# %%

df.head()
import plotly.graph_objects as go

df_diff = df.diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.001)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Hourly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()

# %%

df_diff = df.resample("1D").mean().diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.008)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Daily changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
df_diff = df.resample("7D").mean().diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.01)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Weekly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
# %%
46/25:
# %%

df.head()
import plotly.graph_objects as go

df_diff = df.diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.001)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Hourly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
46/26: df_diff
46/27:
# df_diff["Sum"]
df.mean()
46/28:
# df_diff["Sum"]
df.mean(axis=1)
46/29: df = df.mean(axis=1)
46/30:
# %%
import plotly.graph_objects as go

df_diff = df.diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.001)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Hourly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
46/31: df.columns
46/32:

df_wind_locations = pd.read_csv(wind_locations_file)
df_nve_wind_locations = pd.read_csv(wind_nve_locations_file, index_col=0)
df_nve_wind_locations = df_nve_wind_locations.sort_values(by="lat")  # Sort by south to north

df_locations = pd.concat([df_wind_locations, df_nve_wind_locations], axis=0)
df_locations = df_locations.reset_index(drop=True)
df_locations = df_locations.sort_values(by="lat")  # Sort by south to north

df = pd.read_csv(wind_data_file, index_col=0)
df.index = pd.to_datetime(df.index)
46/33: df["Sum"] = df.mean(axis=1)
46/34:
# %%
import plotly.graph_objects as go

df_diff = df.diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.001)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Hourly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
46/35:
df_diff = df.resample("1D").mean().diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.008)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Daily changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
46/36:

# %%
df_diff = df.resample("7D").mean().diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.01)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Weekly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
# %%
46/37:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))


diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


layout = go.Layout(
    title_text="", xaxis_title_text="Weekly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
46/38:
def func():
    return 2
46/39: func()
48/1:
import powerplantmatching as pm
import pandas as pd
48/2:
geo = pm.data.GEO()

geo.head()
48/3:
entsoe = pm.data.ENTSOE()

entsoe.head()
48/4: geo.powerplant.plot_map(figsize=(11, 8));
48/5: geo.powerplant.lookup().head(20).to_frame()
48/6:
print('Total capacity of GEO is: \n  {} MW \n'.format(geo.Capacity.sum()));
print('The technology types are: \n {} '.format(geo.Technology.unique()))
48/7: geo.powerplant.fill_missing_commissioning_years().head()
48/8:
print('Total capacity of GEO is: \n  {} MW \n'.format(geo.Capacity.sum()));
print('The technology types are: \n {} '.format(geo.Technology.unique()))
48/9: geo.to_latex()
48/10: print(geo.to_latex())
49/1: nve_hydro_dataset = "https://www.nve.no/umbraco/api/Powerplant/GetHydroPowerPlantsInOperation"
49/2:
import powerplantmatching as pm
import pandas as pd
49/3: nve_hydro_dataset = "https://www.nve.no/umbraco/api/Powerplant/GetHydroPowerPlantsInOperation"
49/4: df = pd.read_json(nve_hydro_dataset)
49/5: df.head()
49/6: pm.core.package_config['data_dir']
49/7: df_orig = pm.powerplants(from_url=True)
49/8: df_orig.head()
49/9: df.Fueltype
49/10: df_orig.Fueltype
49/11: df_orig.Fueltype.unique()
49/12: df_orig[df_orig.Fueltype=="Hydro"]
49/13: df_orig[(df_orig.Fueltype=="Hydro") & df_orig.Country =="Norway"]
49/14: df_orig[(df_orig.Fueltype=="Hydro") & (df_orig.Country =="Norway")]
49/15: df_orig[(df_orig.Fueltype=="Hydro") & (df_orig.Country =="Norway")].Capacity.sum()
49/16: df_orig[(df_orig.Fueltype=="Hydro") & (df_orig.Country =="Norway")].Capacity.sum()
49/17: df_orig[(df_orig.Fueltype=="Hydro") & (df_orig.Country =="Norway")]
49/18: df.MaksYtelse.sum()
49/19: df.columns
49/20: df_orig.columns
49/21: df.head()
49/22: df.VannKVType
49/23: df.VannKVType.unique()
49/24: df.BruttoFallhoyde_M.plot()
49/25: df.BruttoFallhoyde_M.hist()
49/26: df_jrc = pm.data.JRC()
49/27: df_jrc.head()
49/28: df_orig[(df_orig.Fueltype=="Hydro") & (df_orig.Country =="Norway")].Capacity.sum()
49/29: df_jrc[(df_jrc.Fueltype=="Hydro") & (df_jrc.Country =="Norway")].Capacity.sum()
49/30: df_jrc[(df_jrc.Fueltype=="Hydro") & (df_jrc.Country =="Norway")]
49/31: df_jrc[(df_jrc.Fueltype=="Hydro") & (df_jrc.Country =="Norway")].Capacity.min()
49/32:
import requests
r = requests.get(nve_hydro_dataset)
49/33: r.content
49/34: df.shape
49/35: df_jrc.head()
49/36: df.head()
49/37: df_jrc[(df_jrc.Fueltype=="Hydro") & (df_jrc.Country =="Norway")]
50/1: import nve_sintef_model
50/2: detd_path = "/Users/mah/Library/CloudStorage/OneDrive-NTNU/Postdoc/Dataset/nve-datasett/detd-filer/DETD_vassdrag"
49/38: df_jrc.Set.unique()
49/39: df_jrc[(df_jrc.Fueltype=="Hydro") & (df_jrc.Country =="Norway") & (df_jrc.Set == "PP")]
49/40: df_jrc[(df_jrc.Fueltype=="Hydro") & (df_jrc.Country =="Norway") & (df_jrc.Set == "Store")]
49/41: df_jrc.Technology.unique()
49/42: df_jrc.Technology.value_counts()
49/43: df_jrc.Fueltype.value_counts()
49/44: df_jrc.Capacity.info()
49/45: df_entsoe = pm.data.ENTSOE()
49/46: df_entsoe.Technology.value_counts()
49/47: df.VannKVType.value_counts()
49/48: df_entsoe[(df_entsoe.Fueltype=="Hydro") & (df_entsoe.Country =="Norway")]
49/49: df_entsoe[(df_entsoe.Fueltype=="Hydro") & (df_entsoe.Country =="Norway")].Tecnology.value_counts()
49/50: df_entsoe[(df_entsoe.Fueltype=="Hydro") & (df_entsoe.Country =="Norway")].Technology.value_counts()
49/51: df_pp = pm.powerplants(from_url=True)
49/52: df_pp[(df_pp.Fueltype=="Hydro") & (df_pp.Country =="Norway")]
49/53: df_pp[(df_pp.Fueltype=="Hydro") & (df_pp.Country =="Norway")].Technology.value_counts()
49/54: df_pp[(df_pp.Fueltype=="Hydro") & (df_pp.Country =="Norway")].Set.value_counts()
49/55: df_pp[(df_pp.Fueltype=="Hydro") & (df_pp.Country =="Norway")].Technology.value_counts()
52/1: import atlite
52/2:
import os
import sys
sys.path.insert(1, os.path.join(sys.path[0], ".."))
52/3: import atlite
53/1: import atlite
53/2:
import logging

logging.basicConfig(level=logging.INFO)
53/3:
import logging

logging.basicConfig(level=logging.DEBUG)
53/4:
cutout = atlite.Cutout(
    path="western-europe-2011-01.nc",
    module="era5",
    x=slice(-13.6913, 1.7712),
    y=slice(49.9096, 60.8479),
    time="2011-01",
)
53/5: cutout.extent
53/6: cutout.prepare()
53/7: cutout.prepare()
53/8: cutout.prepare()
53/9: cutout.prepare()
53/10: cutout.data
53/11: cutout.prepared_features
53/12: cutout.wind
54/1:
import atlite
import xarray as xr
import pandas as pd
import scipy.sparse as sp
import numpy as np

import pgeocode
from collections import OrderedDict
54/2:
import atlite
import xarray as xr
import pandas as pd
import scipy.sparse as sp
import numpy as np

import pgeocode
from collections import OrderedDict
54/3:
import matplotlib.pyplot as plt

%matplotlib inline

import seaborn as sns

sns.set_style("whitegrid")
54/4:
import matplotlib.pyplot as plt

%matplotlib inline

import seaborn as sns

sns.set_style("whitegrid")
54/5:
import requests
import os
import zipfile


def download_file(url, local_filename):
    # variant of http://stackoverflow.com/a/16696317
    if not os.path.exists(local_filename):
        r = requests.get(url, stream=True)
        with open(local_filename, "wb") as f:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
    return local_filename
54/6:
opsd_fn = download_file(
    "https://data.open-power-system-data.org/index.php?package=time_series&version=2019-06-05&action=customDownload&resource=3&filter%5B_contentfilter_cet_cest_timestamp%5D%5Bfrom%5D=2012-01-01&filter%5B_contentfilter_cet_cest_timestamp%5D%5Bto%5D=2013-05-01&filter%5BRegion%5D%5B%5D=DE&filter%5BVariable%5D%5B%5D=solar_generation_actual&filter%5BVariable%5D%5B%5D=wind_generation_actual&downloadCSV=Download+CSV",
    "time_series_60min_singleindex_filtered.csv",
)
54/7:
opsd = pd.read_csv(opsd_fn, parse_dates=True, index_col=0)

# we later use the (in current version) timezone unaware datetime64
# to work together with this format, we have to remove the timezone
# timezone information. We are working with UTC everywhere.

opsd.index = opsd.index.tz_convert(None)

# We are only interested in the 2012 data
opsd = opsd[("2011" < opsd.index) & (opsd.index < "2013")]
54/8: opsd.head()
54/9: opsd.DE_solar_generation_actual.plot()
54/10: opsd.DE_wind_generation_actual.plot()
54/11:
eeg_fn = download_file(
    "http://www.energymap.info/download/eeg_anlagenregister_2015.08.utf8.csv.zip",
    "eeg_anlagenregister_2015.08.utf8.csv.zip",
)

with zipfile.ZipFile(eeg_fn, "r") as zip_ref:
    zip_ref.extract("eeg_anlagenregister_2015.08.utf8.csv")
54/12:
import cartopy.io.shapereader as shpreader
import geopandas as gpd

shp = shpreader.Reader(
    shpreader.natural_earth(
        resolution="10m", category="cultural", name="admin_0_countries"
    )
)
de_record = list(filter(lambda c: c.attributes["ISO_A2"] == "DE", shp.records()))[0]
de = pd.Series({**de_record.attributes, "geometry": de_record.geometry})
x1, y1, x2, y2 = de["geometry"].bounds
54/13:
import cartopy.io.shapereader as shpreader
import geopandas as gpd

shp = shpreader.Reader(
    shpreader.natural_earth(
        resolution="10m", category="cultural", name="admin_0_countries"
    )
)
de_record = list(filter(lambda c: c.attributes["ISO_A2"] == "DE", shp.records()))[0]
de = pd.Series({**de_record.attributes, "geometry": de_record.geometry})
x1, y1, x2, y2 = de["geometry"].bounds
54/14: de
54/15: de.geometry
54/16:
import cartopy.io.shapereader as shpreader
import geopandas as gpd

shp = shpreader.Reader(
    shpreader.natural_earth(
        resolution="10m", category="cultural", name="admin_0_countries"
    )
)
de_record = list(filter(lambda c: c.attributes["ISO_A2"] == "NO", shp.records()))[0]
de = pd.Series({**de_record.attributes, "geometry": de_record.geometry})
x1, y1, x2, y2 = de["geometry"].bounds
54/17:
import cartopy.io.shapereader as shpreader
import geopandas as gpd

shp = shpreader.Reader(
    shpreader.natural_earth(
        resolution="10m", category="cultural", name="admin_0_countries"
    )
)
de_record = list(filter(lambda c: c.attributes["ISO_A2"] == "no", shp.records()))[0]
de = pd.Series({**de_record.attributes, "geometry": de_record.geometry})
x1, y1, x2, y2 = de["geometry"].bounds
54/18: shp.records
54/19: shp.records()
54/20:
import cartopy.io.shapereader as shpreader
import geopandas as gpd

shp = shpreader.Reader(
    shpreader.natural_earth(
        resolution="10m", category="cultural", name="admin_0_countries"
    )
)
de_record = list(filter(lambda c: c.attributes["ISO_A2"] == "NOR", shp.records()))[0]
de = pd.Series({**de_record.attributes, "geometry": de_record.geometry})
x1, y1, x2, y2 = de["geometry"].bounds
54/21:
import cartopy.io.shapereader as shpreader
import geopandas as gpd

shp = shpreader.Reader(
    shpreader.natural_earth(
        resolution="10m", category="cultural", name="admin_0_countries"
    )
)
de_record = list(filter(lambda c: c.attributes["ISO_A2"] == "NO", shp.records()))[0]
de = pd.Series({**de_record.attributes, "geometry": de_record.geometry})
x1, y1, x2, y2 = de["geometry"].bounds
54/22:
import cartopy.io.shapereader as shpreader
import geopandas as gpd

shp = shpreader.Reader(
    shpreader.natural_earth(
        resolution="10m", category="cultural", name="admin_0_countries"
    )
)
de_record = list(filter(lambda c: c.attributes["ISO_A2"] == "DE", shp.records()))[0]
de = pd.Series({**de_record.attributes, "geometry": de_record.geometry})
x1, y1, x2, y2 = de["geometry"].bounds
54/23: shp.records()
54/24: de.geometry
54/25: x2
54/26: x1
54/27: de.geometry.bounds
54/28:
cutout = atlite.Cutout(
    "germany-2012",
    module="era5",
    x=slice(x1 - 0.2, x2 + 0.2),
    y=slice(y1 - 0.2, y2 + 0.2),
    chunks={"time": 100},
    time="2012",
)
54/29: cutout.prepare()
55/1:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
56/1:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
56/2:
%load_ext autoreload
%autoreload 2
56/3:
# start coding here
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
56/4:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
56/5:
df_wind_locations = pd.read_csv(wind_locations_file)
df_nve_wind_locations = pd.read_csv(wind_nve_locations_file, index_col=0)
df_nve_wind_locations = df_nve_wind_locations.sort_values(by="lat")  # Sort by south to north

df_locations = pd.concat([df_wind_locations, df_nve_wind_locations], axis=0)
df_locations = df_locations.reset_index(drop=True)
df_locations = df_locations.sort_values(by="lat")  # Sort by south to north

df = pd.read_csv(wind_data_file, index_col=0)
df.index = pd.to_datetime(df.index)
56/6:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
56/7: df.head()
56/8: df_locations.head()
56/9: df_nve_wind_locations.head()
56/10:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
56/11:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
56/12:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map", )
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
56/13:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map", )
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
57/1:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
57/2:
%load_ext autoreload
%autoreload 2
57/3:
# start coding here
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
57/4:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
57/5:
df_wind_locations = pd.read_csv(wind_locations_file)
df_nve_wind_locations = pd.read_csv(wind_nve_locations_file, index_col=0)
df_nve_wind_locations = df_nve_wind_locations.sort_values(by="lat")  # Sort by south to north

df_locations = pd.concat([df_wind_locations, df_nve_wind_locations], axis=0)
df_locations = df_locations.reset_index(drop=True)
df_locations = df_locations.sort_values(by="lat")  # Sort by south to north

df = pd.read_csv(wind_data_file, index_col=0)
df.index = pd.to_datetime(df.index)
57/6: df.head()
57/7: df_locations.head()
57/8: df_nve_wind_locations.head()
57/9:

df_mean_wind_nve = df[df_nve_wind_locations["location"]].mean()
# df_mean_wind_nve.to_csv("data/mean_wind.csv")
fig = px.bar(df_mean_wind_nve, text_auto=".2", title="")
fig.update_traces(textfont_size=12, textangle=0, textposition="inside", cliponaxis=False)
fig.update_layout(
    xaxis_title="Wind farm", yaxis_title="Mean wind power output", showlegend=False, template="plotly_white", width=1000
)
fig.show()
57/10:

# Plot correlation
fig = get_corr_figure(df)
fig.show()

fig = get_corr_distance_figure(df, df_locations)
fig.show()
57/11:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map", )
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
57/12:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map", )
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
57/13:

df_mean_wind_nve = df[df_nve_wind_locations["location"]].mean()
# df_mean_wind_nve.to_csv("data/mean_wind.csv")
fig = px.bar(df_mean_wind_nve, text_auto=".2", title="")
fig.update_traces(textfont_size=12, textangle=0, textposition="inside", cliponaxis=False)
fig.update_layout(
    xaxis_title="Wind farm", yaxis_title="Mean wind power output", showlegend=False, template="plotly_white", width=1000
)
fig.show()
57/14:

# Plot correlation
fig = get_corr_figure(df)
fig.show()

fig = get_corr_distance_figure(df, df_locations)
fig.show()
57/15:

# Plot short-term variation
n_shifts = 25
quantile = 0.8

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile)
fig.update_layout(width=900)
fig.show()
57/16:

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile=0.9999)
fig.update_layout(width=900)
fig.show()
57/17:

fig = px.area(df.resample("1D").mean())
fig.show()
57/18:
resample_period = "7D"
fig = get_mean_std_wind_figure(df, resample_period)
fig.show()
57/19:
df["BE"].sort_values().plot()
vals = df["BE"].values
x = np.linspace(0, 1, len(vals))
px.line(x=x, y=np.sort(vals))
57/20:

px.line(df["BE"].sample(10000).sort_values().values)
57/21:

### Line plots
area = "Utsira nord"
resample_period = "7D"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
57/22:
resample_period = "1H"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
57/23:
# resample_period = "1H"
# fig = get_mean_std_wind_yearly_figure(df, resample_period)
57/24:
## Scatter plots
df.columns
area_a = "SÃ¸rlige NordsjÃ¸ II"  # "Utsira nord"
area_b = "DE West"  # "AuvÃ¦r"

fig = get_scatter_2d_figure(df.sample(10000), area_a, area_b)
fig.show()
57/25:

fig = get_histogram_2d_figure(df, area_a, area_b)
fig.show()
57/26:

## Kernel density plots
N = 50
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="epanechnikov"
)
fig.show()
57/27:

fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
57/28:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "Nordmela"  # "Utsira nord"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
57/29:

# fig1 = get_scatter_density_2d_figure(df.sample(10000), area_a, area_b)
# fig1.show()
57/30:
# %%
import plotly.graph_objects as go
df["Sum"] = df.mean(axis=1)
df_diff = df.diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.001)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Hourly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
57/31:
df_diff = df.resample("1D").mean().diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.008)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Daily changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
57/32:

# %%
df_diff = df.resample("7D").mean().diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.01)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Weekly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
# %%
57/33:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))


diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
57/34: import plotly.io as pi
57/35: import plotly.io as pio
57/36: pio.renderers.default
57/37: pio.renderers.default = "iframe"
57/38:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map", )
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
57/39: pio.renderers.default = "notebook"
57/40: df.head()
57/41: df_locations.head()
57/42: df_nve_wind_locations.head()
57/43:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map", )
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
57/44: pio.renderers.default = "plotly_mimetype+notebook"
47/1:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
47/2:
%load_ext autoreload
%autoreload 2
47/3:
# start coding here
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
47/4: import plotly.io as pio
47/5: pio.renderers.default
47/6:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
47/7:
df_wind_locations = pd.read_csv(wind_locations_file)
df_nve_wind_locations = pd.read_csv(wind_nve_locations_file, index_col=0)
df_nve_wind_locations = df_nve_wind_locations.sort_values(by="lat")  # Sort by south to north

df_locations = pd.concat([df_wind_locations, df_nve_wind_locations], axis=0)
df_locations = df_locations.reset_index(drop=True)
df_locations = df_locations.sort_values(by="lat")  # Sort by south to north

df = pd.read_csv(wind_data_file, index_col=0)
df.index = pd.to_datetime(df.index)
47/8: df.head()
47/9: df_locations.head()
47/10: df_nve_wind_locations.head()
47/11:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map", )
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
47/12:

df_mean_wind_nve = df[df_nve_wind_locations["location"]].mean()
# df_mean_wind_nve.to_csv("data/mean_wind.csv")
fig = px.bar(df_mean_wind_nve, text_auto=".2", title="")
fig.update_traces(textfont_size=12, textangle=0, textposition="inside", cliponaxis=False)
fig.update_layout(
    xaxis_title="Wind farm", yaxis_title="Mean wind power output", showlegend=False, template="plotly_white", width=1000
)
fig.show()
47/13:

df_mean_wind_nve = df[df_nve_wind_locations["location"]].mean()
# df_mean_wind_nve.to_csv("data/mean_wind.csv")
fig = px.bar(df_mean_wind_nve, text_auto=".2", title="")
fig.update_traces(textfont_size=12, textangle=0, textposition="inside", cliponaxis=False)
fig.update_layout(
    xaxis_title="Wind farm", yaxis_title="Mean wind power output", showlegend=False, template="plotly_white", width=1000
)
fig.show()
47/14:

# Plot correlation
fig = get_corr_figure(df)
fig.show()

fig = get_corr_distance_figure(df, df_locations)
fig.show()
47/15:

# Plot correlation
fig = get_corr_figure(df)
fig.show()
47/16:

fig = get_corr_distance_figure(df, df_locations)
fig.show()*
47/17:

fig = get_corr_distance_figure(df, df_locations)
fig.show()
47/18:

fig = get_corr_distance_figure(df, df_locations, resolution='1H')
fig.show()
47/19:

fig = get_corr_distance_figure(df, df_locations, resolution='1H')
fig.show()
47/20:
fig = get_corr_distance_figure(df, df_locations, resolution='7D')
fig.show()
47/21:
fig = get_corr_distance_figure(df, df_locations, resolution='1D')
fig.show()
59/1:

fig = get_corr_distance_figure(df, df_locations, resolution='1H')
fig.show()
59/2:

fig = get_corr_distance_figure(df, df_locations, resolution='1H')
fig.show()
60/1:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
60/2:
%load_ext autoreload
%autoreload 2
60/3:
# start coding here
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
60/4: import plotly.io as pio
60/5: pio.renderers.default
60/6:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
60/7:
df_wind_locations = pd.read_csv(wind_locations_file)
df_nve_wind_locations = pd.read_csv(wind_nve_locations_file, index_col=0)
df_nve_wind_locations = df_nve_wind_locations.sort_values(by="lat")  # Sort by south to north

df_locations = pd.concat([df_wind_locations, df_nve_wind_locations], axis=0)
df_locations = df_locations.reset_index(drop=True)
df_locations = df_locations.sort_values(by="lat")  # Sort by south to north

df = pd.read_csv(wind_data_file, index_col=0)
df.index = pd.to_datetime(df.index)
60/8:
# Plot locations on map
fig = px.scatter_mapbox(
    df_locations,
    lat="lat",
    lon="lon",
    color="location",
    zoom=3,
    size_max=10,
    height=600,
    size=[3 for _ in df_locations.iterrows()],
)
fig.update_layout(mapbox_style="open-street-map", )
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show(config=dict(editable=True))
60/9:

df_mean_wind_nve = df[df_nve_wind_locations["location"]].mean()
# df_mean_wind_nve.to_csv("data/mean_wind.csv")
fig = px.bar(df_mean_wind_nve, text_auto=".2", title="")
fig.update_traces(textfont_size=12, textangle=0, textposition="inside", cliponaxis=False)
fig.update_layout(
    xaxis_title="Wind farm", yaxis_title="Mean wind power output", showlegend=False, template="plotly_white", width=1000
)
fig.show()
60/10:

# Plot correlation
fig = get_corr_figure(df)
fig.show()
60/11:

fig = get_corr_distance_figure(df, df_locations, resolution='1H')
fig.show()
60/12:

# Plot correlation
fig = get_corr_figure(df)
fig.show()
60/13:

fig = get_corr_distance_figure(df, df_locations, resolution='1H')
fig.show()
60/14: from Wind.analyze import get_corr_distance_df
60/15: df_corr_1h = get_corr_distance_df(df, df_locations, resolution="1H")
60/16: df_corr_1H = get_corr_distance_df(df, df_locations, resolution="1H")
60/17: df_corr_1D = get_corr_distance_df(df, df_locations, resolution="1D")
60/18: df_corr_7D = get_corr_distance_df(df, df_locations, resolution="7D")
60/19:

fig = get_corr_distance_figure(df, df_locations, resolution='1H')
fig.show()
60/20: df_corr_1H = get_corr_distance_df(df, df_locations, resolution="1H")
60/21: df_corr_1D = get_corr_distance_df(df, df_locations, resolution="1D")
60/22: df_corr_7D = get_corr_distance_df(df, df_locations, resolution="7D")
60/23:
corrs = {}
resolutions = ["1H", "1D", "7D"]
for res in resolutions:
    corrs[res] = get_corr_distance_df(df, df_locations,resolution=res)
60/24:
from Wind.analyze import get_corr_distance_df
import plotly.graph_objects as go
60/25:
from Wind.analyze import get_corr_distance_df, get_exponential_function
import plotly.graph_objects as go
60/26:
corrs = {}
resolutions = ["1H", "1D", "7D"]
colors = ["black", "blue", "green"]
data = []
for i, res in enumerate(resolutions):
    df_corr_dist = get_corr_distance_df(df, df_locations,resolution=res)
    data.append(
        go.Scatter(
            x=df_corr_dist["Distance [km]"],
            y=df_corr_dist["Correlation"],
            text=df_corr_dist["Span"],
            marker=dict(color=colors[i], size=5, line=dict(width=0)),
            mode="markers",
            name=f"Between two wind farms - {res}"
        )
    )
    xn, yn, popt = get_exponential_function(df_corr_dist)
    data.append(
        go.Scatter(
            x=xn,
            y=yn,
            line=dict(color=colors[i], width=3),
            name=f"Exponential fit with {res} resolution",#r"$1.05 \exp(\frac{-1}{490.4}x) + 0.02$",
        )
    )

fig = go.Figure(data=data)
fig.update_layout(template=my_template, title=f"", xaxis_title="Distance [km]", yaxis_title="Correlation [-]")
fig.show()
60/27:
from Wind.analyze import get_corr_distance_df, get_exponential_function
from Wind.plotly_template import my_template
import plotly.graph_objects as go
60/28:
corrs = {}
resolutions = ["1H", "1D", "7D"]
colors = ["black", "blue", "green"]
data = []
for i, res in enumerate(resolutions):
    df_corr_dist = get_corr_distance_df(df, df_locations,resolution=res)
    data.append(
        go.Scatter(
            x=df_corr_dist["Distance [km]"],
            y=df_corr_dist["Correlation"],
            text=df_corr_dist["Span"],
            marker=dict(color=colors[i], size=5, line=dict(width=0)),
            mode="markers",
            name=f"Between two wind farms - {res}"
        )
    )
    xn, yn, popt = get_exponential_function(df_corr_dist)
    data.append(
        go.Scatter(
            x=xn,
            y=yn,
            line=dict(color=colors[i], width=3),
            name=f"Exponential fit with {res} resolution",#r"$1.05 \exp(\frac{-1}{490.4}x) + 0.02$",
        )
    )

fig = go.Figure(data=data)
fig.update_layout(template=my_template, title=f"", xaxis_title="Distance [km]", yaxis_title="Correlation [-]")
fig.show()
60/29:
corrs = {}
resolutions = ["1H", "1D", "7D", "14D"]
colors = ["black", "blue", "green", "red"]
data = []
for i, res in enumerate(resolutions):
    df_corr_dist = get_corr_distance_df(df, df_locations,resolution=res)
    data.append(
        go.Scatter(
            x=df_corr_dist["Distance [km]"],
            y=df_corr_dist["Correlation"],
            text=df_corr_dist["Span"],
            marker=dict(color=colors[i], size=5, line=dict(width=0)),
            mode="markers",
            name=f"Between two wind farms - {res}"
        )
    )
    xn, yn, popt = get_exponential_function(df_corr_dist)
    data.append(
        go.Scatter(
            x=xn,
            y=yn,
            line=dict(color=colors[i], width=3),
            name=f"Exponential fit with {res} resolution",#r"$1.05 \exp(\frac{-1}{490.4}x) + 0.02$",
        )
    )

fig = go.Figure(data=data)
fig.update_layout(template=my_template, title=f"", xaxis_title="Distance [km]", yaxis_title="Correlation [-]")
fig.show()
60/30:
corrs = {}
resolutions = ["1H", "1D", "7D", "30D"]
colors = ["black", "blue", "green", "red"]
data = []
for i, res in enumerate(resolutions):
    df_corr_dist = get_corr_distance_df(df, df_locations,resolution=res)
    data.append(
        go.Scatter(
            x=df_corr_dist["Distance [km]"],
            y=df_corr_dist["Correlation"],
            text=df_corr_dist["Span"],
            marker=dict(color=colors[i], size=5, line=dict(width=0)),
            mode="markers",
            name=f"Between two wind farms - {res}"
        )
    )
    xn, yn, popt = get_exponential_function(df_corr_dist)
    data.append(
        go.Scatter(
            x=xn,
            y=yn,
            line=dict(color=colors[i], width=3),
            name=f"Exponential fit with {res} resolution",#r"$1.05 \exp(\frac{-1}{490.4}x) + 0.02$",
        )
    )

fig = go.Figure(data=data)
fig.update_layout(template=my_template, title=f"", xaxis_title="Distance [km]", yaxis_title="Correlation [-]")
fig.show()
60/31:
from Wind.analyze import get_corr_distance_df, get_exponential_function, get_multiple_corr_distance_figure
from Wind.plotly_template import my_template
import plotly.graph_objects as go
60/32:
resolutions = ["1H", "1D", "7D", "30D"]
colors = ["black", "blue", "green", "red"]
get_multiple_corr_distance_figure(df, df_locations, resolutions, colors)
60/33:
resolutions = ["1H", "1D", "7D", "30D"]
colors = ["black", "blue", "green", "red"]
fig = get_multiple_corr_distance_figure(df, df_locations, resolutions, colors)
fig.show()
fig.write_image("images/corr-distance.pdf")
60/34:

n_shifts = 25
quantile = 0.8

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile)
fig.update_layout(width=900)
fig.show()
60/35:

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile=0.9999)
fig.update_layout(width=900)
fig.show()
60/36:

fig = px.area(df.resample("1D").mean())
fig.show()
60/37:

n_shifts = 25
quantile = 0.9

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile)
fig.update_layout(width=900)
fig.show()
60/38:

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile=0.9999)
fig.update_layout(width=900)
fig.show()
60/39:

fig = get_hours_shift_figure(df, df_nve_wind_locations, n_shifts, quantile=0.9999)
fig.update_layout(width=900)
fig.show()
fig.write_image("images/shift-quantile0.99.pdf")
60/40:

fig = get_hours_shift_figure(df.resample("1D").mean(), df_nve_wind_locations, n_shifts, quantile=0.9999)
fig.update_layout(width=900)
fig.show()
60/41:

fig = get_hours_shift_figure(df.resample("1D").mean(), df_nve_wind_locations, n_shifts, quantile=0.99)
fig.update_layout(width=900)
fig.show()
60/42:
resample_period = "7D"
fig = get_mean_std_wind_figure(df, resample_period)
fig.show()
60/43:
# resample_period = "7D"
# fig = get_mean_std_wind_figure(df, resample_period)
# fig.show()
60/44:
df["BE"].sort_values().plot()
vals = df["BE"].values
x = np.linspace(0, 1, len(vals))
px.line(x=x, y=np.sort(vals))
60/45:
df["BE"].sort_values().plot()
vals = df["BE"].values
x = np.linspace(0, 1, len(vals))
px.line(x=x, y=np.sort(vals))
60/46:

px.line(df["BE"].sample(10000).sort_values().values)
60/47:

### Line plots
area = "Utsira nord"
resample_period = "7D"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
60/48:
resample_period = "1H"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
60/49:

### Line plots
area = "Utsira nord"
resample_period = "7D"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
60/50:

### Line plots
area = "Utsira nord"
resample_period = "7D"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
60/51:
resample_period = "1H"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
60/52:

### Line plots
area = "Utsira nord"
resample_period = "7D"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
fig.write_image("images/Utsira nord-std-wind-7D.pdf")
60/53:
resample_period = "1H"
fig = get_line_plot_with_mean(df, area, resample_period)
fig.show()
fig.write_image("images/Utsira nord-std-wind-1H.pdf")
60/54:
## Scatter plots
df.columns
area_a = "SÃ¸rlige NordsjÃ¸ II"  # "Utsira nord"
area_b = "DE West"  # "AuvÃ¦r"

fig = get_scatter_2d_figure(df.sample(10000), area_a, area_b)
fig.show()
60/55:

fig = get_histogram_2d_figure(df, area_a, area_b)
fig.show()
60/56:

## Kernel density plots
N = 50
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="epanechnikov"
)
fig.show()
60/57:

fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/58:

fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/59:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "Nordmela"  # "Utsira nord"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=4, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/60:

fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=2, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/61:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "Nordmela"  # "Utsira nord"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=2, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/62:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "DE West"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=2, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/63:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "DE West"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=3, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/64:

fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=3, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/65:
area_a = "SÃ¸rlige NordsjÃ¸ II"  # "Utsira nord"
area_b = "Nordmela"  # "AuvÃ¦r"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=3, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
60/66:
get_hours_shift_figure
area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "DE West"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, 0, z_max=3, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
fig
60/67:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "DE West"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=3, n_scatter_samples=0, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
fig
60/68:

area_a = "SÃ¸rlige NordsjÃ¸ II"
area_b = "DE West"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=3, n_scatter_samples=0, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
fig.write_image("images/scatter-Soerlige Nordsjoe II-DE West.pdf")
60/69:
area_a = "SÃ¸rlige NordsjÃ¸ II"  # "Utsira nord"
area_b = "Nordmela"  # "AuvÃ¦r"
fig = get_scatter_with_kernel_density_2d_figure(
    df, area_a, area_b, N, z_max=3, n_scatter_samples=500, bandwidth=0.1, rtol=0.01, kernel="gaussian"
)
fig.show()
fig.write_image("images/scatter-Soerlige Nordsjoe II-Nordmela.pdf")
60/70:

# Plot correlation
fig = get_corr_figure(df)
fig.show()
60/71:

# Plot correlation
fig = get_corr_figure(df)
fig.show()
60/72:

# Plot correlation
fig = get_corr_figure(df)
fig.show()
60/73:

# Plot correlation
fig = get_corr_figure(df.resample('1D').mean())
fig.show()
60/74: df = df.drop(columns=["All 15 wind farms", "Farms north of Stadt", "Farms south of Stadt"])
60/75:

# Plot correlation
fig = get_corr_figure(df.resample('1D').mean())
fig.show()
60/76:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
}, index = df.index)
60/77:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean())
fig.show()
60/78:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean())
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/79: len(df.columns), len(df_agg)
60/80: len(df.columns), len(df_agg.columns)
60/81:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean())
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/82:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean())
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/83:

# Plot correlation
fig = get_corr_figure(df.resample('1D').mean())
fig.show()
60/84:

# Plot correlation
fig = get_corr_figure(df.resample('1D').mean())
fig.show()
60/85:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=len(df.columns)/len(df_agg.columns))
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/86:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/(len(df.columns)/len(df_agg.columns)))
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/87:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/3)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/88:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/89:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({
    "All 15 wind farms": df.mean(axis=1),
    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2.5)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/90:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({

    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "All 15 wind farms": df.mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2.5)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/91:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({

    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "All 15 wind farms": df.mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2.5)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/92:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({

    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "All 15 wind farms": df.mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2.5)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/93:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({

    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "All 15 wind farms": df.mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2.5)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/94:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({

    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "All 15 wind farms": df.mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2.5)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/95:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({

    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "All 15 wind farms": df.mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2.5)
fig.show()

print(nl_cols, uk_cols, de_cols, dk_cols)
60/96:

# Plot correlation
fig = get_corr_figure(df.resample('1D').mean())
fig.show()
60/97:

# Plot correlation
fig = get_corr_figure(df.resample('1D').mean())
fig.show()
fig.write_image("images/corr-matrix-1day.pdf")
60/98:
froya_lat = df_nve_wind_locations[df_nve_wind_locations["location"] == "FrÃ¸yabanken"]["lat"].values[0]

cols_south = df_nve_wind_locations[df_nve_wind_locations["lat"] < froya_lat]["location"].to_list()
cols_north = df_nve_wind_locations[df_nve_wind_locations["lat"] >= froya_lat]["location"].to_list()
nl_cols = [i for i in df.columns if "NL" in i]
uk_cols = [i for i in df.columns if "UK" in i]
de_cols = [i for i in df.columns if "DE" in i]
dk_cols = [i for i in df.columns if "DK" in i]

df_agg = pd.DataFrame({

    "Farms north of Stadt": df[cols_north].mean(axis=1),
    "All 15 wind farms": df.mean(axis=1),
    "Farms south of Stadt": df[cols_south].mean(axis=1),
    "DK": df[dk_cols].mean(axis=1),
    "UK": df[uk_cols].mean(axis=1),
    "DE": df[de_cols].mean(axis=1),
    "NL": df[nl_cols].mean(axis=1)
}, index = df.index)

fig = get_corr_figure(df_agg.resample('1D').mean(), scale_size=1/2.5)
fig.show()
fig.write_image("images/corr-matrix-1day-aggregated.pdf")

print(nl_cols, uk_cols, de_cols, dk_cols)
60/99:
# %%
import plotly.graph_objects as go
df["Sum"] = df.mean(axis=1)
df_diff = df.diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.001)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Hourly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
60/100:
df_diff = df.resample("1D").mean().diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.008)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Daily changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
60/101:

# %%
df_diff = df.resample("7D").mean().diff(periods=1)

data = []
for col in ["BE", "SÃ¸rlige NordsjÃ¸ II", "Sum"]:
    bins = np.arange(-0.7, 0.7, 0.01)
    hist, bin_edges = np.histogram(df_diff[col], bins=bins, density=True)
    data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name=col))

layout = go.Layout(
    title_text="", xaxis_title_text="Weekly changes of wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()
# %%
60/102:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))


diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay"
)

fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/103:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))


diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis]))))


diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))


diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))


layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/104:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/105:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

viz_cols
fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/106:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/107:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.scatter.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.scatter.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.scatter.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.scatter.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.scatter.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/108:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/109:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.04).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/110:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/111:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("1H").mean().diff(periods=2)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template
)

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/112:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=600
)

for i,trace in enumerate(data):
    if i//2:
        trace.visible = False

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/113:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=600
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = False

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/114:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=600
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/115:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=600, legend=dict(x=1, y=1)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/116:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=600, legend=dict(x=0.8, y=1)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/117:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=600, legend=dict(x=0.8, y=0.9)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/118:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=600, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/119:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=400, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

# viz_cols
# fig.update_traces(visible="legendonly", selector=lambda t: not t.name in viz_cols)


fig = go.Figure(data=data, layout=layout)
fig.show()

# %%
60/120:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("2H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))
60/121:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=400, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

fig = go.Figure(data=data, layout=layout)
fig.show()
fig.write_image("images/changes-wind-output")

# %%
60/122:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("2H").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))
60/123: diff
60/124:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().pct_change(periods=1).dropna()
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("2H").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))
60/125: diff
60/126: hist
60/127: bin_edges
60/128: diff.dropna().values[:, np.newaxis]
60/129:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("2H").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().pct_change(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))
60/130:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))
60/131: diff.dropna().values[:, np.newaxis]
60/132:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "BE"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("2H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))
60/133:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=400, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

fig = go.Figure(data=data, layout=layout)
fig.show()
fig.write_image("images/changes-wind-output")

# %%
60/134:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability", barmode="overlay", template=my_template,
    width=600, height=400, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

fig = go.Figure(data=data, layout=layout)
fig.show()
fig.write_image("images/changes-wind-output.pdf")

# %%
60/135:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability density function (pdf)", barmode="overlay", template=my_template,
    width=600, height=400, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

fig = go.Figure(data=data, layout=layout)
fig.show()
fig.write_image("images/changes-wind-output.pdf")

# %%
60/136:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability density function", barmode="overlay", template=my_template,
    width=600, height=400, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

fig = go.Figure(data=data, layout=layout)
fig.show()
fig.write_image("images/changes-wind-output.pdf")

# %%
60/137:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "SÃ¸relige NordsjÃ¸ II"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("2H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))
60/138:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability density function", barmode="overlay", template=my_template,
    width=600, height=400, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

fig = go.Figure(data=data, layout=layout)
fig.show()
fig.write_image("images/changes-wind-output.pdf")

# %%
60/139: df.columns
60/140:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "SÃ¸relige NordsjÃ¸ II"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("2H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))
60/141:

from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go

data = []
col = "SÃ¸rlige NordsjÃ¸ II"
diff = df[col].resample("1H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="hourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="hourly"))

diff = df[col].resample("2H").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.001)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="bihourly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.02).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="bihourly"))

diff = df[col].resample("1D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.008)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="daily"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.03).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="daily"))

diff = df[col].resample("7D").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.01)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="weekly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="weekly"))

diff = df[col].resample("1M").mean().diff(periods=1)
bins = np.arange(-0.7, 0.7, 0.02)
hist, bin_edges = np.histogram(diff, bins=bins, density=True)
data.append(go.Bar(x=bin_edges, y=hist, opacity=0.8, name="monthly"))
kde = KernelDensity(kernel="gaussian", bandwidth=0.05).fit(diff.dropna().values[:, np.newaxis])
data.append(go.Line(x=bins, y=np.exp(kde.score_samples(bins[:, np.newaxis])), name="monthly"))
60/142:

layout = go.Layout(
    title_text="", xaxis_title_text="Changes in wind outputs", yaxis_title_text="Probability density function", barmode="overlay", template=my_template,
    width=600, height=400, legend=dict(x=0.8, y=0.98)
)

for i,trace in enumerate(data):
    if i//2 or i == 0:
        trace.visible = "legendonly"

fig = go.Figure(data=data, layout=layout)
fig.show()
fig.write_image("images/changes-wind-output.pdf")

# %%
60/143:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
60/144:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
input_lagrangian_models = Path(snakemake.input[3])
   1:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
input_lagrangian_models = Path(snakemake.input[3])
   2:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
   3:
%load_ext autoreload
%autoreload 2
   4:
# start coding here
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
   5: import plotly.io as pio
   6: pio.renderers.default
   7:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
input_lagrangian_models = Path(snakemake.input[3])
   8: snakemake.input
   9:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\x92\x04\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94\x8c\x13images/my_image.pdf\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh^K\x01h`K\x01hbh[ub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
  10:
%load_ext autoreload
%autoreload 2
  11:
# start coding here
import os
import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import plotly.express as px

sys.path.insert(1, os.path.join(sys.path[0], ".."))
from Wind.analyze import (
    get_corr_figure,
    get_hours_shift_figure,
    get_mean_std_wind_figure,
    get_corr_distance_figure,
    get_line_plot_with_mean,
    get_histogram_2d_figure,
    get_scatter_2d_figure,
    get_scatter_with_kernel_density_2d_figure,
    get_scatter_density_2d_figure,
)
  12: import plotly.io as pio
  13: pio.renderers.default
  14:
wind_locations_file = Path(snakemake.input[0])
wind_nve_locations_file = Path(snakemake.input[1])
wind_data_file = Path(snakemake.input[2])
input_lagrangian_models = Path(snakemake.input[3])
  15: snakemake.input
  16:

######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/mah/Library/Caches/pypoetry/virtualenvs/wind-covariation-5bQZunuM-py3.11/lib/python3.11/site-packages', '/Users/mah/Library/Caches/snakemake/snakemake/source-cache/runtime-cache/tmp9pqlbvxf/file/Users/mah/gitsource/wind-covariation/notebooks', '/Users/mah/gitsource/wind-covariation/notebooks']); import pickle; snakemake = pickle.loads(b'\x80\x04\x95\'\x06\x00\x00\x00\x00\x00\x00\x8c\x10snakemake.script\x94\x8c\tSnakemake\x94\x93\x94)\x81\x94}\x94(\x8c\x05input\x94\x8c\x0csnakemake.io\x94\x8c\nInputFiles\x94\x93\x94)\x81\x94(\x8c data/offshore_wind_locations.csv\x94\x8c data/nve_offshore_wind_areas.csv\x94\x8c\x1cdata/processed/wind_data.csv\x94e}\x94(\x8c\x06_names\x94}\x94(\x8c\tlocations\x94K\x00N\x86\x94\x8c\rnve_locations\x94K\x01N\x86\x94\x8c\x08combined\x94K\x02N\x86\x94u\x8c\x12_allowed_overrides\x94]\x94(\x8c\x05index\x94\x8c\x04sort\x94eh\x18\x8c\tfunctools\x94\x8c\x07partial\x94\x93\x94h\x06\x8c\x19Namedlist._used_attribute\x94\x93\x94\x85\x94R\x94(h\x1e)}\x94\x8c\x05_name\x94h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bh\x10h\nh\x12h\x0bh\x14h\x0cub\x8c\x06output\x94h\x06\x8c\x0bOutputFiles\x94\x93\x94)\x81\x94(\x8c\x18images/corr-distance.pdf\x94\x8c\x1cimages/shift-quantile0.9.pdf\x94\x8c\x1fimages/shift-quantile0.9999.pdf\x94\x8c"images/Utsira nord-std-wind-7D.pdf\x94\x8c"images/Utsira nord-std-wind-1H.pdf\x94\x8c0images/scatter-soerlige-nordsjoe-ii-nordmela.pdf\x94\x8c/images/scatter-soerlige-nordsjoe-ii-de-west.pdf\x94\x8c\x1bimages/corr-matrix-1day.pdf\x94\x8c&images/corr-matrix-1day-aggregated.pdf\x94\x8c3images/changes-wind-output-sorelige-nordsjoe-ii.pdf\x94e}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06params\x94h\x06\x8c\x06Params\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\twildcards\x94h\x06\x8c\tWildcards\x94\x93\x94)\x81\x94}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x07threads\x94K\x01\x8c\tresources\x94h\x06\x8c\tResources\x94\x93\x94)\x81\x94(K\x01K\x01\x8c0/var/folders/_c/vsl_fz_n62q_dg7lz4v2lb180000gn/T\x94e}\x94(h\x0e}\x94(\x8c\x06_cores\x94K\x00N\x86\x94\x8c\x06_nodes\x94K\x01N\x86\x94\x8c\x06tmpdir\x94K\x02N\x86\x94uh\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bhgK\x01hiK\x01hkhdub\x8c\x03log\x94h\x06\x8c\x03Log\x94\x93\x94)\x81\x94\x8c\x1elogs/run_analysis_notebook.log\x94a}\x94(h\x0e}\x94h\x16]\x94(h\x18h\x19eh\x18h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x18sNt\x94bh\x19h\x1ch\x1e\x85\x94R\x94(h\x1e)}\x94h"h\x19sNt\x94bub\x8c\x06config\x94}\x94\x8c\x04rule\x94\x8c\x15run_analysis_notebook\x94\x8c\x0fbench_iteration\x94N\x8c\tscriptdir\x94\x8c//Users/mah/gitsource/wind-covariation/notebooks\x94ub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/mah/gitsource/wind-covariation');
######## snakemake preamble end #########
  17:
# start coding here
snakemake.input
  18: %history -g
  19: %history
  20: %history -g
  21: %history -g
